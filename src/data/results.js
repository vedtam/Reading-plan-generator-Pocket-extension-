export default {
  "3622210930": {
    "item_id": "3622210930",
    "resolved_id": "3622210930",
    "given_url": "https://37signals.com/seven-shipping-principles",
    "given_title": "37signals — Seven Shipping Principles",
    "favorite": "0",
    "status": "0",
    "time_added": "1675795037",
    "time_updated": "1675795040",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 11,
    "resolved_title": "Seven Shipping Principles",
    "resolved_url": "https://37signals.com/seven-shipping-principles",
    "excerpt": "This might sound self-evident, but in reality there are all sorts of pressures that might lead you to ship mediocre or even iffy work. We promised the feature! We spent a lot of time on it! It’s not horrible! From the user’s perspective, it’s actually sorta fine?",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "0",
    "word_count": "1800",
    "lang": "en",
    "time_to_read": 8,
    "top_image_url": "https://37signals.com/assets/images/opengraph.png",
    "tags": {
      "readinglist": {
        "item_id": "3622210930",
        "tag": "readinglist"
      }
    },
    "listen_duration_estimate": 697,
    "content": {
      "title": "",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><div lang=\"en\">\n<h3>We only ship good work</h3>\n<p>This might sound self-evident, but in reality there are all sorts of pressures that might lead you to ship mediocre or even iffy work. We promised the feature! We spent a lot of time on it! It’s not <em>horrible</em>! From the user’s perspective, it’s actually sorta fine?</p>\n<p>None of these excuses are adequate reasons for us to ship anything but good work at 37signals. This means solid implementations in terms of design and programming, CSS and JavaScript, Ruby and operations. Work we’d be pleased to have to revisit 9 months later without a sigh of dread.</p>\n<p>Most things we work on at 37signals will be around <a href=\"https://basecamp.com/about/policies/until-the-end-of-the-internet\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-0\">until the end of the internet</a>. So whatever short-term gain you can capture by shipping short of good, you’ll lose when you have to work on it again later (and eventually you will!).</p>\n<p>This isn’t just about productivity, now or in the future, but also about pride in the work. The delight that comes from working with code and design that’s properly built. It’s hard to put a specific price on that, but easy to recognize the smile it puts on your face.</p>\n<p>This is not a general license to gold-plate everything. We intentionally constrain ourselves through the cycles in <a href=\"https://basecamp.com/shapeup\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-1\">Shape Up</a>, such that we don’t end up spending 2 months on stuff that warranted 2 weeks worth of work. Not every batch of work is going to be a 10/10. But if it’s less than 8/10, it probably shouldn’t go out the door. If it’s less than a 7/10, there’s no way it should go out the door, except as an emergency patch you immediately return to cleanup.</p>\n<p>Shipping good work takes discipline and sacrifice. We will afford ourselves both.</p>\n<hr><h3>We ship when we’re confident</h3>\n<p>The reason we do automated and exploratory testing is so we can ship work with confidence that it won’t cause major problems. But exactly how much confidence is required depends both on the team and the criticality of the problem. It’s a measure to gauge in relation to the specifics of the work, not mindlessly delegate to a standardized process.</p>\n<p>If you’re working on a small change that’s of low criticality, you don’t need the same due diligence as if you’re working on a big change of high criticality. That might seem obvious, but it’s easy just to run the familiar process, and forget to check whether it makes sense for the particulars.</p>\n<p>High criticality work involves anything that mutates or munges data. If you can lose data, it’s high criticality. It also involves anything that makes larger changes to existing and big features. If you’re rewriting how todos in Basecamp work, you better be pretty sure you’ve checked everything twice or thrice. If a bug makes it to production on high criticality work, it’s a big deal.</p>\n<p>Lower criticality work is anything that merely changes the presentation of existing data, or works on entirely new sets of data, or deals with screens or features off the critical path. If a bug makes it to production on lower criticality work, it won’t be a big deal. You fix it and move on.</p>\n<p>It’s up to the team to gauge whether what they’re working on is high or lower criticality or somewhere in-between. It also depends on who works on it. Someone well-versed in the domain? A lead with 10 years of experience? They’ll deem more things lower criticality than someone who just started recently. This is natural and right.</p>\n<p>If you need exploratory testing done by QA to feel confident that your work is solid enough for the criticality at stake, that’s what you should do. Even if it means waiting a bit to be able to ship. But if you’re confident that the work is likely to be good enough for the criticality at stake, you should just ship, stick around to deal with issues, and keep moving forward.</p>\n<p>You gauge your confidence, and if in doubt, check with someone more senior. The process, the tools, and the testing are there to increase your confidence. Not to tell you when it’s time to ship or not.</p>\n<hr><h3>We ship when the work is finished</h3>\n<p>It’s rare here to keep work that’s essentially finished behind a feature flag for long if at all. We don’t start a lot of highly speculative projects. If it’s been kicked off, it’s because we’ve decided we need it, and that means it’s going out the kitchen when it’s cooked.</p>\n<p>This means we’re not sitting on a large inventory of possibly-maybe projects that linger behind a feature flag until something magical happens. We clear the decks so we can get on with new work.</p>\n<p>That runs the risk that we’ll occasionally ship a project that isn’t quite right once we see it in the wild. That’s okay. If it’s bad enough, we’ll fix it then. We don’t have billions of users. They won’t die or leave in droves just because we launch something that perhaps could have been better if it had been done a different way from the start.</p>\n<p>The momentum of shipping good work has a quality all of its own. We don’t halt the momentum unless it’s properly bad, we don’t linger around second-guessing ourselves ad nauseam. The show must go on.</p>\n<hr><h3>We own the issues after we ship</h3>\n<p>Clean up your own mess if you make one. Pay attention to the error tracker for a while after launching. Be the first port of call for support when they get feedback from customers. If you did the work, you have the context, and you should put it straight if it’s crooked.</p>\n<p>This sometimes means that you will have started something new and then get pulled away to fix something old. If it’s a substantial issue, that’s just what it is. You put the new on hold while you fix the old.</p>\n<p>But you also have to keep the line on what “substantial” means. Almost everything we ship is met with a range of feedback. People will want more or less or different or better or whatever no matter how perfect the work. You can spend an eternity refining and reworking even the smallest change if you let yourself. We don’t let ourselves.</p>\n<p>So you’re on the hook to make it right, to make it work as intended, but all the feedback that goes beyond that is filed for consideration against all the other ideas and claims for our time.</p>\n<p>Ship it, fix it, forget it.</p>\n<hr><h3>We don’t ship if it isn’t right</h3>\n<p>It can be frustrating when work is held in the 11th hour because an issue or an insight that just popped up gives us pause. Why didn’t you mention this three weeks ago?? Yes, it’s indeed better to get feedback early, but often that’s just not possible, because the issue or the insight isn’t apparent until it’s go time. That’s when you really check your gut, and the truth spills out.</p>\n<p>But however frustrating it is to hit pause for a moment until the gut is settled, it’s far more frustrating to ship something that isn’t right to everyone. Once the work is in the wild, it’s hard to put it back in the box.</p>\n<p>It’s more important that what we ship is fundamentally right than letting stuff go out the door because it’s socially easier or doesn’t rub anyone the wrong way. It’s better still if we can get things right early, but when reality denies us that opportunity, we still pull the cord and stop the train if it’s not right.</p>\n<p>Everything that goes out has to easily pass the dual questions of <em>Is It Right?</em> and <em>Is It Good?</em></p>\n<hr><h3>We ship our collective best effort</h3>\n<p>Catching quality issues in implementation, design, or concept is ultimately everyone’s domain. That doesn’t mean all who work here get to determine whether something is important enough to halt shipping or even be addressed. But it does mean that everyone looking at the work is eligible to raise reservations or ideas for improvement.</p>\n<p>It’s natural to get a bit defensive when someone points out a perceived deficiency. Don’t fight that feeling, but let it float. Use it to interrogate the issue from both sides, the pro and the con. You don’t have to roll over on every objection, but you do have to consider them.</p>\n<p>And if the feedback comes from someone more senior than you, you should default to believing they probably have a point, because they’ve seen more and done more. That’s why they’re senior. It doesn’t mean they’re always right, but the odds are decent that they are, and you’ll learn faster if you take the default stance of the beginner’s mind.</p>\n<p>And if the feedback comes from someone more junior than you, make it a teaching moment if you choose to skip the suggestion. Why isn’t this relevant, right, or proportionate to deal with right now? And if the suggestion is adopted, celebrate together.</p>\n<p>Regardless of where the insight originates, it’s the higher quality that ultimately sets the bar. If what we’re working on could be better, simpler, faster, and we have the time to make it so, that’s what we’re going to do.</p>\n<hr><h3>We ship to our appetite</h3>\n<p>One of Shape Up’s key concepts is the appetite. Projects are kicked off on a premise that they’re only worth doing if a good version of their pitch can be done within 2, 4, or 6 weeks. Like a great company can be a terrible stock at an exorbitant price, so too can a great pitch become a mistake if it’s pursued far beyond the appetite.</p>\n<p>The time constraint imposed by the appetite is meant to force trade-offs and concessions. To curb the ambition that naturally turns every idea into a project that drags on forever by people drawn to perfection.</p>\n<p>You’ll nearly always hit the time constraint with more ideas, more minor issues, and more polish to do. That might feel frustrating in the moment. <em>If only I had two more weeks!!</em> But if you had two more weeks, chances are you’d just expand your ambitions accordingly, and you’d wish for two more weeks in addition to that at the end.</p>\n<p>Constraints force us to make choices and rank what we’d rather have if it’s “or” not “and”. They serve as an objective way to force other stakeholders to accept the art of the possible. Everyone can come up with ideas for more, but it’s much harder to decide on what would you rather if you can’t have both. What we usually need are substitutions, not additions.</p>\n<p>Feature creep and blown estimates are the industry standard. Our standard is that we ship the best work within the time we’ve given it, and we hold our heads proud to the compromises that entail.</p>\n</div></article></div>",
      "textContent": "\nWe only ship good work\nThis might sound self-evident, but in reality there are all sorts of pressures that might lead you to ship mediocre or even iffy work. We promised the feature! We spent a lot of time on it! It’s not horrible! From the user’s perspective, it’s actually sorta fine?\nNone of these excuses are adequate reasons for us to ship anything but good work at 37signals. This means solid implementations in terms of design and programming, CSS and JavaScript, Ruby and operations. Work we’d be pleased to have to revisit 9 months later without a sigh of dread.\nMost things we work on at 37signals will be around until the end of the internet. So whatever short-term gain you can capture by shipping short of good, you’ll lose when you have to work on it again later (and eventually you will!).\nThis isn’t just about productivity, now or in the future, but also about pride in the work. The delight that comes from working with code and design that’s properly built. It’s hard to put a specific price on that, but easy to recognize the smile it puts on your face.\nThis is not a general license to gold-plate everything. We intentionally constrain ourselves through the cycles in Shape Up, such that we don’t end up spending 2 months on stuff that warranted 2 weeks worth of work. Not every batch of work is going to be a 10/10. But if it’s less than 8/10, it probably shouldn’t go out the door. If it’s less than a 7/10, there’s no way it should go out the door, except as an emergency patch you immediately return to cleanup.\nShipping good work takes discipline and sacrifice. We will afford ourselves both.\nWe ship when we’re confident\nThe reason we do automated and exploratory testing is so we can ship work with confidence that it won’t cause major problems. But exactly how much confidence is required depends both on the team and the criticality of the problem. It’s a measure to gauge in relation to the specifics of the work, not mindlessly delegate to a standardized process.\nIf you’re working on a small change that’s of low criticality, you don’t need the same due diligence as if you’re working on a big change of high criticality. That might seem obvious, but it’s easy just to run the familiar process, and forget to check whether it makes sense for the particulars.\nHigh criticality work involves anything that mutates or munges data. If you can lose data, it’s high criticality. It also involves anything that makes larger changes to existing and big features. If you’re rewriting how todos in Basecamp work, you better be pretty sure you’ve checked everything twice or thrice. If a bug makes it to production on high criticality work, it’s a big deal.\nLower criticality work is anything that merely changes the presentation of existing data, or works on entirely new sets of data, or deals with screens or features off the critical path. If a bug makes it to production on lower criticality work, it won’t be a big deal. You fix it and move on.\nIt’s up to the team to gauge whether what they’re working on is high or lower criticality or somewhere in-between. It also depends on who works on it. Someone well-versed in the domain? A lead with 10 years of experience? They’ll deem more things lower criticality than someone who just started recently. This is natural and right.\nIf you need exploratory testing done by QA to feel confident that your work is solid enough for the criticality at stake, that’s what you should do. Even if it means waiting a bit to be able to ship. But if you’re confident that the work is likely to be good enough for the criticality at stake, you should just ship, stick around to deal with issues, and keep moving forward.\nYou gauge your confidence, and if in doubt, check with someone more senior. The process, the tools, and the testing are there to increase your confidence. Not to tell you when it’s time to ship or not.\nWe ship when the work is finished\nIt’s rare here to keep work that’s essentially finished behind a feature flag for long if at all. We don’t start a lot of highly speculative projects. If it’s been kicked off, it’s because we’ve decided we need it, and that means it’s going out the kitchen when it’s cooked.\nThis means we’re not sitting on a large inventory of possibly-maybe projects that linger behind a feature flag until something magical happens. We clear the decks so we can get on with new work.\nThat runs the risk that we’ll occasionally ship a project that isn’t quite right once we see it in the wild. That’s okay. If it’s bad enough, we’ll fix it then. We don’t have billions of users. They won’t die or leave in droves just because we launch something that perhaps could have been better if it had been done a different way from the start.\nThe momentum of shipping good work has a quality all of its own. We don’t halt the momentum unless it’s properly bad, we don’t linger around second-guessing ourselves ad nauseam. The show must go on.\nWe own the issues after we ship\nClean up your own mess if you make one. Pay attention to the error tracker for a while after launching. Be the first port of call for support when they get feedback from customers. If you did the work, you have the context, and you should put it straight if it’s crooked.\nThis sometimes means that you will have started something new and then get pulled away to fix something old. If it’s a substantial issue, that’s just what it is. You put the new on hold while you fix the old.\nBut you also have to keep the line on what “substantial” means. Almost everything we ship is met with a range of feedback. People will want more or less or different or better or whatever no matter how perfect the work. You can spend an eternity refining and reworking even the smallest change if you let yourself. We don’t let ourselves.\nSo you’re on the hook to make it right, to make it work as intended, but all the feedback that goes beyond that is filed for consideration against all the other ideas and claims for our time.\nShip it, fix it, forget it.\nWe don’t ship if it isn’t right\nIt can be frustrating when work is held in the 11th hour because an issue or an insight that just popped up gives us pause. Why didn’t you mention this three weeks ago?? Yes, it’s indeed better to get feedback early, but often that’s just not possible, because the issue or the insight isn’t apparent until it’s go time. That’s when you really check your gut, and the truth spills out.\nBut however frustrating it is to hit pause for a moment until the gut is settled, it’s far more frustrating to ship something that isn’t right to everyone. Once the work is in the wild, it’s hard to put it back in the box.\nIt’s more important that what we ship is fundamentally right than letting stuff go out the door because it’s socially easier or doesn’t rub anyone the wrong way. It’s better still if we can get things right early, but when reality denies us that opportunity, we still pull the cord and stop the train if it’s not right.\nEverything that goes out has to easily pass the dual questions of Is It Right? and Is It Good?\nWe ship our collective best effort\nCatching quality issues in implementation, design, or concept is ultimately everyone’s domain. That doesn’t mean all who work here get to determine whether something is important enough to halt shipping or even be addressed. But it does mean that everyone looking at the work is eligible to raise reservations or ideas for improvement.\nIt’s natural to get a bit defensive when someone points out a perceived deficiency. Don’t fight that feeling, but let it float. Use it to interrogate the issue from both sides, the pro and the con. You don’t have to roll over on every objection, but you do have to consider them.\nAnd if the feedback comes from someone more senior than you, you should default to believing they probably have a point, because they’ve seen more and done more. That’s why they’re senior. It doesn’t mean they’re always right, but the odds are decent that they are, and you’ll learn faster if you take the default stance of the beginner’s mind.\nAnd if the feedback comes from someone more junior than you, make it a teaching moment if you choose to skip the suggestion. Why isn’t this relevant, right, or proportionate to deal with right now? And if the suggestion is adopted, celebrate together.\nRegardless of where the insight originates, it’s the higher quality that ultimately sets the bar. If what we’re working on could be better, simpler, faster, and we have the time to make it so, that’s what we’re going to do.\nWe ship to our appetite\nOne of Shape Up’s key concepts is the appetite. Projects are kicked off on a premise that they’re only worth doing if a good version of their pitch can be done within 2, 4, or 6 weeks. Like a great company can be a terrible stock at an exorbitant price, so too can a great pitch become a mistake if it’s pursued far beyond the appetite.\nThe time constraint imposed by the appetite is meant to force trade-offs and concessions. To curb the ambition that naturally turns every idea into a project that drags on forever by people drawn to perfection.\nYou’ll nearly always hit the time constraint with more ideas, more minor issues, and more polish to do. That might feel frustrating in the moment. If only I had two more weeks!! But if you had two more weeks, chances are you’d just expand your ambitions accordingly, and you’d wish for two more weeks in addition to that at the end.\nConstraints force us to make choices and rank what we’d rather have if it’s “or” not “and”. They serve as an objective way to force other stakeholders to accept the art of the possible. Everyone can come up with ideas for more, but it’s much harder to decide on what would you rather if you can’t have both. What we usually need are substitutions, not additions.\nFeature creep and blown estimates are the industry standard. Our standard is that we ship the best work within the time we’ve given it, and we hold our heads proud to the compromises that entail.\n",
      "length": 10002,
      "excerpt": "This might sound self-evident, but in reality there are all sorts of pressures that might lead you to ship mediocre or even iffy work. We promised the feature! We spent a lot of time on it! It’s not horrible! From the user’s perspective, it’s actually sorta fine?",
      "siteName": null,
      "summary": "The 37signals team is committed to shipping only good work. This means solid implementations in terms of design and programming, CSS and JavaScript, Ruby and operations. Not every batch of work is going to be a 10/10, but if it’s less than 8/10 it probably shouldn’t go out the door.",
      "topics": [
        {
          "label": "Computers & Internet",
          "score": 0.9907021522521973
        },
        {
          "label": "Sports",
          "score": 0.9482164978981018
        },
        {
          "label": "Business & Finance",
          "score": 0.7407694458961487
        }
      ]
    }
  },
  "3762820979": {
    "item_id": "3762820979",
    "resolved_id": "3762820979",
    "given_url": "https://huggingface.co/blog/rlhf",
    "given_title": "Illustrating Reinforcement Learning from Human Feedback (RLHF)",
    "favorite": "0",
    "status": "0",
    "time_added": "1675795172",
    "time_updated": "1675795175",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 10,
    "resolved_title": "Illustrating Reinforcement Learning from Human Feedback (RLHF)",
    "resolved_url": "https://huggingface.co/blog/rlhf",
    "excerpt": "Language models have shown impressive capabilities in the past few years by generating diverse and compelling text from human input prompts. However, what makes a \"good\" text is inherently hard to define as it is subjective and context dependent.",
    "is_article": "1",
    "is_index": "1",
    "has_video": "0",
    "has_image": "1",
    "word_count": "3010",
    "lang": "en",
    "time_to_read": 14,
    "top_image_url": "https://huggingface.co/blog/assets/120_rlhf/thumbnail.png",
    "tags": {
      "readinglist": {
        "item_id": "3762820979",
        "tag": "readinglist"
      }
    },
    "image": {
      "item_id": "3762820979",
      "src": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/chatgpt-explains.png",
      "width": "500",
      "height": "0"
    },
    "images": {
      "1": {
        "item_id": "3762820979",
        "image_id": "1",
        "src": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/chatgpt-explains.png",
        "width": "500",
        "height": "0",
        "credit": "",
        "caption": ""
      },
      "2": {
        "item_id": "3762820979",
        "image_id": "2",
        "src": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/pretraining.png",
        "width": "500",
        "height": "0",
        "credit": "",
        "caption": ""
      },
      "3": {
        "item_id": "3762820979",
        "image_id": "3",
        "src": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/reward-model.png",
        "width": "600",
        "height": "0",
        "credit": "",
        "caption": ""
      },
      "4": {
        "item_id": "3762820979",
        "image_id": "4",
        "src": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/rlhf.png",
        "width": "650",
        "height": "0",
        "credit": "",
        "caption": ""
      }
    },
    "listen_duration_estimate": 1165,
    "content": {
      "title": "",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><div lang=\"en\">\n\n\n<p>Language models have shown impressive capabilities in the past few years by generating diverse and compelling text from human input prompts. However, what makes a \"good\" text is inherently hard to define as it is subjective and context dependent. There are many applications such as writing stories where you want creativity, pieces of informative text which should be truthful, or code snippets that we want to be executable.</p>\n<p>Writing a loss function to capture these attributes seems intractable and most language models are still trained with a simple next token prediction loss (e.g. cross entropy). To compensate for the shortcomings of the loss itself people define metrics that are designed to better capture human preferences such as <a href=\"https://en.wikipedia.org/wiki/BLEU\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-0\">BLEU</a> or <a href=\"https://en.wikipedia.org/wiki/ROUGE_(metric)\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-1\">ROUGE</a>. While being better suited than the loss function itself at measuring performance these metrics simply compare generated text to references with simple rules and are thus also limited. Wouldn't it be great if we use human feedback for generated text as a measure of performance or go even one step further and use that feedback as a loss to optimize the model? That's the idea of Reinforcement Learning from Human Feedback (RLHF); use methods from reinforcement learning to directly optimize a language model with human feedback. RLHF has enabled language models to begin to align a model trained on a general corpus of text data to that of complex human values.</p>\n<p>RLHF's most recent success was its use in <a href=\"https://openai.com/blog/chatgpt/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-2\">ChatGPT</a>. Given ChatGPT's impressive abilities, we asked it to explain RLHF for us:</p>\n\n<p>It does surprisingly well, but doesn't quite cover everything. We'll fill in those gaps!</p>\n<h2><a href=\"https://huggingface.co/blog/rlhf#rlhf-lets-take-it-step-by-step\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-3\"><span></span></a> <span>RLHF: Let’s take it step by step</span></h2>\n<p>Reinforcement learning from Human Feedback (also referenced as RL from human preferences) is a challenging concept because it involves a multiple-model training process and different stages of deployment. In this blog post, we’ll break down the training process into three core steps:</p>\n<ol><li>Pretraining a language model (LM),</li>\n<li>gathering data and training a reward model, and</li>\n<li>fine-tuning the LM with reinforcement learning.</li>\n</ol><p>To start, we'll look at how language models are pretrained.</p>\n<h3><a href=\"https://huggingface.co/blog/rlhf#pretraining-language-models\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-4\"><span></span></a> <span>Pretraining language models</span></h3>\n<p>As a starting point RLHF use a language model that has already been pretrained with the classical pretraining objectives (see this <a href=\"https://huggingface.co/blog/how-to-train\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-5\">blog post</a> for more details). OpenAI used a smaller version of GPT-3 for its first popular RLHF model, <a href=\"https://openai.com/blog/instruction-following/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-6\">InstructGPT</a>. Anthropic used transformer models from 10 million to 52 billion parameters trained for this task. DeepMind used their 280 billion parameter model <a href=\"https://arxiv.org/abs/2112.11446\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-7\">Gopher</a>.</p>\n<p>This initial model <em>can</em> also be fine-tuned on additional text or conditions, but does not necessarily need to be. For example, OpenAI fine-tuned on human-generated text that was “preferable” and Anthropic generated their initial LM for RLHF by distilling an original LM on context clues for their “helpful, honest, and harmless” criteria. These are both sources of what I refer to as expensive, <em>augmented</em> data, but it is not a required technique to understand RLHF.</p>\n<p>In general, there is not a clear answer on “which model” is the best for the starting point of RLHF. This will be a common theme in this blog – the design space of options in RLHF training are not thoroughly explored.</p>\n<p>Next, with a language model, one needs to generate data to train a <strong>reward model</strong>, which is how human preferences are integrated into the system.</p>\n<div id=\"RIL_IMG_2\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fhuggingface%2Fdocumentation-images%2Fresolve%2Fmain%2Fblog%2Frlhf%2Fpretraining.png\">\n          \n          \n        </figure>\n      </div>\n<h3><a href=\"https://huggingface.co/blog/rlhf#reward-model-training\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-8\"><span></span></a> <span>Reward model training</span></h3>\n<p>Generating a reward model (RM, also referred to as a preference model) calibrated with human preferences is where the relatively new research in RLHF begins. The underlying goal is to get a model or system that takes in a sequence of text, and returns a scalar reward which should numerically represent the human preference. The system can be an end-to-end LM, or a modular system outputting a reward (e.g. a model ranks outputs, and the ranking is converted to reward). The output being a <strong>scalar</strong> <strong>reward</strong> is crucial for existing RL algorithms being integrated seamlessly later in the RLHF process.</p>\n<p>These LMs for reward modeling can be both another fine-tuned LM or a LM trained from scratch on the preference data. For example, Anthropic uses a specialized method of fine-tuning to initialize these models after pretraining (preference model pretraining, PMP) because they found it be more sample efficient than fine-tuning, but no one variation of reward modeling is considered the clear best choice today.</p>\n<p>The training dataset of prompt-generation pairs for the RM is generated by sampling a set of prompts from a predefined dataset (Anthropic’s data generated primarily with a chat tool on Amazon Mechanical Turk is <a href=\"https://huggingface.co/datasets/Anthropic/hh-rlhf\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-9\">available</a> on the Hub, and OpenAI used prompts submitted by users to the GPT API). The prompts are passed through the initial language model to generate new text.</p>\n<p>Human annotators are used to rank the generated text outputs from the LM. One may initially think that humans should apply a scalar score directly to each piece of text in order to generate a reward model, but this is difficult to do in practice. The differing values of humans cause these scores to uncalibrated and noisy. Instead, rankings are used to compare the outputs of multiple models and create a much better regularized dataset.</p>\n<p>There are multiple methods for ranking the text. One method that has been successful is to have users compare generated text from two language models conditioned on the same prompt. By comparing model outputs in head-to-head matchups, an <a href=\"https://en.wikipedia.org/wiki/Elo_rating_system\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-10\">Elo</a> system can be used to generate a ranking of the models and outputs relative to each-other. These different methods of ranking are normalized into a scalar reward signal for training.</p>\n<p>An interesting artifact of this process is that the successful RLHF systems to date have used reward language models with varying sizes relative to the text generation (e.g. OpenAI 175B LM, 6B reward model, Anthropic used LM and reward models from 10B to 52B, DeepMind uses 70B Chinchilla models for both LM and reward). An intuition would be that these preference models need to have similar capacity to understand the text given to them as a model would need in order to generate said text.</p>\n<p>At this point in the RLHF system, we have an initial language model that can be used to generate text and a preference model that takes in any text and assigns it a score of how well humans perceive it. Next, we use <strong>reinforcement learning (RL)</strong> to optimize the original language model with respect to the reward model.</p>\n<div id=\"RIL_IMG_3\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fhuggingface.co%2Fdatasets%2Fhuggingface%2Fdocumentation-images%2Fresolve%2Fmain%2Fblog%2Frlhf%2Freward-model.png\">\n          \n          \n        </figure>\n      </div>\n<h3><a href=\"https://huggingface.co/blog/rlhf#fine-tuning-with-rl\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-11\"><span></span></a> <span>Fine-tuning with RL</span></h3>\n<p>Training a language model with reinforcement learning was, for a long time, something that people would have thought is impossible both for engineering and algorithmic reasons. What multiple organizations seem to have gotten to work is fine-tuning some or all of the parameters of a <strong>copy of the initial LM</strong> with a policy-gradient RL algorithm, Proximal Policy Optimization (PPO). Parameters of the LM are frozen because fine-tuning an entire 10B or 100B+ parameter model is prohibitively expensive (for more, see Low-Rank Adaptation (<a href=\"https://arxiv.org/abs/2106.09685\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-12\">LoRA</a>) for LMs or the <a href=\"https://arxiv.org/abs/2209.14375\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-13\">Sparrow</a> LM from DeepMind). PPO has been around for a relatively long time – there are <a href=\"https://spinningup.openai.com/en/latest/algorithms/ppo.html\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-14\">tons</a> of <a href=\"https://huggingface.co/blog/deep-rl-ppo\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-15\">guides</a> on how it works. The relative maturity of this method made it a favorable choice for scaling up to the new application of distributed training for RLHF. It turns out that many of the core RL advancements to do RLHF have been figuring out how to update such a large model with a familiar algorithm (more on that later).</p>\n<p>Let's first formulate this fine-tuning task as a RL problem. First, the <strong>policy</strong> is a language model that takes in a prompt and returns a sequence of text (or just probability distributions over text). The <strong>action space</strong> of this policy is all the tokens corresponding to the vocabulary of the language model (often on the order of 50k tokens) and the <strong>observation space</strong> is the possible input token sequences, which is also quite large (size of vocabulary ^ number of input tokens). The <strong>reward function</strong> is a combination of the preference model and a constraint on policy shift.</p>\n<p>The reward function is where the system combines all of the models we have discussed into one RLHF process. Given a prompt, <em>x</em>,&nbsp; from the dataset, two texts, <em>y1</em>, <em>y2</em>, are generated – one from the initial language model and one from the current iteration of the fine-tuned policy. The text from the current policy is passed to the preference model, which returns a scalar notion of “preferability”, <span><span><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>r</mi><mi>θ</mi></msub></mrow>r_\\theta</math></span></span>. This text is compared to the text from the initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback–Leibler <a href=\"https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-16\">(KL) divergence</a> between these sequences of distributions over tokens, <span><span><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><msub><mi>r</mi><mtext>KL</mtext></msub></mrow>r_\\text{KL}</math></span></span>. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via sampling from both distributions (explained by John Schulman <a href=\"http://joschu.net/blog/kl-approx.html\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-17\">here</a>). The final reward sent to the RL update rule is <span><span><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><mrow><mi>r</mi><mo>=</mo><msub><mi>r</mi><mi>θ</mi></msub><mo>−</mo><mi>λ</mi><msub><mi>r</mi><mtext>KL</mtext></msub></mrow>r = r_\\theta - \\lambda r_\\text{KL}</math></span></span>.</p>\n<p>Some RLHF systems have added additional terms to the reward function. For example, OpenAI experimented successfully on InstructGPT by mixing in additional pre-training gradients (from the human annotation set) into the update rule for PPO. It is likely as RLHF is further investigated, the formulation of this reward function will continue to evolve.</p>\n<p>Finally, the <strong>update rule</strong> is the parameter update from PPO that maximizes the reward metrics in the current batch of data (PPO is on-policy, which means the parameters are only updated with the current batch of prompt-generation pairs). PPO is a trust region optimization algorithm that uses constraints on the gradient to ensure the update step does not destabilize the learning process. DeepMind used a similar reward setup for Gopher but used <a href=\"http://proceedings.mlr.press/v48/mniha16.html?ref=https://githubhelp.com\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-18\">synchronous advantage actor-critic</a> (A2C) to optimize the gradients, which is notably different but has not been reproduced externally.</p>\n\n<p>Optionally, RLHF can continue from this point by iteratively updating the reward model and the policy together. As the RL policy updates, users can continue ranking these outputs versus the model's earlier versions. Most papers have yet to discuss implementing this operation, as the deployment mode needed to collect this type of data only works for dialogue agents with access to an engaged user base. Anthropic discusses this option as <em>Iterated Online RLHF</em> (see the original <a href=\"https://arxiv.org/abs/2204.05862\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-19\">paper</a>), where iterations of the policy are included in the ELO ranking system across models. This introduces complex dynamics of the policy and reward model evolving, which represents a complex and open research question.</p>\n<h2><a href=\"https://huggingface.co/blog/rlhf#open-source-tools-for-rlhf\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-20\"><span></span></a> <span>Open-source tools for RLHF</span></h2>\n<p>The first <a href=\"https://github.com/openai/lm-human-preferences\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-21\">code</a> released to perform RLHF on LMs was from OpenAI in TensorFlow in 2019.</p>\n<p>Today, there are already a few active repositories for RLHF in PyTorch that grew out of this. The primary repositories are Transformers Reinforcement Learning (<a href=\"https://github.com/lvwerra/trl\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-22\">TRL</a>), <a href=\"https://github.com/CarperAI/trlx\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-23\">TRLX</a> which originated as a fork of TRL, and Reinforcement Learning for Language models (<a href=\"https://github.com/allenai/RL4LMs\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-24\">RL4LMs</a>).</p>\n<p>TRL is designed to fine-tune pretrained LMs in the Hugging Face ecosystem with PPO. TRLX is an expanded fork of TRL built by <a href=\"https://carper.ai/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-25\">CarperAI</a> to handle larger models for online and offline training. At the moment, TRLX has an API capable of production-ready RLHF with PPO and Implicit Language Q-Learning <a href=\"https://sea-snell.github.io/ILQL_site/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-26\">ILQL</a> at the scales required for LLM deployment (e.g. 33 billion parameters). Future versions of TRLX will allow for language models up to 200B parameters. As such, interfacing with TRLX is optimized for machine learning engineers with experience at this scale.</p>\n<p><a href=\"https://github.com/allenai/RL4LMs\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-27\">RL4LMs</a> offers building blocks for fine-tuning and evaluating LLMs with a wide variety of RL algorithms (PPO, NLPO, A2C and TRPO), reward functions and metrics. Moreover, the library is easily customizable, which allows training of any encoder-decoder or encoder transformer-based LM on any arbitrary user-specified reward function. Notably, it is well-tested and benchmarked on a broad range of tasks in <a href=\"https://arxiv.org/abs/2210.01241\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-28\">recent work</a> amounting up to 2000 experiments highlighting several practical insights on data budget comparison (expert demonstrations vs. reward modeling), handling reward hacking and training instabilities, etc. RL4LMs current plans include distributed training of larger models and new RL algorithms.</p>\n<p>Both TRLX and RL4LMs are under heavy further development, so expect more features beyond these soon.</p>\n<p>There is a large <a href=\"https://huggingface.co/datasets/Anthropic/hh-rlhf\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-29\">dataset</a> created by Anthropic available on the Hub.</p>\n<h2><a href=\"https://huggingface.co/blog/rlhf#whats-next-for-rlhf\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-30\"><span></span></a> <span>What’s next for RLHF?</span></h2>\n<p>While these techniques are extremely promising and impactful and have caught the attention of the biggest research labs in AI, there are still clear limitations. The models, while better, can still output harmful or factually inaccurate text without any uncertainty. This imperfection represents a long-term challenge and motivation for RLHF – operating in an inherently human problem domain means there will never be a clear final line to cross for the model to be labeled as <em>complete</em>.</p>\n<p>When deploying a system using RLHF, gathering the human preference data is quite expensive due to the mandatory and thoughtful human component. RLHF performance is only as good as the quality of its human annotations, which takes on two varieties: human-generated text, such as fine-tuning the initial LM in InstructGPT, and labels of human preferences between model outputs.</p>\n<p>Generating well-written human text answering specific prompts is very costly, as it often requires hiring part-time staff (rather than being able to rely on product users or crowdsourcing). Thankfully, the scale of data used in training the reward model for most applications of RLHF (~50k labeled preference samples) is not as expensive. However, it is still a higher cost than academic labs would likely be able to afford. Currently, there only exists one large-scale dataset for RLHF on a general language model (from <a href=\"https://huggingface.co/datasets/Anthropic/hh-rlhf\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-31\">Anthropic</a>) and a couple of smaller-scale task-specific datasets (such as summarization data from <a href=\"https://github.com/openai/summarize-from-feedback\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-32\">OpenAI</a>). The second challenge of data for RLHF is that human annotators can often disagree, adding a substantial potential variance to the training data without ground truth.</p>\n<p>With these limitations, huge swaths of unexplored design options could still enable RLHF to take substantial strides. Many of these fall within the domain of improving the RL optimizer. PPO is a relatively old algorithm, but there are no structural reasons that other algorithms could offer benefits and permutations on the existing RLHF workflow. One large cost of the feedback portion of fine-tuning the LM policy is that every generated piece of text from the policy needs to be evaluated on the reward model (as it acts like part of the environment in the standard RL framework). To avoid these costly forward passes of a large model, offline RL could be used as a policy optimizer. Recently, new algorithms have emerged, such as <a href=\"https://arxiv.org/abs/2206.11871\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-33\">implicit language Q-learning</a> (ILQL) [<a href=\"https://youtu.be/fGq4np3brbs\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-34\">Talk</a> on ILQL at CarperAI], that fit particularly well with this type of optimization. Other core trade-offs in the RL process, like exploration-exploitation balance, have also not been documented. Exploring these directions would at least develop a substantial understanding of how RLHF functions and, if not, provide improved performance.</p>\n<p>We hosted a lecture on Tuesday 13 December 2022 that expanded on this post; you can watch it <a href=\"https://www.youtube.com/watch?v=2MBJOuVq380&amp;feature=youtu.be\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-35\">here</a>!</p>\n<h3><a href=\"https://huggingface.co/blog/rlhf#further-reading\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-36\"><span></span></a> <span>Further reading</span></h3>\n<p>Here is a list of the most prevalent papers on RLHF to date. The field was recently popularized with the emergence of DeepRL (around 2017) and has grown into a broader study of the applications of LLMs from many large technology companies. Here are some papers on RLHF that pre-date the LM focus:</p>\n<p>And here is a snapshot of the growing set of papers that show RLHF's performance for LMs:</p>\n<ul><li><a href=\"https://arxiv.org/abs/1909.08593\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-37\">Fine-Tuning Language Models from Human Preferences</a> (Zieglar et al. 2019): An early paper that studies the impact of reward learning on four specific tasks.</li>\n<li><a href=\"https://proceedings.neurips.cc/paper/2020/hash/1f89885d556929e98d3ef9b86448f951-Abstract.html\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-38\">Learning to summarize with human feedback</a> (Stiennon et al., 2020): RLHF applied to the task of summarizing text. Also, <a href=\"https://arxiv.org/abs/2109.10862\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-39\">Recursively Summarizing Books with Human Feedback</a> (OpenAI Alignment Team 2021), follow on work summarizing books.</li>\n<li><a href=\"https://arxiv.org/abs/2112.09332\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-40\">WebGPT: Browser-assisted question-answering with human feedback</a> (OpenAI, 2021): Using RLHF to train an agent to navigate the web.</li>\n<li>InstructGPT: <a href=\"https://arxiv.org/abs/2203.02155\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-41\">Training language models to follow instructions with human feedback</a> (OpenAI Alignment Team 2022): RLHF applied to a general language model [<a href=\"https://openai.com/blog/instruction-following/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-42\">Blog post</a> on InstructGPT].</li>\n<li>GopherCite: <a href=\"https://www.deepmind.com/publications/gophercite-teaching-language-models-to-support-answers-with-verified-quotes\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-43\">Teaching language models to support answers with verified quotes</a> (Menick et al. 2022): Train a LM with RLHF to return answers with specific citations.</li>\n<li>Sparrow: <a href=\"https://arxiv.org/abs/2209.14375\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-44\">Improving alignment of dialogue agents via targeted human judgements</a> (Glaese et al. 2022): Fine-tuning a dialogue agent with RLHF</li>\n<li><a href=\"https://openai.com/blog/chatgpt/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-45\">ChatGPT: Optimizing Language Models for Dialogue</a> (OpenAI 2022): Training a LM with RLHF for suitable use as an all-purpose chat bot.</li>\n<li><a href=\"https://arxiv.org/abs/2210.10760\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-46\">Scaling Laws for Reward Model Overoptimization</a> (Gao et al. 2022): studies the scaling properties of the learned preference model in RLHF.</li>\n<li><a href=\"https://arxiv.org/abs/2204.05862\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-47\">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</a> (Anthropic, 2022): A detailed documentation of training a LM assistant with RLHF.</li>\n<li><a href=\"https://arxiv.org/abs/2209.07858\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-48\">Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned</a> (Ganguli et al. 2022): A detailed documentation of efforts to “discover, measure, and attempt to reduce [language models] potentially harmful outputs.”</li>\n<li><a href=\"https://arxiv.org/abs/2208.02294\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-49\">Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning</a> (Cohen at al. 2022): Using RL to enhance the conversational skill of an open-ended dialogue agent.</li>\n<li><a href=\"https://arxiv.org/abs/2210.01241\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-50\">Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization</a> (Ramamurthy and Ammanabrolu et al. 2022): Discusses the design space of open-source tools in RLHF and proposes a new algorithm NLPO (Natural Language Policy Optimization) as an alternative to PPO.</li>\n</ul><p><strong>Citation:</strong> If you found this useful for your academic work, please consider citing our work, in text:</p>\n<pre><code>Lambert, et al., \"Illustrating Reinforcement Learning from Human Feedback (RLHF)\", Hugging Face Blog, 2022.\n</code></pre>\n<p>BibTeX citation:</p>\n<pre><code>@article{lambert2022illustrating,\n  author = {Lambert, Nathan and Castricato, Louis and von Werra, Leandro and Havrilla, Alex},\n  title = {Illustrating Reinforcement Learning from Human Feedback (RLHF)},\n  journal = {Hugging Face Blog},\n  year = {2022},\n  note = {https://huggingface.co/blog/rlhf},\n}\n</code></pre>\n<p><em>Thanks to <a href=\"https://robertkirk.github.io/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-51\">Robert Krik</a> for fixing some factual errors regarding specific implementations of RLHF. Thanks to <a href=\"https://www.cs.utexas.edu/~pstone/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-52\">Peter Stone</a> for helping expand the related works further into history.</em></p>\n</div></article></div>",
      "textContent": "\n\n\nLanguage models have shown impressive capabilities in the past few years by generating diverse and compelling text from human input prompts. However, what makes a \"good\" text is inherently hard to define as it is subjective and context dependent. There are many applications such as writing stories where you want creativity, pieces of informative text which should be truthful, or code snippets that we want to be executable.\nWriting a loss function to capture these attributes seems intractable and most language models are still trained with a simple next token prediction loss (e.g. cross entropy). To compensate for the shortcomings of the loss itself people define metrics that are designed to better capture human preferences such as BLEU or ROUGE. While being better suited than the loss function itself at measuring performance these metrics simply compare generated text to references with simple rules and are thus also limited. Wouldn't it be great if we use human feedback for generated text as a measure of performance or go even one step further and use that feedback as a loss to optimize the model? That's the idea of Reinforcement Learning from Human Feedback (RLHF); use methods from reinforcement learning to directly optimize a language model with human feedback. RLHF has enabled language models to begin to align a model trained on a general corpus of text data to that of complex human values.\nRLHF's most recent success was its use in ChatGPT. Given ChatGPT's impressive abilities, we asked it to explain RLHF for us:\n\nIt does surprisingly well, but doesn't quite cover everything. We'll fill in those gaps!\n RLHF: Let’s take it step by step\nReinforcement learning from Human Feedback (also referenced as RL from human preferences) is a challenging concept because it involves a multiple-model training process and different stages of deployment. In this blog post, we’ll break down the training process into three core steps:\nPretraining a language model (LM),\ngathering data and training a reward model, and\nfine-tuning the LM with reinforcement learning.\nTo start, we'll look at how language models are pretrained.\n Pretraining language models\nAs a starting point RLHF use a language model that has already been pretrained with the classical pretraining objectives (see this blog post for more details). OpenAI used a smaller version of GPT-3 for its first popular RLHF model, InstructGPT. Anthropic used transformer models from 10 million to 52 billion parameters trained for this task. DeepMind used their 280 billion parameter model Gopher.\nThis initial model can also be fine-tuned on additional text or conditions, but does not necessarily need to be. For example, OpenAI fine-tuned on human-generated text that was “preferable” and Anthropic generated their initial LM for RLHF by distilling an original LM on context clues for their “helpful, honest, and harmless” criteria. These are both sources of what I refer to as expensive, augmented data, but it is not a required technique to understand RLHF.\nIn general, there is not a clear answer on “which model” is the best for the starting point of RLHF. This will be a common theme in this blog – the design space of options in RLHF training are not thoroughly explored.\nNext, with a language model, one needs to generate data to train a reward model, which is how human preferences are integrated into the system.\n\n        \n          \n          \n          \n        \n      \n Reward model training\nGenerating a reward model (RM, also referred to as a preference model) calibrated with human preferences is where the relatively new research in RLHF begins. The underlying goal is to get a model or system that takes in a sequence of text, and returns a scalar reward which should numerically represent the human preference. The system can be an end-to-end LM, or a modular system outputting a reward (e.g. a model ranks outputs, and the ranking is converted to reward). The output being a scalar reward is crucial for existing RL algorithms being integrated seamlessly later in the RLHF process.\nThese LMs for reward modeling can be both another fine-tuned LM or a LM trained from scratch on the preference data. For example, Anthropic uses a specialized method of fine-tuning to initialize these models after pretraining (preference model pretraining, PMP) because they found it be more sample efficient than fine-tuning, but no one variation of reward modeling is considered the clear best choice today.\nThe training dataset of prompt-generation pairs for the RM is generated by sampling a set of prompts from a predefined dataset (Anthropic’s data generated primarily with a chat tool on Amazon Mechanical Turk is available on the Hub, and OpenAI used prompts submitted by users to the GPT API). The prompts are passed through the initial language model to generate new text.\nHuman annotators are used to rank the generated text outputs from the LM. One may initially think that humans should apply a scalar score directly to each piece of text in order to generate a reward model, but this is difficult to do in practice. The differing values of humans cause these scores to uncalibrated and noisy. Instead, rankings are used to compare the outputs of multiple models and create a much better regularized dataset.\nThere are multiple methods for ranking the text. One method that has been successful is to have users compare generated text from two language models conditioned on the same prompt. By comparing model outputs in head-to-head matchups, an Elo system can be used to generate a ranking of the models and outputs relative to each-other. These different methods of ranking are normalized into a scalar reward signal for training.\nAn interesting artifact of this process is that the successful RLHF systems to date have used reward language models with varying sizes relative to the text generation (e.g. OpenAI 175B LM, 6B reward model, Anthropic used LM and reward models from 10B to 52B, DeepMind uses 70B Chinchilla models for both LM and reward). An intuition would be that these preference models need to have similar capacity to understand the text given to them as a model would need in order to generate said text.\nAt this point in the RLHF system, we have an initial language model that can be used to generate text and a preference model that takes in any text and assigns it a score of how well humans perceive it. Next, we use reinforcement learning (RL) to optimize the original language model with respect to the reward model.\n\n        \n          \n          \n          \n        \n      \n Fine-tuning with RL\nTraining a language model with reinforcement learning was, for a long time, something that people would have thought is impossible both for engineering and algorithmic reasons. What multiple organizations seem to have gotten to work is fine-tuning some or all of the parameters of a copy of the initial LM with a policy-gradient RL algorithm, Proximal Policy Optimization (PPO). Parameters of the LM are frozen because fine-tuning an entire 10B or 100B+ parameter model is prohibitively expensive (for more, see Low-Rank Adaptation (LoRA) for LMs or the Sparrow LM from DeepMind). PPO has been around for a relatively long time – there are tons of guides on how it works. The relative maturity of this method made it a favorable choice for scaling up to the new application of distributed training for RLHF. It turns out that many of the core RL advancements to do RLHF have been figuring out how to update such a large model with a familiar algorithm (more on that later).\nLet's first formulate this fine-tuning task as a RL problem. First, the policy is a language model that takes in a prompt and returns a sequence of text (or just probability distributions over text). The action space of this policy is all the tokens corresponding to the vocabulary of the language model (often on the order of 50k tokens) and the observation space is the possible input token sequences, which is also quite large (size of vocabulary ^ number of input tokens). The reward function is a combination of the preference model and a constraint on policy shift.\nThe reward function is where the system combines all of the models we have discussed into one RLHF process. Given a prompt, x,  from the dataset, two texts, y1, y2, are generated – one from the initial language model and one from the current iteration of the fine-tuned policy. The text from the current policy is passed to the preference model, which returns a scalar notion of “preferability”, rθr_\\theta. This text is compared to the text from the initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback–Leibler (KL) divergence between these sequences of distributions over tokens, rKLr_\\text{KL}. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via sampling from both distributions (explained by John Schulman here). The final reward sent to the RL update rule is r=rθ−λrKLr = r_\\theta - \\lambda r_\\text{KL}.\nSome RLHF systems have added additional terms to the reward function. For example, OpenAI experimented successfully on InstructGPT by mixing in additional pre-training gradients (from the human annotation set) into the update rule for PPO. It is likely as RLHF is further investigated, the formulation of this reward function will continue to evolve.\nFinally, the update rule is the parameter update from PPO that maximizes the reward metrics in the current batch of data (PPO is on-policy, which means the parameters are only updated with the current batch of prompt-generation pairs). PPO is a trust region optimization algorithm that uses constraints on the gradient to ensure the update step does not destabilize the learning process. DeepMind used a similar reward setup for Gopher but used synchronous advantage actor-critic (A2C) to optimize the gradients, which is notably different but has not been reproduced externally.\n\nOptionally, RLHF can continue from this point by iteratively updating the reward model and the policy together. As the RL policy updates, users can continue ranking these outputs versus the model's earlier versions. Most papers have yet to discuss implementing this operation, as the deployment mode needed to collect this type of data only works for dialogue agents with access to an engaged user base. Anthropic discusses this option as Iterated Online RLHF (see the original paper), where iterations of the policy are included in the ELO ranking system across models. This introduces complex dynamics of the policy and reward model evolving, which represents a complex and open research question.\n Open-source tools for RLHF\nThe first code released to perform RLHF on LMs was from OpenAI in TensorFlow in 2019.\nToday, there are already a few active repositories for RLHF in PyTorch that grew out of this. The primary repositories are Transformers Reinforcement Learning (TRL), TRLX which originated as a fork of TRL, and Reinforcement Learning for Language models (RL4LMs).\nTRL is designed to fine-tune pretrained LMs in the Hugging Face ecosystem with PPO. TRLX is an expanded fork of TRL built by CarperAI to handle larger models for online and offline training. At the moment, TRLX has an API capable of production-ready RLHF with PPO and Implicit Language Q-Learning ILQL at the scales required for LLM deployment (e.g. 33 billion parameters). Future versions of TRLX will allow for language models up to 200B parameters. As such, interfacing with TRLX is optimized for machine learning engineers with experience at this scale.\nRL4LMs offers building blocks for fine-tuning and evaluating LLMs with a wide variety of RL algorithms (PPO, NLPO, A2C and TRPO), reward functions and metrics. Moreover, the library is easily customizable, which allows training of any encoder-decoder or encoder transformer-based LM on any arbitrary user-specified reward function. Notably, it is well-tested and benchmarked on a broad range of tasks in recent work amounting up to 2000 experiments highlighting several practical insights on data budget comparison (expert demonstrations vs. reward modeling), handling reward hacking and training instabilities, etc. RL4LMs current plans include distributed training of larger models and new RL algorithms.\nBoth TRLX and RL4LMs are under heavy further development, so expect more features beyond these soon.\nThere is a large dataset created by Anthropic available on the Hub.\n What’s next for RLHF?\nWhile these techniques are extremely promising and impactful and have caught the attention of the biggest research labs in AI, there are still clear limitations. The models, while better, can still output harmful or factually inaccurate text without any uncertainty. This imperfection represents a long-term challenge and motivation for RLHF – operating in an inherently human problem domain means there will never be a clear final line to cross for the model to be labeled as complete.\nWhen deploying a system using RLHF, gathering the human preference data is quite expensive due to the mandatory and thoughtful human component. RLHF performance is only as good as the quality of its human annotations, which takes on two varieties: human-generated text, such as fine-tuning the initial LM in InstructGPT, and labels of human preferences between model outputs.\nGenerating well-written human text answering specific prompts is very costly, as it often requires hiring part-time staff (rather than being able to rely on product users or crowdsourcing). Thankfully, the scale of data used in training the reward model for most applications of RLHF (~50k labeled preference samples) is not as expensive. However, it is still a higher cost than academic labs would likely be able to afford. Currently, there only exists one large-scale dataset for RLHF on a general language model (from Anthropic) and a couple of smaller-scale task-specific datasets (such as summarization data from OpenAI). The second challenge of data for RLHF is that human annotators can often disagree, adding a substantial potential variance to the training data without ground truth.\nWith these limitations, huge swaths of unexplored design options could still enable RLHF to take substantial strides. Many of these fall within the domain of improving the RL optimizer. PPO is a relatively old algorithm, but there are no structural reasons that other algorithms could offer benefits and permutations on the existing RLHF workflow. One large cost of the feedback portion of fine-tuning the LM policy is that every generated piece of text from the policy needs to be evaluated on the reward model (as it acts like part of the environment in the standard RL framework). To avoid these costly forward passes of a large model, offline RL could be used as a policy optimizer. Recently, new algorithms have emerged, such as implicit language Q-learning (ILQL) [Talk on ILQL at CarperAI], that fit particularly well with this type of optimization. Other core trade-offs in the RL process, like exploration-exploitation balance, have also not been documented. Exploring these directions would at least develop a substantial understanding of how RLHF functions and, if not, provide improved performance.\nWe hosted a lecture on Tuesday 13 December 2022 that expanded on this post; you can watch it here!\n Further reading\nHere is a list of the most prevalent papers on RLHF to date. The field was recently popularized with the emergence of DeepRL (around 2017) and has grown into a broader study of the applications of LLMs from many large technology companies. Here are some papers on RLHF that pre-date the LM focus:\nAnd here is a snapshot of the growing set of papers that show RLHF's performance for LMs:\nFine-Tuning Language Models from Human Preferences (Zieglar et al. 2019): An early paper that studies the impact of reward learning on four specific tasks.\nLearning to summarize with human feedback (Stiennon et al., 2020): RLHF applied to the task of summarizing text. Also, Recursively Summarizing Books with Human Feedback (OpenAI Alignment Team 2021), follow on work summarizing books.\nWebGPT: Browser-assisted question-answering with human feedback (OpenAI, 2021): Using RLHF to train an agent to navigate the web.\nInstructGPT: Training language models to follow instructions with human feedback (OpenAI Alignment Team 2022): RLHF applied to a general language model [Blog post on InstructGPT].\nGopherCite: Teaching language models to support answers with verified quotes (Menick et al. 2022): Train a LM with RLHF to return answers with specific citations.\nSparrow: Improving alignment of dialogue agents via targeted human judgements (Glaese et al. 2022): Fine-tuning a dialogue agent with RLHF\nChatGPT: Optimizing Language Models for Dialogue (OpenAI 2022): Training a LM with RLHF for suitable use as an all-purpose chat bot.\nScaling Laws for Reward Model Overoptimization (Gao et al. 2022): studies the scaling properties of the learned preference model in RLHF.\nTraining a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback (Anthropic, 2022): A detailed documentation of training a LM assistant with RLHF.\nRed Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned (Ganguli et al. 2022): A detailed documentation of efforts to “discover, measure, and attempt to reduce [language models] potentially harmful outputs.”\nDynamic Planning in Open-Ended Dialogue using Reinforcement Learning (Cohen at al. 2022): Using RL to enhance the conversational skill of an open-ended dialogue agent.\nIs Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization (Ramamurthy and Ammanabrolu et al. 2022): Discusses the design space of open-source tools in RLHF and proposes a new algorithm NLPO (Natural Language Policy Optimization) as an alternative to PPO.\nCitation: If you found this useful for your academic work, please consider citing our work, in text:\nLambert, et al., \"Illustrating Reinforcement Learning from Human Feedback (RLHF)\", Hugging Face Blog, 2022.\n\nBibTeX citation:\n@article{lambert2022illustrating,\n  author = {Lambert, Nathan and Castricato, Louis and von Werra, Leandro and Havrilla, Alex},\n  title = {Illustrating Reinforcement Learning from Human Feedback (RLHF)},\n  journal = {Hugging Face Blog},\n  year = {2022},\n  note = {https://huggingface.co/blog/rlhf},\n}\n\nThanks to Robert Krik for fixing some factual errors regarding specific implementations of RLHF. Thanks to Peter Stone for helping expand the related works further into history.\n",
      "length": 19116,
      "excerpt": "Language models have shown impressive capabilities in the past few years by generating diverse and compelling text from human input prompts. However, what makes a \"good\" text is inherently hard to define as it is subjective and context dependent. There are many applications such as writing stories where you want creativity, pieces of informative text which should be truthful, or code snippets that we want to be executable.",
      "siteName": null,
      "summary": "Reinforcement Learning from Human Feedback (RLHF) is a new way of training language models. RLHF uses reinforcement learning to directly optimize a language model with human feedback. The training process is broken down into three steps: Pretraining language models, gathering data and training a reward model, and fine-tuning the model.",
      "topics": [
        {
          "label": "Education & Reference",
          "score": 0.9661175012588501
        },
        {
          "label": "Society & Culture",
          "score": 0.7522262930870056
        },
        {
          "label": "Computers & Internet",
          "score": 0.7344860434532166
        }
      ]
    }
  },
  "3772247052": {
    "item_id": "3772247052",
    "resolved_id": "3772247052",
    "given_url": "https://towardsdatascience.com/dense-vectors-capturing-meaning-with-code-88fc18bd94b9",
    "given_title": "Dense Vectors Explained | Towards Data Science",
    "favorite": "0",
    "status": "0",
    "time_added": "1675709696",
    "time_updated": "1675709701",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 16,
    "resolved_title": "Dense Vectors: Capturing Meaning with Code",
    "resolved_url": "https://towardsdatascience.com/dense-vectors-capturing-meaning-with-code-88fc18bd94b9",
    "excerpt": "There is perhaps no greater contributor to the success of modern Natural Language Processing (NLP) technology than vector representations of language. The meteoric rise of NLP was ignited with the introduction of word2vec in 2013 [1].",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "0",
    "word_count": "2828",
    "lang": "en",
    "time_to_read": 13,
    "top_image_url": "https://miro.medium.com/max/1200/0*FeuwDpYkFTVVemk4",
    "tags": {
      "readinglist": {
        "item_id": "3772247052",
        "tag": "readinglist"
      }
    },
    "authors": {
      "141714615": {
        "item_id": "3772247052",
        "author_id": "141714615",
        "name": "James Briggs",
        "url": "https://jamescalam.medium.com"
      }
    },
    "domain_metadata": {
      "name": "Towards Data Science",
      "logo": "https://logo.clearbit.com/towardsdatascience.com?size=800",
      "greyscale_logo": "https://logo.clearbit.com/towardsdatascience.com?size=800&greyscale=true"
    },
    "listen_duration_estimate": 1095,
    "content": {
      "title": "",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><p><em>Sparse vectors are called sparse because vectors are sparsely populated with information. Typically we would be looking at thousands of zeros to find a few ones (our relevant information). Consequently, these vectors can contain many dimensions, often in the tens of thousands.</em></p><p>Where sparse vectors represent text syntax, we could view dense vectors as <em>numerical representations of semantic meaning</em>. Typically, we are taking words and encoding them into very dense, high-dimensional vectors. The abstract meaning and relationship of words are numerically encoded.</p><p><em>Dense vectors are still highly dimensional (784-dimensions are common, but it can be more or less). However, each dimension contains relevant information, determined by a neural net — compressing these vectors is more complex, so they typically use more memory.</em></p><p><em>We usually take an existing public model to generate vectors. For almost every scenario there is a high-performance model out there and it is easier, faster, and often much more accurate to use them. There are cases, for example for industry or language-specific embeddings where you sometimes need to fine-tune or even train a new model from scratch, but it isn’t common.</em></p><div><p>We will explore a few of the most exciting and valuable of these technologies, including:</p><ul><li>The ‘2vec’ methods</li><li>Sentence Transformers</li><li>Dense Passage Retrievers (DPR)</li><li>Vision Transformers (ViT)</li></ul><h2>Word2Vec</h2><p>Although we now have superior technologies for building embeddings, no overview on dense vectors would be complete without word2vec. Although <em>not</em> the first, it was the first widely used dense embedding model thanks to (1) being <em>very good</em>, and (2) the release of the <a rel=\"noopener noreferrer\" href=\"https://code.google.com/archive/p/word2vec/\" target=\"_blank\" id=\"reader.external-link.num-1\">word2vec toolkit</a> — allowing easy training or usage of pre-trained word2vec embeddings.</p><p>Given a sentence, word embeddings are created by taking a specific word (translated to a one-hot encoded vector) and mapping it to surrounding words through an encoder-decoder neural net.</p><figure><figcaption>The skip-gram approach to building dense vectors embeddings in word2vec.</figcaption></figure><p>This is the <em>skip-gram</em> version of word2vec, which, given a word <code>fox</code>, attempts to predict surrounding words (its context). After training, we discard the left and right blocks, keeping only the middle dense vector. This vector represents the word to the left of the diagram and can be used to embed this word for downstream language models.</p><p>We also have the <em>continuous bag of words (CBOW)</em>, which switches direction and aims to predict a word based on its context. This time we produce an embedding for the word on the right (in this case, still <code>fox</code>).</p><figure><figcaption>The continuous bag of words (CBOW) approach to building dense vector embeddings in word2vec.</figcaption></figure><p>Both skip-gram and CBOW are alike in that they produce a dense embedding vector from the middle <em>hidden layer</em> of the encoder-decoder network.</p><p>From this, Mikolov et al. produced the infamous <code>King - Man + Woman == Queen</code> example of vector arithmetic applied to language we saw earlier [2].</p><p>Word2vec spurred a flurry of advances in NLP. Still, when it came to representing longer chunks of text using single vectors — word2vec was useless. It allowed us to encode single words (or n-grams) but nothing more, meaning long chunks of text could only be represented by <em>many</em> vectors.</p><p>To compare longer chunks of text effectively, we need it to be represented by a single vector. Because of this limitation, several <em>extended</em> embedding methods quickly cropped up, such as sentence2vec and doc2vec.</p><p>Whether word2vec, sentence2vec, or even (batter|pitcher)2vec (representations of Major League Baseball players [3]), we now have vastly superior technologies for building these dense vectors. So although <em>‘2vec’</em> is where it started, we don’t often see them in use today.</p><h2>Sentence Similarity</h2><p>We’ve explored the beginnings of word-based embedding with word2vec and briefly touched on the other <em>2vecs</em> that popped up, aiming to apply this vector embedding approach to longer chunks of text.</p><p>We see this same evolution with transformer models. These models produce incredibly information-rich dense vectors, which can be used for a variety of applications from sentiment analysis to question-answering. Thanks to these rich embeddings, transformers have become the dominant modern-day language models.</p><p><a rel=\"noopener noreferrer\" href=\"https://www.pinecone.io/learn/semantic-search/\" target=\"_blank\" id=\"reader.external-link.num-2\">BERT</a> is perhaps the most famous of these transformer architectures (although the following applies to <em>most</em> transformer models).</p><p>Within BERT, we produce vector embeddings for each word (or <em>token</em>) similar to word2vec. However, embeddings are much richer thanks to much deeper networks — and we can even encode the <em>context</em> of words thanks to the attention mechanism.</p><p>The attention mechanism allows BERT to prioritize which context words should have the biggest impact on a specific embedding by considering the <em>alignment</em> of said context words (we can imagine it as BERT literally <em>paying attention</em> to specific words depending on the context).</p><p>What we mean by ‘context’ is, where word2vec would produce the same vector for ‘bank’ whether it was <em>“a grassy bank”</em> or <em>“the bank of England”</em> — BERT would instead modify the encoding for <em>bank</em> based on the surrounding context, thanks to the attention mechanism.</p><p>However, there is a problem here. We want to focus on comparing <em>sentences</em>, not words. And BERT embeddings are produced for each token. So this doesn’t help us in sentence-pair comparisons. What we need is a single vector that represents our sentences or paragraphs like sentence2vec.</p><p>The first transformer explicitly built for this was <em>Sentence-BERT (SBERT)</em>, a modified version of BERT [4].</p><p>BERT (and SBERT) use a <em>WordPiece</em> tokenizer — meaning that every word is equal to one <em>or more</em> tokens. SBERT allows us to create a single vector embedding for sequences containing no more than 128 tokens. Anything beyond this limit is cut.</p><p>This limit isn’t ideal for <em>long</em> pieces of text, but more than enough when comparing sentences or small-average-length paragraphs. And many of the latest models allow for longer sequence lengths too!</p><h2>Embedding With Sentence Transformers</h2><p>Let’s look at how we can quickly pull together some sentence embeddings using the <code>sentence-transformers</code> library [5]. First, we import the library and initialize a sentence transformer model from Microsoft called <code>all-mpnet-base-v2</code> (maximum sequence length of <code>384</code>).</p><figure></figure><p>Then we can go ahead and encode a few sentences, some more similar than others — while sharing <em>very few</em> matching words.</p><figure></figure><p>And what does our sentence transformer produce from these sentences? A 768-dimensional dense representation of our sentence. The performance of these embeddings when compared using a similarity metric such as cosine similarity is, in most cases — excellent.</p><figure></figure><p>Despite our most semantically similar sentences about bees and their queen sharing <em>zero</em> descriptive words, our model correctly embeds these sentences in the closest vector space when measured with cosine similarity!</p><h2>Question-Answering</h2><p>Another widespread use of transformer models is for questions and answers (Q&amp;A). Within Q&amp;A, there are several different <em>architectures</em> we can use. One of the most common is <em>open domain Q&amp;A (ODQA)</em>.</p><p>ODQA allows us to take a big set of sentences/paragraphs that contain answers to our questions (such as paragraphs from Wikipedia pages). We then ask a question to return a small chunk of one (or more) of those paragraphs which best answers our question.</p><p>When doing this, we are making use of three components or models:</p><ul><li>Some sort of <strong>database</strong> to store our sentence/paragraphs (called <em>contexts</em>).</li><li>A <strong>retriever</strong> retrieves contexts that it sees as similar to our question.</li><li>A <strong>reader</strong> model which extracts the <em>answer</em> from our related context(s).</li></ul><figure><figcaption>An example open domain question-answering (ODQA) architecture.</figcaption></figure><p>The <em>retriever</em> portion of this architecture is our focus here. Imagine we use a sentence-transformer model. Given a question, the retriever would return sentences most similar to our question — but we want answers <em>not</em> questions.</p><p>Instead, we want a model that can map question-answers pairs to the same point in vector space. So given the two sentences:</p><pre><span>\"What is the capital of France?\" AND \"The capital of France is Paris.\"</span></pre><p>We want a model that maps these two sentences to the same (or <em>very close</em>) vectors. And so when we receive a question <code>\"What is the capital of France?\"</code>, we want the output vector to have very high similarity to the vector representation of <code>\"The capital of France is Paris.\"</code> in our <a rel=\"noopener noreferrer\" href=\"https://www.pinecone.io/learn/vector-database/\" target=\"_blank\" id=\"reader.external-link.num-3\">vector database</a>.</p><p>The most popular model for this is Facebook AI’s <em>Dense Passage Retriever (DPR)</em>.</p><p>DPR consists of two smaller models — a <em>context</em> encoder and a <em>query</em> encoder. Again they’re both using the BERT architecture and are trained in parallel on question-answer pairs. We use a contrastive loss function, calculated as the difference between the two vectors output by each encoder [6].</p><figure><figcaption>Bi-encoder structure of DPR, we have both a <em>question encoder</em> and a <em>context encoder</em> — both are optimized to output the same (or close) embeddings for each question-context pair.</figcaption></figure><p>So when we give our question encoder <code>\"What is the capital of France?\"</code>, we would hope that the output vector would be similar to the vector output by our context encoder for <code>\"The capital of France is Paris.\"</code>.</p><p>We can’t rely on all of the question-answer relationships on having been seen during training. So when we input a new question such as <code>\"What is the capital of Australia?\"</code> our model might output a vector that we could think of as similar to <code>\"The capital of Australia is ___\"</code>. When we compare that to context embeddings in our database, this <em>should</em> be similar to <code>\"The capital of Australia is Canberra\"</code> (or so we hope).</p><h2>Fast DPR Setup</h2><p>Let’s take a quick look at building some context and query embeddings with DPR. We’ll be using the <code>transformers</code> library from Hugging Face.</p><p>First, we initialize tokenizers and models for both our context (<code>ctx</code>) model and <code>question</code> model.</p><figure></figure><p>Given a question and several contexts we tokenize and encode like so:</p><figure></figure><p><em>Note that we have included the questions within our contexts to confirm that the bi-encoder architecture is not just producing a straightforward semantic similarity operation as with sentence-transformers.</em></p><p>Now we can compare our query embeddings <code>xq</code> against all of our context embeddings <code>xb</code> to see which are the most similar with <em>cosine similarity</em>.</p><figure></figure><p>Out of our three questions, we returned two correct answers as the <em>very top</em> answer. It’s clear that DPR is not the <em>perfect</em> model, particularly when considering the simple nature of our questions and small dataset for DPR to retrieve from.</p><p>On the positive side however, in ODQA we would return many more contexts and allow a <em>reader</em> model to identify the best answers. Reader models can ‘re-rank’ contexts, so retrieving the top context immediately is not required to return the correct answer. If we were to retrieve the most relevant result 66% of the time, it would likely be a good result.</p><p>We can also see that despite hiding <em>exact matches</em> to our questions in the contexts, they interfered with only our last question, being correctly ignored by the first two questions.</p><h2>Vision Transformers</h2><p>Computer vision (CV) has become the stage for some exciting advances from transformer models — which have historically been restricted to NLP.</p><p>These advances look to make transformers the first widely adopted ML models that excel in both NLP <em>and</em> CV. And in the same way that we’ve been creating dense vectors representing language. We can do the same for images — and even encode images and text into the same vector space.</p><figure><figcaption>We can encode text and images to the same vector space using specific text and image encoders. Photo credit <a rel=\"noopener noreferrer\" href=\"https://unsplash.com/photos/T-0EW-SEbsE\" target=\"_blank\" id=\"reader.external-link.num-4\">Alvan Nee</a>.</figcaption></figure><p>The <a rel=\"noopener noreferrer\" href=\"https://www.pinecone.io/learn/vision-transformers/\" target=\"_blank\" id=\"reader.external-link.num-5\"><em>Vision Transformer (ViT)</em></a> was the first transformer applied to CV without the assistance of any upstream CNNs (as with VisualBERT [7]). The authors found that ViT can <em>sometimes</em> outperform state-of-the-art (SOTA) CNNs (the long-reigning masters of CV) [8].</p><p>These ViT transformers have been used alongside the more traditional language transformers to produce fascinating image and text encoders, as with <a rel=\"noopener noreferrer\" href=\"https://www.pinecone.io/learn/clip/\" target=\"_blank\" id=\"reader.external-link.num-6\">OpenAI’s CLIP</a> model [9].</p><p>The CLIP model uses two encoders like DPR, but this time we use a ViT model as our image encoder and a <em>masked self-attention</em> transformer like BERT for text [10]. As with DPR, these two models are trained in parallel and optimized via a contrastive loss function — producing <em>high similarity</em> vectors for image-text pairs.</p><p>That means that we can encode a set of images and then match those images to a caption of our choosing. And we can use the same encoding and cosine similarity logic we have used throughout the article. Let’s go ahead and try.</p><h2>Image-Text Embedding</h2><p>Let’s first get a few images to test. We will be using three images of dogs doing different things from Unsplash (links in the caption below).</p><figure></figure><p>We can initialize the CLIP <code>model</code> and <code>processor</code> using <code>transformers</code> from Hugging Face.</p><figure></figure><p>Now let’s create three true captions (plus some random) to describe our images and preprocess them through our <code>processor</code> before passing them on to our <code>model</code>. We will get output logits and use an <code>argmax</code> function to get our predictions.</p><figure></figure><p>And there, we have flawless image-to-text matching with CLIP! Of course, it is not perfect (our examples here are reasonably straightforward), but it produces some awe-inspiring results in no time at all.</p><p>Our model has dealt with comparing text and image embeddings. Still, if we wanted to extract those same embeddings used in the comparison, we access <code>outputs.text_embeds</code> and <code>outputs.image_embeds</code>.</p><figure></figure><p>And again, we can follow the same logic as we previously used with cosine similarity to find the closest matches. Let’s compare the embedding for <code>'a dog hiding behind a tree'</code> with our three images with this alternative approach.</p><figure></figure><p>As expected, we return the dog hiding behind a tree!</p></div></div>",
      "textContent": "Sparse vectors are called sparse because vectors are sparsely populated with information. Typically we would be looking at thousands of zeros to find a few ones (our relevant information). Consequently, these vectors can contain many dimensions, often in the tens of thousands.Where sparse vectors represent text syntax, we could view dense vectors as numerical representations of semantic meaning. Typically, we are taking words and encoding them into very dense, high-dimensional vectors. The abstract meaning and relationship of words are numerically encoded.Dense vectors are still highly dimensional (784-dimensions are common, but it can be more or less). However, each dimension contains relevant information, determined by a neural net — compressing these vectors is more complex, so they typically use more memory.We usually take an existing public model to generate vectors. For almost every scenario there is a high-performance model out there and it is easier, faster, and often much more accurate to use them. There are cases, for example for industry or language-specific embeddings where you sometimes need to fine-tune or even train a new model from scratch, but it isn’t common.We will explore a few of the most exciting and valuable of these technologies, including:The ‘2vec’ methodsSentence TransformersDense Passage Retrievers (DPR)Vision Transformers (ViT)Word2VecAlthough we now have superior technologies for building embeddings, no overview on dense vectors would be complete without word2vec. Although not the first, it was the first widely used dense embedding model thanks to (1) being very good, and (2) the release of the word2vec toolkit — allowing easy training or usage of pre-trained word2vec embeddings.Given a sentence, word embeddings are created by taking a specific word (translated to a one-hot encoded vector) and mapping it to surrounding words through an encoder-decoder neural net.The skip-gram approach to building dense vectors embeddings in word2vec.This is the skip-gram version of word2vec, which, given a word fox, attempts to predict surrounding words (its context). After training, we discard the left and right blocks, keeping only the middle dense vector. This vector represents the word to the left of the diagram and can be used to embed this word for downstream language models.We also have the continuous bag of words (CBOW), which switches direction and aims to predict a word based on its context. This time we produce an embedding for the word on the right (in this case, still fox).The continuous bag of words (CBOW) approach to building dense vector embeddings in word2vec.Both skip-gram and CBOW are alike in that they produce a dense embedding vector from the middle hidden layer of the encoder-decoder network.From this, Mikolov et al. produced the infamous King - Man + Woman == Queen example of vector arithmetic applied to language we saw earlier [2].Word2vec spurred a flurry of advances in NLP. Still, when it came to representing longer chunks of text using single vectors — word2vec was useless. It allowed us to encode single words (or n-grams) but nothing more, meaning long chunks of text could only be represented by many vectors.To compare longer chunks of text effectively, we need it to be represented by a single vector. Because of this limitation, several extended embedding methods quickly cropped up, such as sentence2vec and doc2vec.Whether word2vec, sentence2vec, or even (batter|pitcher)2vec (representations of Major League Baseball players [3]), we now have vastly superior technologies for building these dense vectors. So although ‘2vec’ is where it started, we don’t often see them in use today.Sentence SimilarityWe’ve explored the beginnings of word-based embedding with word2vec and briefly touched on the other 2vecs that popped up, aiming to apply this vector embedding approach to longer chunks of text.We see this same evolution with transformer models. These models produce incredibly information-rich dense vectors, which can be used for a variety of applications from sentiment analysis to question-answering. Thanks to these rich embeddings, transformers have become the dominant modern-day language models.BERT is perhaps the most famous of these transformer architectures (although the following applies to most transformer models).Within BERT, we produce vector embeddings for each word (or token) similar to word2vec. However, embeddings are much richer thanks to much deeper networks — and we can even encode the context of words thanks to the attention mechanism.The attention mechanism allows BERT to prioritize which context words should have the biggest impact on a specific embedding by considering the alignment of said context words (we can imagine it as BERT literally paying attention to specific words depending on the context).What we mean by ‘context’ is, where word2vec would produce the same vector for ‘bank’ whether it was “a grassy bank” or “the bank of England” — BERT would instead modify the encoding for bank based on the surrounding context, thanks to the attention mechanism.However, there is a problem here. We want to focus on comparing sentences, not words. And BERT embeddings are produced for each token. So this doesn’t help us in sentence-pair comparisons. What we need is a single vector that represents our sentences or paragraphs like sentence2vec.The first transformer explicitly built for this was Sentence-BERT (SBERT), a modified version of BERT [4].BERT (and SBERT) use a WordPiece tokenizer — meaning that every word is equal to one or more tokens. SBERT allows us to create a single vector embedding for sequences containing no more than 128 tokens. Anything beyond this limit is cut.This limit isn’t ideal for long pieces of text, but more than enough when comparing sentences or small-average-length paragraphs. And many of the latest models allow for longer sequence lengths too!Embedding With Sentence TransformersLet’s look at how we can quickly pull together some sentence embeddings using the sentence-transformers library [5]. First, we import the library and initialize a sentence transformer model from Microsoft called all-mpnet-base-v2 (maximum sequence length of 384).Then we can go ahead and encode a few sentences, some more similar than others — while sharing very few matching words.And what does our sentence transformer produce from these sentences? A 768-dimensional dense representation of our sentence. The performance of these embeddings when compared using a similarity metric such as cosine similarity is, in most cases — excellent.Despite our most semantically similar sentences about bees and their queen sharing zero descriptive words, our model correctly embeds these sentences in the closest vector space when measured with cosine similarity!Question-AnsweringAnother widespread use of transformer models is for questions and answers (Q&A). Within Q&A, there are several different architectures we can use. One of the most common is open domain Q&A (ODQA).ODQA allows us to take a big set of sentences/paragraphs that contain answers to our questions (such as paragraphs from Wikipedia pages). We then ask a question to return a small chunk of one (or more) of those paragraphs which best answers our question.When doing this, we are making use of three components or models:Some sort of database to store our sentence/paragraphs (called contexts).A retriever retrieves contexts that it sees as similar to our question.A reader model which extracts the answer from our related context(s).An example open domain question-answering (ODQA) architecture.The retriever portion of this architecture is our focus here. Imagine we use a sentence-transformer model. Given a question, the retriever would return sentences most similar to our question — but we want answers not questions.Instead, we want a model that can map question-answers pairs to the same point in vector space. So given the two sentences:\"What is the capital of France?\" AND \"The capital of France is Paris.\"We want a model that maps these two sentences to the same (or very close) vectors. And so when we receive a question \"What is the capital of France?\", we want the output vector to have very high similarity to the vector representation of \"The capital of France is Paris.\" in our vector database.The most popular model for this is Facebook AI’s Dense Passage Retriever (DPR).DPR consists of two smaller models — a context encoder and a query encoder. Again they’re both using the BERT architecture and are trained in parallel on question-answer pairs. We use a contrastive loss function, calculated as the difference between the two vectors output by each encoder [6].Bi-encoder structure of DPR, we have both a question encoder and a context encoder — both are optimized to output the same (or close) embeddings for each question-context pair.So when we give our question encoder \"What is the capital of France?\", we would hope that the output vector would be similar to the vector output by our context encoder for \"The capital of France is Paris.\".We can’t rely on all of the question-answer relationships on having been seen during training. So when we input a new question such as \"What is the capital of Australia?\" our model might output a vector that we could think of as similar to \"The capital of Australia is ___\". When we compare that to context embeddings in our database, this should be similar to \"The capital of Australia is Canberra\" (or so we hope).Fast DPR SetupLet’s take a quick look at building some context and query embeddings with DPR. We’ll be using the transformers library from Hugging Face.First, we initialize tokenizers and models for both our context (ctx) model and question model.Given a question and several contexts we tokenize and encode like so:Note that we have included the questions within our contexts to confirm that the bi-encoder architecture is not just producing a straightforward semantic similarity operation as with sentence-transformers.Now we can compare our query embeddings xq against all of our context embeddings xb to see which are the most similar with cosine similarity.Out of our three questions, we returned two correct answers as the very top answer. It’s clear that DPR is not the perfect model, particularly when considering the simple nature of our questions and small dataset for DPR to retrieve from.On the positive side however, in ODQA we would return many more contexts and allow a reader model to identify the best answers. Reader models can ‘re-rank’ contexts, so retrieving the top context immediately is not required to return the correct answer. If we were to retrieve the most relevant result 66% of the time, it would likely be a good result.We can also see that despite hiding exact matches to our questions in the contexts, they interfered with only our last question, being correctly ignored by the first two questions.Vision TransformersComputer vision (CV) has become the stage for some exciting advances from transformer models — which have historically been restricted to NLP.These advances look to make transformers the first widely adopted ML models that excel in both NLP and CV. And in the same way that we’ve been creating dense vectors representing language. We can do the same for images — and even encode images and text into the same vector space.We can encode text and images to the same vector space using specific text and image encoders. Photo credit Alvan Nee.The Vision Transformer (ViT) was the first transformer applied to CV without the assistance of any upstream CNNs (as with VisualBERT [7]). The authors found that ViT can sometimes outperform state-of-the-art (SOTA) CNNs (the long-reigning masters of CV) [8].These ViT transformers have been used alongside the more traditional language transformers to produce fascinating image and text encoders, as with OpenAI’s CLIP model [9].The CLIP model uses two encoders like DPR, but this time we use a ViT model as our image encoder and a masked self-attention transformer like BERT for text [10]. As with DPR, these two models are trained in parallel and optimized via a contrastive loss function — producing high similarity vectors for image-text pairs.That means that we can encode a set of images and then match those images to a caption of our choosing. And we can use the same encoding and cosine similarity logic we have used throughout the article. Let’s go ahead and try.Image-Text EmbeddingLet’s first get a few images to test. We will be using three images of dogs doing different things from Unsplash (links in the caption below).We can initialize the CLIP model and processor using transformers from Hugging Face.Now let’s create three true captions (plus some random) to describe our images and preprocess them through our processor before passing them on to our model. We will get output logits and use an argmax function to get our predictions.And there, we have flawless image-to-text matching with CLIP! Of course, it is not perfect (our examples here are reasonably straightforward), but it produces some awe-inspiring results in no time at all.Our model has dealt with comparing text and image embeddings. Still, if we wanted to extract those same embeddings used in the comparison, we access outputs.text_embeds and outputs.image_embeds.And again, we can follow the same logic as we previously used with cosine similarity to find the closest matches. Let’s compare the embedding for 'a dog hiding behind a tree' with our three images with this alternative approach.As expected, we return the dog hiding behind a tree!",
      "length": 13636,
      "excerpt": "Sparse vectors are called sparse because vectors are sparsely populated with information. Typically we would be looking at thousands of zeros to find a few ones (our relevant information). Consequently, these vectors can contain many dimensions, often in the tens of thousands.",
      "siteName": null,
      "summary": "Dense vectors are highly dimensional and contain relevant information, determined by a neural net. We can view dense vectors as numerical representations of semantic meaning. For almost every scenario there is a high-performance model out there and it is easier, faster, and often much more accurate to use them.",
      "topics": [
        {
          "label": "Science & Mathematics",
          "score": 0.9941056966781616
        },
        {
          "label": "Computers & Internet",
          "score": 0.9165694117546082
        },
        {
          "label": "Education & Reference",
          "score": 0.8272539377212524
        }
      ]
    }
  },
  "3774539706": {
    "item_id": "3774539706",
    "resolved_id": "3774539706",
    "given_url": "https://www.philschmid.de/fine-tune-flan-t5",
    "given_title": "Fine-tune FLAN-T5 for chat & dialogue summarization",
    "favorite": "0",
    "status": "0",
    "time_added": "1676633045",
    "time_updated": "1676633048",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 1,
    "resolved_title": "Fine-tune FLAN-T5 for chat & dialogue summarization",
    "resolved_url": "https://www.philschmid.de/fine-tune-flan-t5",
    "excerpt": "In this blog, you will learn how to fine-tune google/flan-t5-base for chat & dialogue summarization using Hugging Face Transformers. If you already know T5, FLAN-T5 is just better at everything.",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "0",
    "word_count": "1884",
    "lang": "en",
    "time_to_read": 9,
    "top_image_url": "https://www.philschmid.de/static/blog/fine-tune-flan-t5/thumbnail.png",
    "tags": {
      "readinglist": {
        "item_id": "3774539706",
        "tag": "readinglist"
      }
    },
    "listen_duration_estimate": 729,
    "content": {
      "title": "Fine-tune FLAN-T5 for chat & dialogue summarization",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><div lang=\"en\">\n<p>In this blog, you will learn how to fine-tune <a href=\"https://huggingface.co/google/flan-t5-base\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-0\">google/flan-t5-base</a> for chat &amp; dialogue summarization using Hugging Face Transformers. If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages.</p>\n<p>In this example we will use the <a href=\"https://huggingface.co/datasets/samsum\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-1\">samsum</a> dataset a collection of about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English.</p>\n<p>You will learn how to:</p>\n<p>Before we can start, make sure you have a <a href=\"https://huggingface.co/join\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-2\">Hugging Face Account</a> to save artifacts and experiments.</p>\n<h2>Quick intro: FLAN-T5, just a better T5</h2>\n<p>FLAN-T5 released with the <a href=\"https://arxiv.org/pdf/2210.11416.pdf\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-4\">Scaling Instruction-Finetuned Language Models</a> paper is an enhanced version of T5 that has been finetuned in a mixture of tasks. The paper explores instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. The paper discovers that overall instruction finetuning is a general method for improving the performance and usability of pretrained language models.</p>\n\n<hr><p>Now we know what FLAN-T5 is, let's get started. 🚀</p>\n<p><em>Note: This tutorial was created and run on a g4dn.xlarge AWS EC2 Instance including a NVIDIA T4.</em></p>\n<h2>1. Setup Development Environment</h2>\n<p>Our first step is to install the Hugging Face Libraries, including transformers and datasets. Running the following cell will install all the required packages.</p>\n<div>\n<pre><code><span>!pip install pytesseract transformers datasets rouge<span>-</span>score nltk tensorboard py7zr <span>-</span><span>-</span>upgrade\n</span><span><span># install git-fls for pushing model and logs to the hugging face hub</span>\n</span><span>!sudo apt<span>-</span>get install git<span>-</span>lfs <span>-</span><span>-</span>yes\n</span></code></pre></div>\n<p>This example will use the <a href=\"https://huggingface.co/models\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-6\">Hugging Face Hub</a> as a remote model versioning service. To be able to push our model to the Hub, you need to register on the <a href=\"https://huggingface.co/join\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-7\">Hugging Face</a>. If you already have an account, you can skip this step. After you have an account, we will use the <code>notebook_login</code> util from the <code>huggingface_hub</code> package to log into our account and store our token (access key) on the disk.</p>\n<div>\n<pre><code><span><span>from</span> huggingface_hub <span>import</span> notebook_login\n</span><span>\n</span><span>notebook_login<span>(</span><span>)</span>\n</span></code></pre></div>\n<h2>2. Load and prepare samsum dataset</h2>\n<p>we will use the <a href=\"https://huggingface.co/datasets/samsum\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-9\">samsum</a> dataset a collection of about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English.</p>\n<div>\n<pre><code><span><span>{</span>\n</span><span>  <span>\"id\"</span><span>:</span> <span>\"13818513\"</span><span>,</span>\n</span><span>  <span>\"summary\"</span><span>:</span> <span>\"Amanda baked cookies and will bring Jerry some tomorrow.\"</span><span>,</span>\n</span><span>  <span>\"dialogue\"</span><span>:</span> <span>\"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"</span>\n</span><span><span>}</span>\n</span></code></pre></div>\n\n<p>To load the <code>samsum</code> dataset, we use the <code>load_dataset()</code> method from the 🤗 Datasets library.</p>\n<div>\n<pre><code><span><span>from</span> datasets <span>import</span> load_dataset\n</span><span>\n</span><span><span># Load dataset from the hub</span>\n</span><span>dataset <span>=</span> load_dataset<span>(</span>dataset_id<span>)</span>\n</span><span>\n</span><span><span>print</span><span>(</span><span><span>f\"Train dataset size: </span><span><span>{</span><span>len</span><span>(</span>dataset<span>[</span><span>'train'</span><span>]</span><span>)</span><span>}</span></span><span>\"</span></span><span>)</span>\n</span><span><span>print</span><span>(</span><span><span>f\"Test dataset size: </span><span><span>{</span><span>len</span><span>(</span>dataset<span>[</span><span>'test'</span><span>]</span><span>)</span><span>}</span></span><span>\"</span></span><span>)</span>\n</span><span>\n</span><span><span># Train dataset size: 14732</span>\n</span><span><span># Test dataset size: 819</span>\n</span></code></pre></div>\n<p>Lets checkout an example of the dataset.</p>\n<div>\n<pre><code><span><span>from</span> random <span>import</span> randrange\n</span><span>\n</span><span>\n</span><span>sample <span>=</span> dataset<span>[</span><span>'train'</span><span>]</span><span>[</span>randrange<span>(</span><span>len</span><span>(</span>dataset<span>[</span><span>\"train\"</span><span>]</span><span>)</span><span>)</span><span>]</span>\n</span><span><span>print</span><span>(</span><span><span>f\"dialogue: \\n</span><span><span>{</span>sample<span>[</span><span>'dialogue'</span><span>]</span><span>}</span></span><span>\\n---------------\"</span></span><span>)</span>\n</span><span><span>print</span><span>(</span><span><span>f\"summary: \\n</span><span><span>{</span>sample<span>[</span><span>'summary'</span><span>]</span><span>}</span></span><span>\\n---------------\"</span></span><span>)</span>\n</span></code></pre></div>\n<p>To train our model we need to convert our inputs (text) to token IDs. This is done by a 🤗 Transformers Tokenizer. If you are not sure what this means check out <a href=\"https://huggingface.co/course/chapter6/1?fw=tf\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-10\">chapter 6</a> of the Hugging Face Course.</p>\n<div>\n<pre><code><span><span>from</span> transformers <span>import</span> AutoTokenizer<span>,</span> AutoModelForSeq2SeqLM\n</span><span>\n</span><span>model_id<span>=</span><span>\"google/flan-t5-base\"</span>\n</span><span>\n</span><span><span># Load tokenizer of FLAN-t5-base</span>\n</span><span>tokenizer <span>=</span> AutoTokenizer<span>.</span>from_pretrained<span>(</span>model_id<span>)</span>\n</span><span>\n</span></code></pre></div>\n<p>before we can start training we need to preprocess our data. Abstractive Summarization is a text2text-generation task. This means our model will take a text as input and generate a summary as output. For this we want to understand how long our input and output will be to be able to efficiently batch our data.</p>\n<div>\n<pre><code><span><span>from</span> datasets <span>import</span> concatenate_datasets\n</span><span>\n</span><span><span># The maximum total input sequence length after tokenization.</span>\n</span><span><span># Sequences longer than this will be truncated, sequences shorter will be padded.</span>\n</span><span>tokenized_inputs <span>=</span> concatenate_datasets<span>(</span><span>[</span>dataset<span>[</span><span>\"train\"</span><span>]</span><span>,</span> dataset<span>[</span><span>\"test\"</span><span>]</span><span>]</span><span>)</span><span>.</span><span>map</span><span>(</span><span>lambda</span> x<span>:</span> tokenizer<span>(</span>x<span>[</span><span>\"dialogue\"</span><span>]</span><span>,</span> truncation<span>=</span><span>True</span><span>)</span><span>,</span> batched<span>=</span><span>True</span><span>,</span> remove_columns<span>=</span><span>[</span><span>\"dialogue\"</span><span>,</span> <span>\"summary\"</span><span>]</span><span>)</span>\n</span><span>max_source_length <span>=</span> <span>max</span><span>(</span><span>[</span><span>len</span><span>(</span>x<span>)</span> <span>for</span> x <span>in</span> tokenized_inputs<span>[</span><span>\"input_ids\"</span><span>]</span><span>]</span><span>)</span>\n</span><span><span>print</span><span>(</span><span><span>f\"Max source length: </span><span><span>{</span>max_source_length<span>}</span></span><span>\"</span></span><span>)</span>\n</span><span>\n</span><span><span># The maximum total sequence length for target text after tokenization.</span>\n</span><span><span># Sequences longer than this will be truncated, sequences shorter will be padded.\"</span>\n</span><span>tokenized_targets <span>=</span> concatenate_datasets<span>(</span><span>[</span>dataset<span>[</span><span>\"train\"</span><span>]</span><span>,</span> dataset<span>[</span><span>\"test\"</span><span>]</span><span>]</span><span>)</span><span>.</span><span>map</span><span>(</span><span>lambda</span> x<span>:</span> tokenizer<span>(</span>x<span>[</span><span>\"summary\"</span><span>]</span><span>,</span> truncation<span>=</span><span>True</span><span>)</span><span>,</span> batched<span>=</span><span>True</span><span>,</span> remove_columns<span>=</span><span>[</span><span>\"dialogue\"</span><span>,</span> <span>\"summary\"</span><span>]</span><span>)</span>\n</span><span>max_target_length <span>=</span> <span>max</span><span>(</span><span>[</span><span>len</span><span>(</span>x<span>)</span> <span>for</span> x <span>in</span> tokenized_targets<span>[</span><span>\"input_ids\"</span><span>]</span><span>]</span><span>)</span>\n</span><span><span>print</span><span>(</span><span><span>f\"Max target length: </span><span><span>{</span>max_target_length<span>}</span></span><span>\"</span></span><span>)</span>\n</span></code></pre></div>\n<div>\n<pre><code><span><span>def</span> <span>preprocess_function</span><span>(</span>sample<span>,</span>padding<span>=</span><span>\"max_length\"</span><span>)</span><span>:</span>\n</span><span>    <span># add prefix to the input for t5</span>\n</span><span>    inputs <span>=</span> <span>[</span><span>\"summarize: \"</span> <span>+</span> item <span>for</span> item <span>in</span> sample<span>[</span><span>\"dialogue\"</span><span>]</span><span>]</span>\n</span><span>\n</span><span>    <span># tokenize inputs</span>\n</span><span>    model_inputs <span>=</span> tokenizer<span>(</span>inputs<span>,</span> max_length<span>=</span>max_source_length<span>,</span> padding<span>=</span>padding<span>,</span> truncation<span>=</span><span>True</span><span>)</span>\n</span><span>\n</span><span>    <span># Tokenize targets with the `text_target` keyword argument</span>\n</span><span>    labels <span>=</span> tokenizer<span>(</span>text_target<span>=</span>sample<span>[</span><span>\"summary\"</span><span>]</span><span>,</span> max_length<span>=</span>max_target_length<span>,</span> padding<span>=</span>padding<span>,</span> truncation<span>=</span><span>True</span><span>)</span>\n</span><span>\n</span><span>    <span># padding in the loss.</span>\n</span><span>    <span>if</span> padding <span>==</span> <span>\"max_length\"</span><span>:</span>\n</span><span>        labels<span>[</span><span>\"input_ids\"</span><span>]</span> <span>=</span> <span>[</span>\n</span><span>            <span>[</span><span>(</span>l <span>if</span> l <span>!=</span> tokenizer<span>.</span>pad_token_id <span>else</span> <span>-</span><span>100</span><span>)</span> <span>for</span> l <span>in</span> label<span>]</span> <span>for</span> label <span>in</span> labels<span>[</span><span>\"input_ids\"</span><span>]</span>\n</span><span>        <span>]</span>\n</span><span>\n</span><span>    model_inputs<span>[</span><span>\"labels\"</span><span>]</span> <span>=</span> labels<span>[</span><span>\"input_ids\"</span><span>]</span>\n</span><span>    <span>return</span> model_inputs\n</span><span>\n</span><span>tokenized_dataset <span>=</span> dataset<span>.</span><span>map</span><span>(</span>preprocess_function<span>,</span> batched<span>=</span><span>True</span><span>,</span> remove_columns<span>=</span><span>[</span><span>\"dialogue\"</span><span>,</span> <span>\"summary\"</span><span>,</span> <span>\"id\"</span><span>]</span><span>)</span>\n</span><span><span>print</span><span>(</span><span><span>f\"Keys of tokenized dataset: </span><span><span>{</span><span>list</span><span>(</span>tokenized_dataset<span>[</span><span>'train'</span><span>]</span><span>.</span>features<span>)</span><span>}</span></span><span>\"</span></span><span>)</span>\n</span></code></pre></div>\n<h2>3. Fine-tune and evaluate FLAN-T5</h2>\n<p>After we have processed our dataset, we can start training our model. Therefore we first need to load our <a href=\"https://huggingface.co/models?search=flan-t5\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-12\">FLAN-T5</a> from the Hugging Face Hub. In the example we are using a instance with a NVIDIA V100 meaning that we will fine-tune the <code>base</code> version of the model. <em>I plan to do a follow-up post on how to fine-tune the <code>xxl</code> version of the model using Deepspeed.</em></p>\n<div>\n<pre><code><span><span>from</span> transformers <span>import</span> AutoModelForSeq2SeqLM\n</span><span>\n</span><span><span># huggingface hub model id</span>\n</span><span>model_id<span>=</span><span>\"google/flan-t5-base\"</span>\n</span><span>\n</span><span><span># load model from the hub</span>\n</span><span>model <span>=</span> AutoModelForSeq2SeqLM<span>.</span>from_pretrained<span>(</span>model_id<span>)</span>\n</span></code></pre></div>\n<p>We want to evaluate our model during training. The <code>Trainer</code> supports evaluation during training by providing a <code>compute_metrics</code>.   <br>\nThe most commonly used metrics to evaluate summarization task is <a href=\"https://en.wikipedia.org/wiki/ROUGE_(metric)\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-13\">rogue_score</a> short for Recall-Oriented Understudy for Gisting Evaluation). This metric does not behave like the standard accuracy: it will compare a generated summary against a set of reference summaries</p>\n<p>We are going to use <code>evaluate</code> library to evaluate the <code>rogue</code> score.</p>\n<div>\n<pre><code><span><span>import</span> evaluate\n</span><span><span>import</span> nltk\n</span><span><span>import</span> numpy <span>as</span> np\n</span><span><span>from</span> nltk<span>.</span>tokenize <span>import</span> sent_tokenize\n</span><span>nltk<span>.</span>download<span>(</span><span>\"punkt\"</span><span>)</span>\n</span><span>\n</span><span><span># Metric</span>\n</span><span>metric <span>=</span> evaluate<span>.</span>load<span>(</span><span>\"rouge\"</span><span>)</span>\n</span><span>\n</span><span><span># helper function to postprocess text</span>\n</span><span><span>def</span> <span>postprocess_text</span><span>(</span>preds<span>,</span> labels<span>)</span><span>:</span>\n</span><span>    preds <span>=</span> <span>[</span>pred<span>.</span>strip<span>(</span><span>)</span> <span>for</span> pred <span>in</span> preds<span>]</span>\n</span><span>    labels <span>=</span> <span>[</span>label<span>.</span>strip<span>(</span><span>)</span> <span>for</span> label <span>in</span> labels<span>]</span>\n</span><span>\n</span><span>    <span># rougeLSum expects newline after each sentence</span>\n</span><span>    preds <span>=</span> <span>[</span><span>\"\\n\"</span><span>.</span>join<span>(</span>sent_tokenize<span>(</span>pred<span>)</span><span>)</span> <span>for</span> pred <span>in</span> preds<span>]</span>\n</span><span>    labels <span>=</span> <span>[</span><span>\"\\n\"</span><span>.</span>join<span>(</span>sent_tokenize<span>(</span>label<span>)</span><span>)</span> <span>for</span> label <span>in</span> labels<span>]</span>\n</span><span>\n</span><span>    <span>return</span> preds<span>,</span> labels\n</span><span>\n</span><span><span>def</span> <span>compute_metrics</span><span>(</span>eval_preds<span>)</span><span>:</span>\n</span><span>    preds<span>,</span> labels <span>=</span> eval_preds\n</span><span>    <span>if</span> <span>isinstance</span><span>(</span>preds<span>,</span> <span>tuple</span><span>)</span><span>:</span>\n</span><span>        preds <span>=</span> preds<span>[</span><span>0</span><span>]</span>\n</span><span>    decoded_preds <span>=</span> tokenizer<span>.</span>batch_decode<span>(</span>preds<span>,</span> skip_special_tokens<span>=</span><span>True</span><span>)</span>\n</span><span>    <span># Replace -100 in the labels as we can't decode them.</span>\n</span><span>    labels <span>=</span> np<span>.</span>where<span>(</span>labels <span>!=</span> <span>-</span><span>100</span><span>,</span> labels<span>,</span> tokenizer<span>.</span>pad_token_id<span>)</span>\n</span><span>    decoded_labels <span>=</span> tokenizer<span>.</span>batch_decode<span>(</span>labels<span>,</span> skip_special_tokens<span>=</span><span>True</span><span>)</span>\n</span><span>\n</span><span>    <span># Some simple post-processing</span>\n</span><span>    decoded_preds<span>,</span> decoded_labels <span>=</span> postprocess_text<span>(</span>decoded_preds<span>,</span> decoded_labels<span>)</span>\n</span><span>\n</span><span>    result <span>=</span> metric<span>.</span>compute<span>(</span>predictions<span>=</span>decoded_preds<span>,</span> references<span>=</span>decoded_labels<span>,</span> use_stemmer<span>=</span><span>True</span><span>)</span>\n</span><span>    result <span>=</span> <span>{</span>k<span>:</span> <span>round</span><span>(</span>v <span>*</span> <span>100</span><span>,</span> <span>4</span><span>)</span> <span>for</span> k<span>,</span> v <span>in</span> result<span>.</span>items<span>(</span><span>)</span><span>}</span>\n</span><span>    prediction_lens <span>=</span> <span>[</span>np<span>.</span>count_nonzero<span>(</span>pred <span>!=</span> tokenizer<span>.</span>pad_token_id<span>)</span> <span>for</span> pred <span>in</span> preds<span>]</span>\n</span><span>    result<span>[</span><span>\"gen_len\"</span><span>]</span> <span>=</span> np<span>.</span>mean<span>(</span>prediction_lens<span>)</span>\n</span><span>    <span>return</span> result\n</span></code></pre></div>\n<p>Before we can start training is to create a <code>DataCollator</code> that will take care of padding our inputs and labels. We will use the <code>DataCollatorForSeq2Seq</code> from the 🤗 Transformers library.</p>\n<div>\n<pre><code><span><span>from</span> transformers <span>import</span> DataCollatorForSeq2Seq\n</span><span>\n</span><span><span># we want to ignore tokenizer pad token in the loss</span>\n</span><span>label_pad_token_id <span>=</span> <span>-</span><span>100</span>\n</span><span><span># Data collator</span>\n</span><span>data_collator <span>=</span> DataCollatorForSeq2Seq<span>(</span>\n</span><span>    tokenizer<span>,</span>\n</span><span>    model<span>=</span>model<span>,</span>\n</span><span>    label_pad_token_id<span>=</span>label_pad_token_id<span>,</span>\n</span><span>    pad_to_multiple_of<span>=</span><span>8</span>\n</span><span><span>)</span>\n</span><span>\n</span></code></pre></div>\n<p>The last step is to define the hyperparameters (<code>TrainingArguments</code>) we want to use for our training. We are leveraging the <a href=\"https://huggingface.co/models\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-14\">Hugging Face Hub</a> integration of the <code>Trainer</code> to automatically push our checkpoints, logs and metrics during training into a repository.</p>\n<div>\n<pre><code><span><span>from</span> huggingface_hub <span>import</span> HfFolder\n</span><span><span>from</span> transformers <span>import</span> Seq2SeqTrainer<span>,</span> Seq2SeqTrainingArguments\n</span><span>\n</span><span><span># Hugging Face repository id</span>\n</span><span>repository_id <span>=</span> <span><span>f\"</span><span><span>{</span>model_id<span>.</span>split<span>(</span><span>'/'</span><span>)</span><span>[</span><span>1</span><span>]</span><span>}</span></span><span>-</span><span><span>{</span>dataset_id<span>}</span></span><span>\"</span></span>\n</span><span>\n</span><span><span># Define training args</span>\n</span><span>training_args <span>=</span> Seq2SeqTrainingArguments<span>(</span>\n</span><span>    output_dir<span>=</span>repository_id<span>,</span>\n</span><span>    per_device_train_batch_size<span>=</span><span>8</span><span>,</span>\n</span><span>    per_device_eval_batch_size<span>=</span><span>8</span><span>,</span>\n</span><span>    predict_with_generate<span>=</span><span>True</span><span>,</span>\n</span><span>    fp16<span>=</span><span>False</span><span>,</span> <span># Overflows with fp16</span>\n</span><span>    learning_rate<span>=</span><span>5e-5</span><span>,</span>\n</span><span>    num_train_epochs<span>=</span><span>5</span><span>,</span>\n</span><span>    <span># logging &amp; evaluation strategies</span>\n</span><span>    logging_dir<span>=</span><span><span>f\"</span><span><span>{</span>repository_id<span>}</span></span><span>/logs\"</span></span><span>,</span>\n</span><span>    logging_strategy<span>=</span><span>\"steps\"</span><span>,</span>\n</span><span>    logging_steps<span>=</span><span>500</span><span>,</span>\n</span><span>    evaluation_strategy<span>=</span><span>\"epoch\"</span><span>,</span>\n</span><span>    save_strategy<span>=</span><span>\"epoch\"</span><span>,</span>\n</span><span>    save_total_limit<span>=</span><span>2</span><span>,</span>\n</span><span>    load_best_model_at_end<span>=</span><span>True</span><span>,</span>\n</span><span>    <span># metric_for_best_model=\"overall_f1\",</span>\n</span><span>    <span># push to hub parameters</span>\n</span><span>    report_to<span>=</span><span>\"tensorboard\"</span><span>,</span>\n</span><span>    push_to_hub<span>=</span><span>False</span><span>,</span>\n</span><span>    hub_strategy<span>=</span><span>\"every_save\"</span><span>,</span>\n</span><span>    hub_model_id<span>=</span>repository_id<span>,</span>\n</span><span>    hub_token<span>=</span>HfFolder<span>.</span>get_token<span>(</span><span>)</span><span>,</span>\n</span><span><span>)</span>\n</span><span>\n</span><span><span># Create Trainer instance</span>\n</span><span>trainer <span>=</span> Seq2SeqTrainer<span>(</span>\n</span><span>    model<span>=</span>model<span>,</span>\n</span><span>    args<span>=</span>training_args<span>,</span>\n</span><span>    train_dataset<span>=</span>tokenized_dataset<span>[</span><span>\"train\"</span><span>]</span><span>,</span>\n</span><span>    eval_dataset<span>=</span>tokenized_dataset<span>[</span><span>\"test\"</span><span>]</span><span>,</span>\n</span><span>    compute_metrics<span>=</span>compute_metrics<span>,</span>\n</span><span><span>)</span>\n</span></code></pre></div>\n<p>We can start our training by using the <code>train</code> method of the <code>Trainer</code>.</p>\n<div>\n<pre><code><span><span># Start training</span>\n</span><span>trainer<span>.</span>train<span>(</span><span>)</span>\n</span></code></pre></div>\n\n<p>Nice, we have trained our model. 🎉 Lets run evaluate the best model again on the test set.</p>\n\n<p>The best score we achieved is an <code>rouge1</code> score of <code>47.23</code>.</p>\n<p>Lets save our results and tokenizer to the Hugging Face Hub and create a model card.</p>\n<div>\n<pre><code><span><span># Save our tokenizer and create model card</span>\n</span><span>tokenizer<span>.</span>save_pretrained<span>(</span>repository_id<span>)</span>\n</span><span>trainer<span>.</span>create_model_card<span>(</span><span>)</span>\n</span><span><span># Push the results to the hub</span>\n</span><span>trainer<span>.</span>push_to_hub<span>(</span><span>)</span>\n</span></code></pre></div>\n<h2>4. Run Inference and summarize ChatGPT dialogues</h2>\n<p>Now we have a trained model, we can use it to run inference. We will use the <code>pipeline</code> API from transformers and a <code>test</code> example from our dataset.</p>\n<div>\n<pre><code><span><span>from</span> transformers <span>import</span> pipeline\n</span><span><span>from</span> random <span>import</span> randrange\n</span><span>\n</span><span><span># load model and tokenizer from huggingface hub with pipeline</span>\n</span><span>summarizer <span>=</span> pipeline<span>(</span><span>\"summarization\"</span><span>,</span> model<span>=</span><span>\"philschmid/flan-t5-base-samsum\"</span><span>,</span> device<span>=</span><span>0</span><span>)</span>\n</span><span>\n</span><span><span># select a random test sample</span>\n</span><span>sample <span>=</span> dataset<span>[</span><span>'test'</span><span>]</span><span>[</span>randrange<span>(</span><span>len</span><span>(</span>dataset<span>[</span><span>\"test\"</span><span>]</span><span>)</span><span>)</span><span>]</span>\n</span><span><span>print</span><span>(</span><span><span>f\"dialogue: \\n</span><span><span>{</span>sample<span>[</span><span>'dialogue'</span><span>]</span><span>}</span></span><span>\\n---------------\"</span></span><span>)</span>\n</span><span>\n</span><span><span># summarize dialogue</span>\n</span><span>res <span>=</span> summarizer<span>(</span>sample<span>[</span><span>\"dialogue\"</span><span>]</span><span>)</span>\n</span><span>\n</span><span><span>print</span><span>(</span><span><span>f\"flan-t5-base summary:\\n</span><span><span>{</span>res<span>[</span><span>0</span><span>]</span><span>[</span><span>'summary_text'</span><span>]</span><span>}</span></span><span>\"</span></span><span>)</span>\n</span></code></pre></div>\n<p>output</p>\n<div>\n<pre><code><span>    dialogue:\n</span><span>    Abby: Have you talked to Miro?\n</span><span>    Dylan: No, not really, I<span>'ve never had an opportunity\n</span></span><span>    Brandon: me neither, but he seems a nice guy\n</span><span>    Brenda: you met him yesterday at the party?\n</span><span><span>    Abby: yes, he'</span>s so interesting\n</span><span>    Abby: told me the story of his father coming from Albania to the US <span>in</span> the early 1990s\n</span><span>    Dylan: really, I had no idea he is Albanian\n</span><span>    Abby: he is, he speaks only Albanian with his parents\n</span><span>    Dylan: fascinating, where does he come from <span>in</span> Albania?\n</span><span>    Abby: from the seacoast\n</span><span>    Abby: Duress I believe, he told me they are not from Tirana\n</span><span>    Dylan: what <span>else</span> did he tell you?\n</span><span>    Abby: That they left kind of illegally\n</span><span>    Abby: it was a big mess and extreme poverty everywhere\n</span><span>    Abby: <span>then</span> suddenly the border was <span>open</span> and they just left\n</span><span>    Abby: people were boarding available ships, whatever, just to get out of there\n</span><span>    Abby: he showed me some pictures, like <span>&lt;</span>file_photo<span>&gt;</span>\n</span><span>    Dylan: insane\n</span><span>    Abby: yes, and his father was among the people\n</span><span>    Dylan: scary but interesting\n</span><span>    Abby: very<span>!</span>\n</span><span>    ---------------\n</span><span>    flan-t5-base summary:\n</span><span>    Abby met Miro yesterday at the party. Miro's father came from Albania to the US <span>in</span> the early 1990s. He speaks Albanian with his parents. The border was <span>open</span> and people were boarding ships to get out of there.\n</span></code></pre></div>\n<hr><p>Thanks for reading! If you have any questions, feel free to contact me on <a href=\"https://twitter.com/_philschmid\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-16\">Twitter</a> or <a href=\"https://www.linkedin.com/in/philipp-schmid-a6a2bb196/\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-17\">LinkedIn</a>.</p>\n</div></article></div>",
      "textContent": "\nIn this blog, you will learn how to fine-tune google/flan-t5-base for chat & dialogue summarization using Hugging Face Transformers. If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages.\nIn this example we will use the samsum dataset a collection of about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English.\nYou will learn how to:\nBefore we can start, make sure you have a Hugging Face Account to save artifacts and experiments.\nQuick intro: FLAN-T5, just a better T5\nFLAN-T5 released with the Scaling Instruction-Finetuned Language Models paper is an enhanced version of T5 that has been finetuned in a mixture of tasks. The paper explores instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. The paper discovers that overall instruction finetuning is a general method for improving the performance and usability of pretrained language models.\n\nNow we know what FLAN-T5 is, let's get started. 🚀\nNote: This tutorial was created and run on a g4dn.xlarge AWS EC2 Instance including a NVIDIA T4.\n1. Setup Development Environment\nOur first step is to install the Hugging Face Libraries, including transformers and datasets. Running the following cell will install all the required packages.\n\n!pip install pytesseract transformers datasets rouge-score nltk tensorboard py7zr --upgrade\n# install git-fls for pushing model and logs to the hugging face hub\n!sudo apt-get install git-lfs --yes\n\nThis example will use the Hugging Face Hub as a remote model versioning service. To be able to push our model to the Hub, you need to register on the Hugging Face. If you already have an account, you can skip this step. After you have an account, we will use the notebook_login util from the huggingface_hub package to log into our account and store our token (access key) on the disk.\n\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\n2. Load and prepare samsum dataset\nwe will use the samsum dataset a collection of about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English.\n\n{\n  \"id\": \"13818513\",\n  \"summary\": \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n  \"dialogue\": \"Amanda: I baked cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\"\n}\n\n\nTo load the samsum dataset, we use the load_dataset() method from the 🤗 Datasets library.\n\nfrom datasets import load_dataset\n\n# Load dataset from the hub\ndataset = load_dataset(dataset_id)\n\nprint(f\"Train dataset size: {len(dataset['train'])}\")\nprint(f\"Test dataset size: {len(dataset['test'])}\")\n\n# Train dataset size: 14732\n# Test dataset size: 819\n\nLets checkout an example of the dataset.\n\nfrom random import randrange\n\n\nsample = dataset['train'][randrange(len(dataset[\"train\"]))]\nprint(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\nprint(f\"summary: \\n{sample['summary']}\\n---------------\")\n\nTo train our model we need to convert our inputs (text) to token IDs. This is done by a 🤗 Transformers Tokenizer. If you are not sure what this means check out chapter 6 of the Hugging Face Course.\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_id=\"google/flan-t5-base\"\n\n# Load tokenizer of FLAN-t5-base\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n\nbefore we can start training we need to preprocess our data. Abstractive Summarization is a text2text-generation task. This means our model will take a text as input and generate a summary as output. For this we want to understand how long our input and output will be to be able to efficiently batch our data.\n\nfrom datasets import concatenate_datasets\n\n# The maximum total input sequence length after tokenization.\n# Sequences longer than this will be truncated, sequences shorter will be padded.\ntokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\nmax_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\nprint(f\"Max source length: {max_source_length}\")\n\n# The maximum total sequence length for target text after tokenization.\n# Sequences longer than this will be truncated, sequences shorter will be padded.\"\ntokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\nmax_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\nprint(f\"Max target length: {max_target_length}\")\n\n\ndef preprocess_function(sample,padding=\"max_length\"):\n    # add prefix to the input for t5\n    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n\n    # tokenize inputs\n    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n\n    # Tokenize targets with the `text_target` keyword argument\n    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n\n    # padding in the loss.\n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\nprint(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n\n3. Fine-tune and evaluate FLAN-T5\nAfter we have processed our dataset, we can start training our model. Therefore we first need to load our FLAN-T5 from the Hugging Face Hub. In the example we are using a instance with a NVIDIA V100 meaning that we will fine-tune the base version of the model. I plan to do a follow-up post on how to fine-tune the xxl version of the model using Deepspeed.\n\nfrom transformers import AutoModelForSeq2SeqLM\n\n# huggingface hub model id\nmodel_id=\"google/flan-t5-base\"\n\n# load model from the hub\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n\nWe want to evaluate our model during training. The Trainer supports evaluation during training by providing a compute_metrics.   \nThe most commonly used metrics to evaluate summarization task is rogue_score short for Recall-Oriented Understudy for Gisting Evaluation). This metric does not behave like the standard accuracy: it will compare a generated summary against a set of reference summaries\nWe are going to use evaluate library to evaluate the rogue score.\n\nimport evaluate\nimport nltk\nimport numpy as np\nfrom nltk.tokenize import sent_tokenize\nnltk.download(\"punkt\")\n\n# Metric\nmetric = evaluate.load(\"rouge\")\n\n# helper function to postprocess text\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    # rougeLSum expects newline after each sentence\n    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n\n    return preds, labels\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    result = {k: round(v * 100, 4) for k, v in result.items()}\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    return result\n\nBefore we can start training is to create a DataCollator that will take care of padding our inputs and labels. We will use the DataCollatorForSeq2Seq from the 🤗 Transformers library.\n\nfrom transformers import DataCollatorForSeq2Seq\n\n# we want to ignore tokenizer pad token in the loss\nlabel_pad_token_id = -100\n# Data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n    pad_to_multiple_of=8\n)\n\n\nThe last step is to define the hyperparameters (TrainingArguments) we want to use for our training. We are leveraging the Hugging Face Hub integration of the Trainer to automatically push our checkpoints, logs and metrics during training into a repository.\n\nfrom huggingface_hub import HfFolder\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\n# Hugging Face repository id\nrepository_id = f\"{model_id.split('/')[1]}-{dataset_id}\"\n\n# Define training args\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=repository_id,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    fp16=False, # Overflows with fp16\n    learning_rate=5e-5,\n    num_train_epochs=5,\n    # logging & evaluation strategies\n    logging_dir=f\"{repository_id}/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=500,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    # metric_for_best_model=\"overall_f1\",\n    # push to hub parameters\n    report_to=\"tensorboard\",\n    push_to_hub=False,\n    hub_strategy=\"every_save\",\n    hub_model_id=repository_id,\n    hub_token=HfFolder.get_token(),\n)\n\n# Create Trainer instance\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    compute_metrics=compute_metrics,\n)\n\nWe can start our training by using the train method of the Trainer.\n\n# Start training\ntrainer.train()\n\n\nNice, we have trained our model. 🎉 Lets run evaluate the best model again on the test set.\n\nThe best score we achieved is an rouge1 score of 47.23.\nLets save our results and tokenizer to the Hugging Face Hub and create a model card.\n\n# Save our tokenizer and create model card\ntokenizer.save_pretrained(repository_id)\ntrainer.create_model_card()\n# Push the results to the hub\ntrainer.push_to_hub()\n\n4. Run Inference and summarize ChatGPT dialogues\nNow we have a trained model, we can use it to run inference. We will use the pipeline API from transformers and a test example from our dataset.\n\nfrom transformers import pipeline\nfrom random import randrange\n\n# load model and tokenizer from huggingface hub with pipeline\nsummarizer = pipeline(\"summarization\", model=\"philschmid/flan-t5-base-samsum\", device=0)\n\n# select a random test sample\nsample = dataset['test'][randrange(len(dataset[\"test\"]))]\nprint(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n\n# summarize dialogue\nres = summarizer(sample[\"dialogue\"])\n\nprint(f\"flan-t5-base summary:\\n{res[0]['summary_text']}\")\n\noutput\n\n    dialogue:\n    Abby: Have you talked to Miro?\n    Dylan: No, not really, I've never had an opportunity\n    Brandon: me neither, but he seems a nice guy\n    Brenda: you met him yesterday at the party?\n    Abby: yes, he's so interesting\n    Abby: told me the story of his father coming from Albania to the US in the early 1990s\n    Dylan: really, I had no idea he is Albanian\n    Abby: he is, he speaks only Albanian with his parents\n    Dylan: fascinating, where does he come from in Albania?\n    Abby: from the seacoast\n    Abby: Duress I believe, he told me they are not from Tirana\n    Dylan: what else did he tell you?\n    Abby: That they left kind of illegally\n    Abby: it was a big mess and extreme poverty everywhere\n    Abby: then suddenly the border was open and they just left\n    Abby: people were boarding available ships, whatever, just to get out of there\n    Abby: he showed me some pictures, like <file_photo>\n    Dylan: insane\n    Abby: yes, and his father was among the people\n    Dylan: scary but interesting\n    Abby: very!\n    ---------------\n    flan-t5-base summary:\n    Abby met Miro yesterday at the party. Miro's father came from Albania to the US in the early 1990s. He speaks Albanian with his parents. The border was open and people were boarding ships to get out of there.\n\nThanks for reading! If you have any questions, feel free to contact me on Twitter or LinkedIn.\n",
      "length": 12706,
      "excerpt": "In this blog, you will learn how to fine-tune google/flan-t5-base for chat & dialogue summarization using Hugging Face Transformers. If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages.",
      "siteName": null,
      "summary": "In this blog, you will learn how to fine-tune google/flan-t5-base for chat & dialogue summarization. If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages.",
      "topics": [
        {
          "label": "Computers & Internet",
          "score": 0.9962887763977051
        },
        {
          "label": "Business & Finance",
          "score": 0.6852269172668457
        },
        {
          "label": "Science & Mathematics",
          "score": 0.5981874465942383
        }
      ]
    }
  },
  "3788905138": {
    "item_id": "3788905138",
    "resolved_id": "3788905138",
    "given_url": "https://aruva.medium.com/using-chatgpt-to-build-system-diagrams-part-i-69efc7603926",
    "given_title": "Using ChatGPT to build System Diagrams — Part I | by aruva - empowering ide",
    "favorite": "0",
    "status": "0",
    "time_added": "1676096146",
    "time_updated": "1676096151",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 9,
    "resolved_title": "Using ChatGPT to build System Diagrams",
    "resolved_url": "https://aruva.medium.com/using-chatgpt-to-build-system-diagrams-part-i-69efc7603926",
    "excerpt": "The popularity of ChatGPT keeps growing and it’s now being used in a wide range of applications. As a software architect, I had the opportunity to explore ChatGPT’s capabilities for use within the financial domain, specifically for Wealth Management.",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "0",
    "word_count": "1228",
    "lang": "en",
    "time_to_read": 6,
    "top_image_url": "https://miro.medium.com/max/1200/0*8yQ4VAMYpigTHTa-",
    "tags": {
      "readinglist": {
        "item_id": "3788905138",
        "tag": "readinglist"
      }
    },
    "authors": {
      "175908023": {
        "item_id": "3788905138",
        "author_id": "175908023",
        "name": "aruva - empowering ideas",
        "url": "https://aruva.medium.com"
      }
    },
    "listen_duration_estimate": 475,
    "content": {
      "title": "Using ChatGPT to build System Diagrams",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><div lang=\"en\"><div><p>The popularity of ChatGPT keeps growing and it’s now being used in a wide range of applications. As a software architect, I had the opportunity to explore ChatGPT’s capabilities for use within the financial domain, specifically for Wealth Management.</p><figure></figure><p>I wanted to confirm if ChatGPT could generate diagrams for a use case explanation and analyze multiple factors at the same time.</p><p>So, I logged into <a rel=\"noopener noreferrer\" href=\"http://chat.openai.com/\" target=\"_blank\" id=\"reader.external-link.num-0\">chat.openai.com</a> and here is my conversation</p><p><strong>Step 1: Let’s define the use-case</strong></p><p>I could have defined my own use-case, but I wanted to test all aspects of ChatGPT, so I asked it to define one for me.</p><figure><figcaption>Use case definition</figcaption></figure></div><div><p><strong>Step 2: Let’s define a Data Model for the use-case</strong></p><p>Looks good to me, let’s start by defining the data model — so I asked ChatGPT to do it and this is what I got</p><figure><figcaption>data model definition</figcaption></figure><p>That sounds about right, lets see what it can do for diagrams. I wanted to drill a bit deeper on the data model so my next prompt was</p><blockquote><p><strong><em>Prompt</em></strong><em>:</em> <code>describe the data model in more detail or tabular format using markdown <br></code><strong><em>ChatGPT</em></strong><em>:</em> I can describe the structure of the data model in a tabular format using markdown.</p></blockquote><figure><figcaption>data structure</figcaption></figure><p>This is a good start. Next I asked for defining relationships</p></div><div><p><strong>Step 3: Let’s define relationships — Entity Relationship Diagram</strong></p><blockquote><p><strong><em>Prompt</em></strong><em>:</em> this is perfect, I would like to depict the relationships in a mermaid diagram, can you give me the markdown code to do this <br><strong><em>ChatGPT:</em></strong> I apologize, but as a text-based model, I am not able to generate Mermaid diagrams or any other visual diagrams. However, I can provide you with an example of how the relationships could be represented in Mermaid syntax:</p></blockquote><figure><figcaption>mermaid syntax for data relationships</figcaption></figure><p>And copying this in a markdown editor enclosed within {mermaid} tags, I got</p><figure><figcaption>data model in mermaid</figcaption></figure><p>Great, we are getting somewhere.</p><p>I have a class diagram / entity-relationship diagram. Let’s see what else is possible</p></div><div><p><strong>Step 4: Sequence Diagram</strong></p><p>Next, I tried to make hypothetical prompts to check responses. My first intention was to get a sequence diagram created</p><blockquote><p><strong><em>Prompt</em></strong><em>:</em> how would a Buy transaction for an asset impact the tables mentioned above, I would like to depict this as a sequence diagram. Can you help describe the diagram in markdown format for mermaid <br><strong><em>ChatGPT</em></strong><em>:</em> Sure, a <code>Buy</code> transaction would have an impact on the following tables:</p><p><code>Asset</code> table: the <code>current_value</code> of the asset would be updated to reflect the current market value of the asset <br><code>Portfolio</code> table: the <code>current_value</code> of the portfolio would be updated to reflect the new asset and its market value, and the <code>asset_allocation</code> would be updated to reflect the new asset. <br><code>Transaction</code> table: a new transaction would be added with the transaction details such as the transaction type, transaction date, asset_id, and portfolio_id</p><p>Here is an example of how the process could be represented in a Mermaid sequence diagram:</p></blockquote><figure><figcaption>sequence diagram for a buy transaction</figcaption></figure><p>and copying this is in a markdown editor</p><figure><figcaption>sequence diagram</figcaption></figure><p>Impressive. But I am not done yet</p></div><div><p><strong>Step 5: State Transition Diagram</strong></p><p>Let me ask about a <code>State Transition</code> diagram</p><blockquote><p><strong><em>Prompt</em></strong><em>:</em> Will the account have states like new, closed, active inactive etc? I would like to depict the states in a state diagram using mermaid. Can you help describe the diagram in markdown format <br><strong><em>ChatGPT</em></strong><em>:</em> It is common for financial accounts to have different states, such as new, closed, active, and inactive, to reflect the current status of the account.</p><p><code>new</code> account would be one that has just been opened and is awaiting funding or verification. <br><code>active</code> account would be one that is currently open and being used for transactions. <br><code>inactive</code> account would be one that is not currently being used but has not been closed. <br><code>closed</code> account would be one that has been closed by the customer or the financial institution, and no longer able to be used for transactions. <br><code>Pending</code> state could be used to indicate that the account is under review or awaiting verification. <br><code>Suspended</code> state could be used to indicate that the account has been temporarily suspended, for example, due to suspicious activity or non-compliance with terms and conditions. <br><code>Frozen</code> state could be used to indicate that the account has been frozen, for example, due to a legal dispute or regulatory action. <br><code>Expired</code> state could be used to indicate that the account has reached its expiration date and is no longer in use.</p><p>here is an example of how the states of an account could be represented in a Mermaid state diagram:</p></blockquote><figure><figcaption>Sequence Diagram</figcaption></figure><p>Looks like, this format is older and may break due to cyclical dependencies. So I asked ChatGPT to improve this</p><blockquote><p><strong><em>Prompt</em></strong><em>:</em> can you try giving the state diagram markdown syntax in new format using <code>[*]</code> <br><strong><em>ChatGPT</em></strong><em>:</em> Sure, here is an example of how a state diagram for a wealth management system's account management component could be represented in Mermaid using the new format with <code>[*]</code>:</p></blockquote><figure><figcaption>mermaid representation of state diagram</figcaption></figure><p><strong>Markdown Editor Representation</strong></p><figure><figcaption>markdown representation</figcaption></figure><p>This diagram shows the different states of an account and the possible transitions between them.</p><ul><li>The initial state is <code>[*]</code> which denotes the start state.</li><li>The account can transition from <code>New</code> state to <code>Active</code> state by activating it.</li><li>The account can transition from <code>New</code> state to <code>Closed</code> state by closing it.</li><li>The account can transition from <code>Active</code> state to <code>Inactive</code> state by deactivating it.</li><li>The account can transition from <code>Active</code> state to <code>Closed</code> state by closing it.</li><li>The account can transition from <code>Inactive</code> state to <code>Active</code> state by activating it.</li><li>The account can transition from <code>Inactive</code> state to <code>Closed</code> state by closing it.</li></ul><p>Now we are getting somewhere. Let’s extend this further to include other states as well</p><blockquote><p><em>Prompt:</em> please include expired, frozen and other relevant states as well <br><em>ChatGPT:</em> Sure, here is an example of how a state diagram for a wealth management system’s account management component could be represented in Mermaid including additional states such as <code>Expired</code>, <code>Frozen</code>, etc:</p><p>Description: <br>This diagram shows the different states of an account and the possible transitions between them.</p><p>The initial state is <code>[*]</code> which denotes the start state. <br>The account can transition from <code>New</code> state to <code>Active</code> state by activating it. <br>The account can transition from <code>New</code> state to <code>Closed</code> state by closing it. <br>The account can transition from <code>Active</code> state to <code>Inactive</code> state by deactivating it. <br>The account can transition from <code>Active</code> state to <code>Frozen</code> state by freezing it. <br>The account can transition from <code>Active</code> state to <code>Closed</code> state by closing it. <br>The account can transition from <code>Inactive</code> state to <code>Active</code> state by activating it. <br>The account can transition from <code>Inactive</code> state to <code>Frozen</code> state by freezing it. <br>The account can transition from <code>Inactive</code> state to <code>Closed</code> state by closing it. <br>The account can transition from <code>Frozen</code> state to <code>Active</code> state by unfreezing it. <br>The account can transition from <code>Frozen</code> state to <code>Closed</code> state by closing it. <br>The account can transition from <code>Closed</code> state to <code>Expired</code> state if the account is expired.</p></blockquote><figure><figcaption>extended state diagram</figcaption></figure><p>And Diagrammatic Representation</p><figure></figure><p>This is awesome. Truly impressive on what can be achieved with ChatGPT and Mermaid</p><p><em>I next moved on to describing and building more diagrams using the C4 model and will add them to a follow up post</em></p><p>Thanks for reading!</p><p>Please subscribe <a href=\"https://aruva.medium.com/membership\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-1\">here</a> to never miss our new posts and don’t forget to clap if you enjoyed reading this post</p></div></div></article></div>",
      "textContent": "The popularity of ChatGPT keeps growing and it’s now being used in a wide range of applications. As a software architect, I had the opportunity to explore ChatGPT’s capabilities for use within the financial domain, specifically for Wealth Management.I wanted to confirm if ChatGPT could generate diagrams for a use case explanation and analyze multiple factors at the same time.So, I logged into chat.openai.com and here is my conversationStep 1: Let’s define the use-caseI could have defined my own use-case, but I wanted to test all aspects of ChatGPT, so I asked it to define one for me.Use case definitionStep 2: Let’s define a Data Model for the use-caseLooks good to me, let’s start by defining the data model — so I asked ChatGPT to do it and this is what I gotdata model definitionThat sounds about right, lets see what it can do for diagrams. I wanted to drill a bit deeper on the data model so my next prompt wasPrompt: describe the data model in more detail or tabular format using markdown ChatGPT: I can describe the structure of the data model in a tabular format using markdown.data structureThis is a good start. Next I asked for defining relationshipsStep 3: Let’s define relationships — Entity Relationship DiagramPrompt: this is perfect, I would like to depict the relationships in a mermaid diagram, can you give me the markdown code to do this ChatGPT: I apologize, but as a text-based model, I am not able to generate Mermaid diagrams or any other visual diagrams. However, I can provide you with an example of how the relationships could be represented in Mermaid syntax:mermaid syntax for data relationshipsAnd copying this in a markdown editor enclosed within {mermaid} tags, I gotdata model in mermaidGreat, we are getting somewhere.I have a class diagram / entity-relationship diagram. Let’s see what else is possibleStep 4: Sequence DiagramNext, I tried to make hypothetical prompts to check responses. My first intention was to get a sequence diagram createdPrompt: how would a Buy transaction for an asset impact the tables mentioned above, I would like to depict this as a sequence diagram. Can you help describe the diagram in markdown format for mermaid ChatGPT: Sure, a Buy transaction would have an impact on the following tables:Asset table: the current_value of the asset would be updated to reflect the current market value of the asset Portfolio table: the current_value of the portfolio would be updated to reflect the new asset and its market value, and the asset_allocation would be updated to reflect the new asset. Transaction table: a new transaction would be added with the transaction details such as the transaction type, transaction date, asset_id, and portfolio_idHere is an example of how the process could be represented in a Mermaid sequence diagram:sequence diagram for a buy transactionand copying this is in a markdown editorsequence diagramImpressive. But I am not done yetStep 5: State Transition DiagramLet me ask about a State Transition diagramPrompt: Will the account have states like new, closed, active inactive etc? I would like to depict the states in a state diagram using mermaid. Can you help describe the diagram in markdown format ChatGPT: It is common for financial accounts to have different states, such as new, closed, active, and inactive, to reflect the current status of the account.new account would be one that has just been opened and is awaiting funding or verification. active account would be one that is currently open and being used for transactions. inactive account would be one that is not currently being used but has not been closed. closed account would be one that has been closed by the customer or the financial institution, and no longer able to be used for transactions. Pending state could be used to indicate that the account is under review or awaiting verification. Suspended state could be used to indicate that the account has been temporarily suspended, for example, due to suspicious activity or non-compliance with terms and conditions. Frozen state could be used to indicate that the account has been frozen, for example, due to a legal dispute or regulatory action. Expired state could be used to indicate that the account has reached its expiration date and is no longer in use.here is an example of how the states of an account could be represented in a Mermaid state diagram:Sequence DiagramLooks like, this format is older and may break due to cyclical dependencies. So I asked ChatGPT to improve thisPrompt: can you try giving the state diagram markdown syntax in new format using [*] ChatGPT: Sure, here is an example of how a state diagram for a wealth management system's account management component could be represented in Mermaid using the new format with [*]:mermaid representation of state diagramMarkdown Editor Representationmarkdown representationThis diagram shows the different states of an account and the possible transitions between them.The initial state is [*] which denotes the start state.The account can transition from New state to Active state by activating it.The account can transition from New state to Closed state by closing it.The account can transition from Active state to Inactive state by deactivating it.The account can transition from Active state to Closed state by closing it.The account can transition from Inactive state to Active state by activating it.The account can transition from Inactive state to Closed state by closing it.Now we are getting somewhere. Let’s extend this further to include other states as wellPrompt: please include expired, frozen and other relevant states as well ChatGPT: Sure, here is an example of how a state diagram for a wealth management system’s account management component could be represented in Mermaid including additional states such as Expired, Frozen, etc:Description: This diagram shows the different states of an account and the possible transitions between them.The initial state is [*] which denotes the start state. The account can transition from New state to Active state by activating it. The account can transition from New state to Closed state by closing it. The account can transition from Active state to Inactive state by deactivating it. The account can transition from Active state to Frozen state by freezing it. The account can transition from Active state to Closed state by closing it. The account can transition from Inactive state to Active state by activating it. The account can transition from Inactive state to Frozen state by freezing it. The account can transition from Inactive state to Closed state by closing it. The account can transition from Frozen state to Active state by unfreezing it. The account can transition from Frozen state to Closed state by closing it. The account can transition from Closed state to Expired state if the account is expired.extended state diagramAnd Diagrammatic RepresentationThis is awesome. Truly impressive on what can be achieved with ChatGPT and MermaidI next moved on to describing and building more diagrams using the C4 model and will add them to a follow up postThanks for reading!Please subscribe here to never miss our new posts and don’t forget to clap if you enjoyed reading this post",
      "length": 7256,
      "excerpt": "The popularity of ChatGPT keeps growing and it’s now being used in a wide range of applications. As a software architect, I had the opportunity to explore ChatGPT’s capabilities for use within the financial domain, specifically for Wealth Management.",
      "siteName": null,
      "summary": "ChatGPT can generate diagrams for a use case explanation and analyze multiple factors at the same time. The popularity of ChatGPT keeps growing and it’s now being used in a wide range of applications. As a software architect, I had the opportunity to explore ChatG PT’S capabilities for use within the financial domain, specifically for Wealth Management.",
      "topics": [
        {
          "label": "Computers & Internet",
          "score": 0.9938475489616394
        },
        {
          "label": "Business & Finance",
          "score": 0.9318660497665405
        },
        {
          "label": "Education & Reference",
          "score": 0.8434854745864868
        }
      ]
    }
  },
  "3794646409": {
    "item_id": "3794646409",
    "resolved_id": "3794646409",
    "given_url": "https://www.menshealth.com/nutrition/a42592284/casein-protein/",
    "given_title": "What Casein Protein Powder Does - Benefits, Side Effects, vs Whey",
    "favorite": "0",
    "status": "0",
    "time_added": "1675794908",
    "time_updated": "1675794910",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 12,
    "resolved_title": "Casein Protein Powder Might Be Your Secret Weapon for Bulking Up",
    "resolved_url": "https://www.menshealth.com/nutrition/a42592284/casein-protein/",
    "excerpt": "YOU'VE LIKELY HEARD about whey protein (the stuff is everywhere) and maybe even pea protein (the plant-based up-and-comer), but there's another protein that's popular among the fitness community: casein.",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "1",
    "word_count": "896",
    "lang": "en",
    "time_to_read": 4,
    "top_image_url": "https://hips.hearstapps.com/hmg-prod/images/gettyimages-1674233091.jpg?crop=1.00xw:0.752xh;0,0.0793xh&resize=1200:*",
    "tags": {
      "readinglist": {
        "item_id": "3794646409",
        "tag": "readinglist"
      }
    },
    "authors": {
      "126761664": {
        "item_id": "3794646409",
        "author_id": "126761664",
        "name": "Perri O. Blumberg",
        "url": "https://www.menshealth.com/author/4538/perri-o-blumberg/"
      }
    },
    "image": {
      "item_id": "3794646409",
      "src": "https://hips.hearstapps.com/rover/profile_photos/bba99f41-f599-4411-be4a-d014d50f0701_1534360513.file",
      "width": "540",
      "height": "540"
    },
    "images": {
      "1": {
        "item_id": "3794646409",
        "image_id": "1",
        "src": "https://hips.hearstapps.com/rover/profile_photos/bba99f41-f599-4411-be4a-d014d50f0701_1534360513.file",
        "width": "540",
        "height": "540",
        "credit": "",
        "caption": ""
      }
    },
    "domain_metadata": {
      "name": "Men's Health",
      "logo": "https://logo.clearbit.com/menshealth.com?size=800",
      "greyscale_logo": "https://logo.clearbit.com/menshealth.com?size=800&greyscale=true"
    },
    "listen_duration_estimate": 347,
    "content": {
      "title": "Casein Protein Powder Might Be Your Secret Weapon for Bulking Up",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><div lang=\"en\"><p><strong>YOU'VE LIKELY HEARD</strong> about <a href=\"https://www.menshealth.com/nutrition/g19547347/best-whey-powder-review/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-0\">whey protein</a> (the stuff is everywhere) and maybe even <a href=\"https://www.menshealth.com/nutrition/a37516220/what-is-pea-protein/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-1\">pea protein</a> (the plant-based up-and-comer), but there's another protein that's popular among the fitness community: casein.</p><p>Casein shares more in common with whey than it does with peas, but casein is also distinctly different from both—and has unique benefits.  <br></p><p>“There are two different types of protein in cow’s, goat’s and sheep’s milk—casein and whey,” says <a href=\"https://crossfit26.com/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-2\"><u>Kim Yawitz</u></a>, R.D., a gym owner in St. Louis, MO. </p><p>“About 80 percent of the protein in <a href=\"https://www.menshealth.com/nutrition/a35630949/healthiest-milk/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-3\">cow’s milk</a> is casein, and about 20 percent of it is whey, according to a <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5149046/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-4\"><u>study</u></a> in the <em>Iranian Journal of Pharmaceutical Research</em>,” says <a href=\"https://www.instagram.com/amydgorin/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-5\"><u>Amy Gorin</u></a>, M.S., R.D.N., an inclusive plant-based dietitian in Stamford, CT and owner of <a href=\"https://plantbasedwithamy.com/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-6\"><u>Plant Based with Amy</u></a>. “Whey and casein are both complete proteins and are different in several ways. For one, whey protein is digested more quickly than casein protein.”</p><p>Ahead, the <a href=\"https://www.menshealth.com/nutrition/a28261495/the-best-healthy-ice-creams/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-7\">scoop</a> on all things casein protein.</p><h2><strong>What is casein protein?</strong></h2><p>To recap, casein is one of two major <a href=\"https://www.menshealth.com/nutrition/a32842318/how-much-protein-do-i-need/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-8\">proteins</a> found in dairy products. “It’s also used to make protein supplements,” says Yawitz, noting that such supplements come in two forms: micellar casein and casein hydrolysate. “Micellar casein is slow-digesting, while casein hydrolysate is predigested for faster absorption.” </p><p>Because casein protein contains all the amino acids required by the human body, it's a complete protein, says Gorin. “It's also more slowly digestible than whey protein and is released into the body more slowly, which means that it keeps you fuller for longer periods of time.”</p><p>Plus, as Gorin points out, “research has found casein to offer many health benefits, including potential cancer-fighting properties.\" Of course, more research is needed to verify these findings, but the possible health benefits of casein protein seem encouraging. </p><h2><strong>Do you need a casein supplement?</strong></h2><p>As you’ve likely gathered, dairy is the best source of casein protein. “Casein is mostly found in dairy products—milk, hard cheese, <a href=\"https://www.menshealth.com/nutrition/a27335542/cottage-cheese-healthy/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-9\">cottage cheese</a>, and yogurt,” says Yawitz. “There’s no specific recommendation for how much casein men need in a day (and you won’t find grams of casein listed on a food label, so it can be difficult to know exactly how much you’re getting from the diet).”</p><p>If you don’t eat a ton of dairy, should you take a supplement? Probably not. “You don’t necessarily need to use a casein supplement, especially if you get plenty of protein throughout the day,” says Yawitz, sharing that a typical dose for men who choose to use casein supplements is 20 to 40 grams daily.</p><h2><strong>Can casein protein help with muscle building and exercise recovery? </strong></h2><p>If you’re all about <a href=\"https://www.menshealth.com/fitness/a19534499/10-muscle-building-tips/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-10\">bulking up</a>, casein protein may be particularly beneficial. </p><p>“Assuming you lift weights or perform other resistance exercises regularly, casein protein can help you build muscle and recover faster,” says Yawitz. “Most importantly, casein supplements can help you get enough protein for muscle repair and growth.”</p><p>As Yawitz highlights, <a href=\"https://jissn.biomedcentral.com/articles/10.1186/s12970-018-0215-1#\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-11\"><u>studies suggest</u></a> that daily protein intakes ranging from 0.73 to 1 gram per pound of body weight can help maximize resistance training-induced muscle growth. “And getting that much protein is often easier said than done, especially if your schedule is hectic or you tend to get full quickly when eating high-protein foods,\" she says.</p><p>Casein supplements typically contain around 25 grams of protein per scoop, says Yawitz, commenting that they are quite convenient to consume, making it one reason why they’re so popular with bodybuilders and other fitness enthusiasts.</p><p>“Even if you get enough protein throughout the day, there may be some added benefit to taking casein supplements (or eating casein-rich foods) before bed,” adds Yawitz. “Going to bed with an empty stomach can increase the likelihood that your body will break down muscle proteins for energy and nutrients, making it more challenging to repair and build muscle. Casein takes several hours to digest and absorb, so taking it before bed could help spare muscle proteins, allowing your muscles to recover better and possibly even grow.” </p><p>Yawitz points to <a href=\"https://www.frontiersin.org/articles/10.3389/fnut.2019.00017/full\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-12\"><u>one study</u></a> where muscle protein synthesis was 22 percent greater in subjects who took a casein supplement before bed, compared to those who took a placebo. </p><p>Still, it’s worth flagging that casein probably isn't the top choice if you’re looking for a post-workout protein supplement. “Whey protein is digested and absorbed much more quickly than casein, so your body can get straight to work repairing and building muscle,” says Yawitz. </p><h2><strong>Are there any negatives to casein protein?</strong></h2><p>It may be obvious, but it bears stating: If you’ve been diagnosed with a milk allergy avoid casein rich-foods and supplements.</p><p>“I’m often asked about casein for those with <a href=\"https://www.menshealth.com/health/a19544249/can-you-eat-dairy-if-you-are-lactose-intolerant/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-13\">lactose intolerance</a>. With lactose intolerance, the sugar in milk is problematic rather than the casein or whey,” elaborates Yawitz. “Most lactose-free dairy products still contain casein, so those are great options if you have a diagnosed intolerance,” she continues, adding that casein powders can be tricky, as they sometimes contain trace amounts of lactose, so it’s best to proceed with caution.</p><p>And another word for the wise: Always check supplements’ labels and look for products that undergo third-party testing—you can confirm this via an insignia on the product packaging—since the FDA doesn’t regulate most nutritional supplements, which leaves a lot of room for inaccuracy on product and nutrition labels, says Yawitz. “Supplements that have undergone third-party testing are evaluated by an objective party to ensure that the labeling is accurate.”</p><div id=\"RIL_IMG_1\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fhips.hearstapps.com%2Frover%2Fprofile_photos%2Fbba99f41-f599-4411-be4a-d014d50f0701_1534360513.file\">\n          \n          \n        </figure>\n      </div></div></article></div>",
      "textContent": "YOU'VE LIKELY HEARD about whey protein (the stuff is everywhere) and maybe even pea protein (the plant-based up-and-comer), but there's another protein that's popular among the fitness community: casein.Casein shares more in common with whey than it does with peas, but casein is also distinctly different from both—and has unique benefits.  “There are two different types of protein in cow’s, goat’s and sheep’s milk—casein and whey,” says Kim Yawitz, R.D., a gym owner in St. Louis, MO. “About 80 percent of the protein in cow’s milk is casein, and about 20 percent of it is whey, according to a study in the Iranian Journal of Pharmaceutical Research,” says Amy Gorin, M.S., R.D.N., an inclusive plant-based dietitian in Stamford, CT and owner of Plant Based with Amy. “Whey and casein are both complete proteins and are different in several ways. For one, whey protein is digested more quickly than casein protein.”Ahead, the scoop on all things casein protein.What is casein protein?To recap, casein is one of two major proteins found in dairy products. “It’s also used to make protein supplements,” says Yawitz, noting that such supplements come in two forms: micellar casein and casein hydrolysate. “Micellar casein is slow-digesting, while casein hydrolysate is predigested for faster absorption.” Because casein protein contains all the amino acids required by the human body, it's a complete protein, says Gorin. “It's also more slowly digestible than whey protein and is released into the body more slowly, which means that it keeps you fuller for longer periods of time.”Plus, as Gorin points out, “research has found casein to offer many health benefits, including potential cancer-fighting properties.\" Of course, more research is needed to verify these findings, but the possible health benefits of casein protein seem encouraging. Do you need a casein supplement?As you’ve likely gathered, dairy is the best source of casein protein. “Casein is mostly found in dairy products—milk, hard cheese, cottage cheese, and yogurt,” says Yawitz. “There’s no specific recommendation for how much casein men need in a day (and you won’t find grams of casein listed on a food label, so it can be difficult to know exactly how much you’re getting from the diet).”If you don’t eat a ton of dairy, should you take a supplement? Probably not. “You don’t necessarily need to use a casein supplement, especially if you get plenty of protein throughout the day,” says Yawitz, sharing that a typical dose for men who choose to use casein supplements is 20 to 40 grams daily.Can casein protein help with muscle building and exercise recovery? If you’re all about bulking up, casein protein may be particularly beneficial. “Assuming you lift weights or perform other resistance exercises regularly, casein protein can help you build muscle and recover faster,” says Yawitz. “Most importantly, casein supplements can help you get enough protein for muscle repair and growth.”As Yawitz highlights, studies suggest that daily protein intakes ranging from 0.73 to 1 gram per pound of body weight can help maximize resistance training-induced muscle growth. “And getting that much protein is often easier said than done, especially if your schedule is hectic or you tend to get full quickly when eating high-protein foods,\" she says.Casein supplements typically contain around 25 grams of protein per scoop, says Yawitz, commenting that they are quite convenient to consume, making it one reason why they’re so popular with bodybuilders and other fitness enthusiasts.“Even if you get enough protein throughout the day, there may be some added benefit to taking casein supplements (or eating casein-rich foods) before bed,” adds Yawitz. “Going to bed with an empty stomach can increase the likelihood that your body will break down muscle proteins for energy and nutrients, making it more challenging to repair and build muscle. Casein takes several hours to digest and absorb, so taking it before bed could help spare muscle proteins, allowing your muscles to recover better and possibly even grow.” Yawitz points to one study where muscle protein synthesis was 22 percent greater in subjects who took a casein supplement before bed, compared to those who took a placebo. Still, it’s worth flagging that casein probably isn't the top choice if you’re looking for a post-workout protein supplement. “Whey protein is digested and absorbed much more quickly than casein, so your body can get straight to work repairing and building muscle,” says Yawitz. Are there any negatives to casein protein?It may be obvious, but it bears stating: If you’ve been diagnosed with a milk allergy avoid casein rich-foods and supplements.“I’m often asked about casein for those with lactose intolerance. With lactose intolerance, the sugar in milk is problematic rather than the casein or whey,” elaborates Yawitz. “Most lactose-free dairy products still contain casein, so those are great options if you have a diagnosed intolerance,” she continues, adding that casein powders can be tricky, as they sometimes contain trace amounts of lactose, so it’s best to proceed with caution.And another word for the wise: Always check supplements’ labels and look for products that undergo third-party testing—you can confirm this via an insignia on the product packaging—since the FDA doesn’t regulate most nutritional supplements, which leaves a lot of room for inaccuracy on product and nutrition labels, says Yawitz. “Supplements that have undergone third-party testing are evaluated by an objective party to ensure that the labeling is accurate.”\n        \n          \n          \n          \n        \n      ",
      "length": 5670,
      "excerpt": "YOU'VE LIKELY HEARD about whey protein (the stuff is everywhere) and maybe even pea protein (the plant-based up-and-comer), but there's another protein that's popular among the fitness community: casein.",
      "siteName": null,
      "summary": "Casein shares more in common with whey than it does with peas. About 80 percent of the protein in cow’s milk is casein, and about 20 percent of it is whey. A typical dose for men who choose to use casein supplements is 20 to 40 grams daily.",
      "topics": [
        {
          "label": "Health",
          "score": 0.9982981085777283
        },
        {
          "label": "Science & Mathematics",
          "score": 0.9482187628746033
        },
        {
          "label": "Business & Finance",
          "score": 0.5674258470535278
        }
      ]
    }
  },
  "3795392597": {
    "item_id": "3795392597",
    "resolved_id": "3795392597",
    "given_url": "https://www.thecollector.com/10-best-philosophical-concepts-to-know/",
    "given_title": "10 Philosophical Concepts You Should Know",
    "favorite": "0",
    "status": "0",
    "time_added": "1675794806",
    "time_updated": "1675794809",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 13,
    "resolved_title": "10 Philosophical Concepts You Should Know",
    "resolved_url": "https://www.thecollector.com/10-best-philosophical-concepts-to-know/",
    "excerpt": "Did you know that philosophers have proposed answers for many of the questions we ask ourselves? Here are some philosophical concepts that everyone should know.",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "1",
    "word_count": "2272",
    "lang": "en",
    "time_to_read": 10,
    "top_image_url": "https://cdn.thecollector.com/wp-content/uploads/2023/01/best-philosophical-concepts-to-know.jpg",
    "tags": {
      "readinglist": {
        "item_id": "3795392597",
        "tag": "readinglist"
      }
    },
    "authors": {
      "174145893": {
        "item_id": "3795392597",
        "author_id": "174145893",
        "name": "Viktoriya Sus",
        "url": "https://www.thecollector.com/author/viktoriya-sus/"
      }
    },
    "image": {
      "item_id": "3795392597",
      "src": "https://cdn.thecollector.com/wp-content/uploads/2023/01/best-philosophical-concepts-to-know.jpg?width=1400&quality=70",
      "width": "0",
      "height": "0"
    },
    "images": {
      "1": {
        "item_id": "3795392597",
        "image_id": "1",
        "src": "https://cdn.thecollector.com/wp-content/uploads/2023/01/best-philosophical-concepts-to-know.jpg?width=1400&quality=70",
        "width": "0",
        "height": "0",
        "credit": "",
        "caption": ""
      },
      "2": {
        "item_id": "3795392597",
        "image_id": "2",
        "src": "https://cdn.thecollector.com/wp-content/uploads/2023/01/gustave-courbet-la-grotte-de-la-lue-painting.jpg?width=1400&quality=55",
        "width": "0",
        "height": "0",
        "credit": "",
        "caption": "La Grotte de la Loue, Gustave Courbet, 1864, via National Gallery of Art"
      },
      "3": {
        "item_id": "3795392597",
        "image_id": "3",
        "src": "https://cdn.thecollector.com/wp-content/uploads/2023/01/van-gogh-self-portrait-painting.jpg?width=1400&quality=55",
        "width": "0",
        "height": "0",
        "credit": "",
        "caption": "Self-Portrait with Grey Felt Hat, Vincent van Gogh, 1887, via Van Gogh Museum"
      },
      "4": {
        "item_id": "3795392597",
        "image_id": "4",
        "src": "https://cdn.thecollector.com/wp-content/uploads/2023/01/sandro-botticelli-sant_agostino-painting.jpg?width=1080&quality=55",
        "width": "0",
        "height": "0",
        "credit": "",
        "caption": "St Augustine, Sandro Botticelli, 1480, via WikiArt"
      },
      "5": {
        "item_id": "3795392597",
        "image_id": "5",
        "src": "https://cdn.thecollector.com/wp-content/uploads/2023/01/frederic-shoberl-hindoo-woman-painting.jpg?width=1400&quality=55",
        "width": "0",
        "height": "0",
        "credit": "",
        "caption": "An Hindoo Woman Throwing Herself on the Funeral Pyre of Her Husband, Frederic Shoberl, via WikiMedia"
      },
      "6": {
        "item_id": "3795392597",
        "image_id": "6",
        "src": "https://cdn.thecollector.com/wp-content/uploads/2023/01/rembrandt-peale-court-of-death-painting.jpg?width=1400&quality=55",
        "width": "0",
        "height": "0",
        "credit": "",
        "caption": "The Court of Death, Rembrandt Peale, 1820, via Detroit Institute of Arts"
      },
      "7": {
        "item_id": "3795392597",
        "image_id": "7",
        "src": "https://cdn.thecollector.com/wp-content/uploads/2023/01/jon-krause-free-will-illustration.jpg?width=1400&quality=55",
        "width": "0",
        "height": "0",
        "credit": "",
        "caption": "Free Will Illustration, Jon Krause, via Harvard Magazine"
      },
      "8": {
        "item_id": "3795392597",
        "image_id": "8",
        "src": "https://cdn.thecollector.com/wp-content/uploads/2023/01/jonas-suyderhoff-rene-descartes-portrait.jpg?width=1200&quality=55",
        "width": "0",
        "height": "0",
        "credit": "",
        "caption": "René Descartes, Jonas Suyderhoff, 1650s, via National Gallery of Art"
      },
      "9": {
        "item_id": "3795392597",
        "image_id": "9",
        "src": "https://cdn.thecollector.com/wp-content/uploads/2023/01/edvard-munch-friedrich-nietzsche-portrait.jpg?width=1400&quality=55",
        "width": "0",
        "height": "0",
        "credit": "",
        "caption": "Friedrich Nietzsche, Edvard Munch, 1960, via WikiMedia"
      },
      "10": {
        "item_id": "3795392597",
        "image_id": "10",
        "src": "https://cdn.thecollector.com/wp-content/uploads/2023/01/benton-spruance-to-freedom-painting.jpg?width=1400&quality=55",
        "width": "0",
        "height": "0",
        "credit": "",
        "caption": "To Freedom, Benton Spruance, 1952, via Whitney Museum of American Art"
      }
    },
    "listen_duration_estimate": 879,
    "content": {
      "title": "10 Philosophical Concepts You Should Know",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><p><span><div id=\"RIL_IMG_1\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fcdn.thecollector.com%2Fwp-content%2Fuploads%2F2023%2F01%2Fbest-philosophical-concepts-to-know.jpg%3Fwidth%3D1400%26quality%3D70\">\n          \n          \n        </figure>\n      </div></span></p>\n\n<p>What is the meaning of life? What are good and evil? What is justice? These are some of the questions that philosophers have been asking for centuries. Philosophy is a complex and fascinating field of study that can sometimes seem daunting to beginners. And while there’s no one answer that everyone will agree on, it’s still important to know some of the most fundamental ideas in philosophy. Here are ten common philosophical concepts everyone should be familiar with, regardless of educational background.</p>\n\n\n<figure aria-describedby=\"caption-attachment-68776\"><p><span><span></span><div id=\"RIL_IMG_2\">\n        <figure>\n          <img alt=\"La Grotte de la Loue, Gustave Courbet, 1864, via National Gallery of Art\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fcdn.thecollector.com%2Fwp-content%2Fuploads%2F2023%2F01%2Fgustave-courbet-la-grotte-de-la-lue-painting.jpg%3Fwidth%3D1400%26quality%3D55\">\n          <figcaption>La Grotte de la Loue, Gustave Courbet, 1864, via National Gallery of Art</figcaption>\n          \n        </figure>\n      </div></span></p>\n</figure>\n<p><a href=\"https://www.thecollector.com/who-are-the-philosopher-kings-plato-republic/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-0\">Plato</a> was the first to separate the “world of things” from the “world of ideas.” According to Plato, the idea (eidos) is the source of a thing, its prototype, the underlying reality of any particular object. For example, the “idea of a table” can either coincide with a particular table in reality or not match. But the “idea of the table” and the “concrete table” will continue to exist separately.</p>\n\n<p>A vivid illustration of the division of the world into the world of ideas and the world of objects is the famous Platonic myth of the cave, in which people see not objects and other people but only their shadows on the wall of the cave. In this metaphor, the shadows projected on the wall of the cave correspond to the individual objects in the world, while the objects whose shadows are on the wall correspond to the ideas – which are more fundamental and real, in Plato’s view.</p>\n\n<p>The cave for Plato is an allegory of our world, where people live, believing that the shadows on the walls of the caves are the only way to know reality. However, in reality, the shadows are just an illusion. Still, because of this illusion, it is difficult for people to pose critical questions about the existence of reality and overcome their “false consciousness.”</p>\n\n<div>\n<div>\n<h3>Get the latest articles delivered to your inbox</h3>\n<p><span>Sign up to our Free Weekly Newsletter</span></p></div>\n\n<p>Developing Platonic ideas, philosophers of later times reached the concept of the transcendent and the “thing-in-itself.”</p>\n</div>\n\n<h2>2. The Concept of Introspection</h2>\n<figure aria-describedby=\"caption-attachment-68782\"><p><span><span></span><div id=\"RIL_IMG_3\">\n        <figure>\n          <img alt=\"Self-Portrait with Grey Felt Hat, Vincent van Gogh, 1887, via Van Gogh Museum\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fcdn.thecollector.com%2Fwp-content%2Fuploads%2F2023%2F01%2Fvan-gogh-self-portrait-painting.jpg%3Fwidth%3D1400%26quality%3D55\">\n          <figcaption>Self-Portrait with Grey Felt Hat, Vincent van Gogh, 1887, via Van Gogh Museum</figcaption>\n          \n        </figure>\n      </div></span></p>\n</figure>\n<p>Introspection is a way of achieving self-knowledge during which a person observes their internal reaction to events in the external world. Introspection is motivated by a fundamental human need to examine the self carefully, to explain to themselves why they believe what they believe and whether there is a possibility that their belief is wrong.</p>\n\n<p>The founder of introspection as a method of inquiry is the British educator and philosopher John <a href=\"https://www.thecollector.com/social-contract-theories-explained/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-1\">Locke</a>, who, relying on the ideas of Rene <a href=\"https://www.thecollector.com/foundationalism-can-we-know-anything-philosophy/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-2\">Descartes</a>, pointed out that there are only two direct sources of all knowledge: the objects of the external world and the human mind. In this regard, all significant psychological facts of consciousness are open to study only by the subject of knowledge itself. It may well be that “blue” for one person is not at all the same as “blue” for another.</p>\n\n<p>The method of introspection helps to keep track of the stages of thinking, breaking down feelings into elements and providing a complete picture of the relationship between thoughts and actions. Introspection teaches us to think more abstractly and broadly, for example, to perceive a “big red apple” as a sensation of red, replaced by an impression of a round one and a trace of a taste sensation. But do not go too deep into introspection – excessive focus on tracking your own impressions can dull the perception of reality.</p>\n\n\n<figure aria-describedby=\"caption-attachment-68778\">\n<figcaption>Illustration to the book Mysterious Stranger by Mark Twain, Author unknown, via Project Gutenberg</figcaption></figure>\n<p>Solipsism is a philosophical concept according to which a person recognizes only their mind as the only reality that always exists and is always available. Mark Twain demonstrates the main message of solipsism in his story&nbsp;<a rel=\"noopener noreferrer\" href=\"https://www.goodreads.com/book/show/102564.The_Mysterious_Stranger\" target=\"_blank\" id=\"reader.external-link.num-3\">The Mysterious Stranger</a>:</p>\n\n<p>“There is no God, no universe, no human race, no earthly life, no heaven, no hell. It is all a Dream, a grotesque and foolish dream. Nothing exists but you. And You are but a Thought – a vagrant Thought, a useless Thought, a homeless Thought, wandering forlorn among the empty eternities.”</p>\n\n<p>The same idea, in general, is illustrated by the films&nbsp;<i>Mr. Nobody, The Beginning</i>, and&nbsp;<a href=\"https://www.thecollector.com/nick-bostrom-simulation-theory/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-4\">The Matrix</a>.</p>\n\n<p>According to solipsism, only a person’s perception of reality and their thoughts are available to them, while the entire external world is beyond the bounds of certainty. Therefore, the existence of things for a person will always be only a matter of faith, since if someone requires proof of their existence, a person will not be able to provide them.</p>\n\n<p>In other words, no person can be sure of the existence of anything outside of their consciousness. Solipsism is not so much doubt about the existence of reality as a recognition of the importance of the role of one’s own mind. The concept of solipsism either needs to be assimilated as it is or to accept “solipsism in reverse,” that is, to give yourself a rational explanation of the relative external world and justify why this external world still exists.</p>\n\n\n<figure aria-describedby=\"caption-attachment-68781\"><p><span><span></span><div id=\"RIL_IMG_4\">\n        <figure>\n          <img alt=\"St Augustine, Sandro Botticelli, 1480, via WikiArt\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fcdn.thecollector.com%2Fwp-content%2Fuploads%2F2023%2F01%2Fsandro-botticelli-sant_agostino-painting.jpg%3Fwidth%3D1080%26quality%3D55\">\n          <figcaption>St Augustine, Sandro Botticelli, 1480, via WikiArt</figcaption>\n          \n        </figure>\n      </div></span></p>\n</figure>\n<p>If the world was created according to some higher plan, why is there so much absurdity and suffering in it? Most believers sooner or later begin to ask this question. <a rel=\"noopener noreferrer\" href=\"https://plato.stanford.edu/entries/evil/\" target=\"_blank\" id=\"reader.external-link.num-5\">Theodicy</a> comes to the aid of the desperate. It is a religious and philosophical concept according to which God is unconditionally recognized as an absolute good, from which any responsibility for the presence of evil in the world is removed.</p>\n\n<p><a rel=\"noopener noreferrer\" href=\"https://www.southampton.ac.uk/philosophy/undergraduate/decision-trees/god-and-evil/2-1.page\" target=\"_blank\" id=\"reader.external-link.num-6\">Leibniz</a> created this doctrine to conditionally “justify” God. The main question of this concept is: why does God not want to rid the world of misfortunes? The possible answers can be reduced to four: either God wants to rid the world of evil but cannot, or he can but does not want to, or he cannot and does not want to, or he can and wants to. The first three options do not correlate with the idea of God as the Absolute, and the last option does not explain the presence of evil in the world.</p>\n\n<p>The problem of theodicy arises in any monotheistic religion where the responsibility for evil in the world should theoretically be assigned to God. In practice, laying responsibility on God is not possible since religions recognize God as a kind of ideal being who has the right to the presumption of innocence.</p>\n\n<p>One of the main ideas of theodicy is that God created the best of all possible worlds, and, therefore, only the best is collected in it, and the presence of evil in this world is considered only as a consequence of the need for ethical diversity. Recognizing theodicy or not is a personal matter related to one’s faith, but it is certainly worth exploring the concept.</p>\n\n\n<figure aria-describedby=\"caption-attachment-68774\"><p><span><span></span><div id=\"RIL_IMG_5\">\n        <figure>\n          <img alt=\"An Hindoo Woman Throwing Herself on the Funeral Pyre of Her Husband, Frederic Shoberl, via WikiMedia\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fcdn.thecollector.com%2Fwp-content%2Fuploads%2F2023%2F01%2Ffrederic-shoberl-hindoo-woman-painting.jpg%3Fwidth%3D1400%26quality%3D55\">\n          <figcaption>An Hindoo Woman Throwing Herself on the Funeral Pyre of Her Husband, Frederic Shoberl, via WikiMedia</figcaption>\n          \n        </figure>\n      </div></span></p>\n</figure>\n<p>Life would be much easier if good and evil were fixed, absolute concepts. But often, we are faced with the fact that what is good in one situation may be evil in another. We are approaching <a rel=\"noopener noreferrer\" href=\"https://plato.stanford.edu/entries/moral-relativism/\" target=\"_blank\" id=\"reader.external-link.num-7\">moral relativism</a>, becoming less definite about what is good and what is bad. This ethical principle denies the dichotomous division of the concepts of “good” and “evil” and does not recognize the existence of mandatory, absolute moral norms and categories.</p>\n\n<p>Moral relativism, unlike moral absolutism, does not hold that there are absolute universal moral standards and principles. It is not morality that dominates the situation, but the situation over morality. That is, not just the fact of some action is important, but its context.</p>\n\n<p>The philosophical doctrine of “permissiveness” recognizes each individual’s right to form their own system of values and their own ideas about the categories of good and evil and allows us to assert that morality is, in essence, a relative concept.</p>\n\n<h2>6. Categorical Imperative or the Golden Rule of Morality</h2>\n<figure aria-describedby=\"caption-attachment-68773\"><p><span><span></span><div id=\"RIL_IMG_6\">\n        <figure>\n          <img alt=\"The Court of Death, Rembrandt Peale, 1820, via Detroit Institute of Arts\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fcdn.thecollector.com%2Fwp-content%2Fuploads%2F2023%2F01%2Frembrandt-peale-court-of-death-painting.jpg%3Fwidth%3D1400%26quality%3D55\">\n          <figcaption>The Court of Death, Rembrandt Peale, 1820, via Detroit Institute of Arts</figcaption>\n          \n        </figure>\n      </div></span></p>\n</figure>\n<p>“Treat others how you want to be treated” – surely many of us have heard this phrase or its semblance at least once. We usually agree that it is perceived as something familiar and self-evident. However, this is not just a common expression or proverb; this phrase is similar to an important philosophical concept in ethics, which is called the “<a href=\"https://www.thecollector.com/ethicists-toolbox-kant-categorical-imperative/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-8\">categorical imperative</a>” or the “golden rule” of morality.</p>\n\n<p>The term “categorical imperative” was introduced by German philosopher Immanuel <a href=\"https://www.thecollector.com/kant-philosophy-of-aesthetics/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-9\">Kant</a>, who developed the concept of an ethics based in autonomy. According to this concept, moral principles always exist, do not depend on the environment, and must constantly connect with each other. The categorical imperative says that a person must use specific principles that guide their behavior.</p>\n\n<p>According to this ethical concept, a person must act according to the maxim, which, in his opinion, <i>could</i> become a universal law. Also, within the framework of this concept, Kant proposes not to consider the other person as a means to an end, but to treat them as the ultimate goal. Of course, such an approach will not save us from mistakes, but decisions become much more conscious if you think that every time you choose, you do so not only for yourself but for all of humanity.</p>\n\n\n<figure aria-describedby=\"caption-attachment-68777\"><p><span><span></span><div id=\"RIL_IMG_7\">\n        <figure>\n          <img alt=\"Free Will Illustration, Jon Krause, via Harvard Magazine\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fcdn.thecollector.com%2Fwp-content%2Fuploads%2F2023%2F01%2Fjon-krause-free-will-illustration.jpg%3Fwidth%3D1400%26quality%3D55\">\n          <figcaption>Free Will Illustration, Jon Krause, via Harvard Magazine</figcaption>\n          \n        </figure>\n      </div></span></p>\n</figure>\n<p>If we want to investigate free will, fate and predestination, we have to consider the concept of determinism – the philosophical doctrine of predestination, the interconnectedness of everything that is happening, and the presence of a unique cause for everything that exists. Everything is predetermined. Everything will happen according to a given pattern – this is the main postulate of <a href=\"https://www.thecollector.com/baruch-spinoza-determinism-role-of-ethics-in-life/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-10\">determinism</a>.</p>\n\n<p>Free will, according to this doctrine, does not exist, and in different interpretations of determinism, the fate of a person depends on various factors: either it is predetermined by God or by an extensive philosophically comprehended category of “nature.”</p>\n\n<p>Within the framework of the doctrine of determinism, no events are considered random but are the consequence of a predetermined, but unknown to man, a chain of events. Determinism excludes belief in free will, in which all responsibility for actions lies with the person themselves, and forces the individual to entrust their fate entirely to the external world’s causality, regularity, and omnipotence. Because of this, determinism is a convenient idea for those who don’t want to take responsibility for themselves.</p>\n\n<h2>8. Cogito Ergo Sum: I Think, Therefore I Am</h2>\n<figure aria-describedby=\"caption-attachment-68780\"><p><span><span></span><div id=\"RIL_IMG_8\">\n        <figure>\n          <img alt=\"René Descartes, Jonas Suyderhoff, 1650s, via National Gallery of Art\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fcdn.thecollector.com%2Fwp-content%2Fuploads%2F2023%2F01%2Fjonas-suyderhoff-rene-descartes-portrait.jpg%3Fwidth%3D1200%26quality%3D55\">\n          <figcaption>René Descartes, Jonas Suyderhoff, 1650s, via National Gallery of Art</figcaption>\n          \n        </figure>\n      </div></span></p>\n</figure>\n<p><a href=\"https://www.thecollector.com/what-does-i-think-therefore-i-am-mean/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-11\">“I think, therefore I am”</a> is a philosophical concept originating from the rationalist philosopher Rene Descartes, and a good starting point for those who doubt everything. This formula arose when Descartes was trying to find the primary, indisputable and absolute truth, based on which one can build a philosophical concept of absolute knowledge.</p>\n\n<p><a href=\"https://www.thecollector.com/foundationalism-can-we-know-anything-philosophy/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-12\">Descartes</a> questioned everything: the outside world, his feelings, God, and public opinion. The only thing that could not be questioned was one’s own existence since the very process of doubting one’s own existence was proof of this existence. Hence the formula appeared: “I doubt, therefore, I think; I think, therefore I am,” which was transformed into “I think, therefore I am” – this phrase became the metaphysical basis of modern philosophy. It proclaimed the dominant position of the Subject, around which it became possible to build reliable knowledge.</p>\n\n\n<figure aria-describedby=\"caption-attachment-68783\"><p><span><span></span><div id=\"RIL_IMG_9\">\n        <figure>\n          <img alt=\"Friedrich Nietzsche, Edvard Munch, 1960, via WikiMedia\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fcdn.thecollector.com%2Fwp-content%2Fuploads%2F2023%2F01%2Fedvard-munch-friedrich-nietzsche-portrait.jpg%3Fwidth%3D1400%26quality%3D55\">\n          <figcaption>Friedrich Nietzsche, Edvard Munch, 1960, via WikiMedia</figcaption>\n          \n        </figure>\n      </div></span></p>\n</figure>\n<p>“<a rel=\"noopener noreferrer\" href=\"https://www.goodreads.com/quotes/22827-god-is-dead-god-remains-dead-and-we-have-killed\" target=\"_blank\" id=\"reader.external-link.num-13\">God is dead</a>. God remains dead. And we have killed him. How shall we comfort ourselves, the murderers of all murderers? What was holiest and mightiest of all that the world has yet owned has bled to death under our knives: who will wipe this blood off us?”</p>\n\n<p>In saying “God is dead,” <a href=\"https://www.thecollector.com/nietzsche-famous-works-and-ideas/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-14\">Nietzsche</a> was not implying the death of God in a literal sense. He meant that in traditional society, the existence of God was a fact; he was in a single reality with people. But in the era of modernity, he ceased to be part of external reality, becoming an internal idea. It caused a crisis in the value system, which was previously based on the Christian worldview. It means that the time has come to revise this system – in fact, this is what the philosophy and culture of postmodernity are doing.</p>\n\n<h2>10. Existential Crisis: A Contemporary Philosophical Concept</h2>\n<figure aria-describedby=\"caption-attachment-68775\"><p><span><span></span><div id=\"RIL_IMG_10\">\n        <figure>\n          <img alt=\"To Freedom, Benton Spruance, 1952, via Whitney Museum of American Art\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fcdn.thecollector.com%2Fwp-content%2Fuploads%2F2023%2F01%2Fbenton-spruance-to-freedom-painting.jpg%3Fwidth%3D1400%26quality%3D55\">\n          <figcaption>To Freedom, Benton Spruance, 1952, via Whitney Museum of American Art</figcaption>\n          \n        </figure>\n      </div></span></p>\n</figure>\n<p><a href=\"https://www.thecollector.com/what-are-main-themes-existentialism/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-15\">Existentialism</a>, one of the main philosophical currents of the 20th century, focuses on the uniqueness of human beings. It is also called the “philosophy of existence.” The forerunner of existentialism was the Danish philosopher Søren <a href=\"https://www.thecollector.com/what-is-existentialism/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-16\">Kierkegaard</a>. Back in the 19th century, he first formulated the concept of “existence,” opposing it to the “system” of the German idealist Hegel.</p>\n\n<p>An existential crisis is a feeling of anxiety and anxiety associated with the loss of the meaning of life. Existential psychologists such as Irvin Yalom and Rollo May have widely studied this concept. Essentially, an existential crisis is the loss of the meaning of life.</p>\n\n<p>An <a href=\"https://www.thecollector.com/who-were-the-most-famous-existentialists/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-17\">existential crisis</a> can be provoked by a difficult situation in the world, uncertainty in the economic sphere, the illness of a loved one, a direct encounter with death, and great life upheavals. An existential crisis is always connected with how a person lives their life, how fully and deeply, and occurs either when this life is threatened – directly or indirectly, or in a situation where a life does not “suit” the person living it.</p>\n\n<p>The notion of <a href=\"https://www.thecollector.com/difference-between-existentialism-and-nihilism/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-18\">existential crisis</a> resulted from the collapse of the traditional value system described above. It is generated by the idea that human existence has neither a predetermined purpose nor an objective meaning. It goes against our deepest need to believe that human life has value. But the absence of the original meaning does not mean the loss of meaning in general. According to the concept of existentialism, the value of life is manifested precisely in the way a person fulfills themselves, in the choices made by them, and in their actions.</p>\n</article></div>",
      "textContent": "\n        \n          \n          \n          \n        \n      \n\nWhat is the meaning of life? What are good and evil? What is justice? These are some of the questions that philosophers have been asking for centuries. Philosophy is a complex and fascinating field of study that can sometimes seem daunting to beginners. And while there’s no one answer that everyone will agree on, it’s still important to know some of the most fundamental ideas in philosophy. Here are ten common philosophical concepts everyone should be familiar with, regardless of educational background.\n\n\n\n        \n          \n          La Grotte de la Loue, Gustave Courbet, 1864, via National Gallery of Art\n          \n        \n      \n\nPlato was the first to separate the “world of things” from the “world of ideas.” According to Plato, the idea (eidos) is the source of a thing, its prototype, the underlying reality of any particular object. For example, the “idea of a table” can either coincide with a particular table in reality or not match. But the “idea of the table” and the “concrete table” will continue to exist separately.\n\nA vivid illustration of the division of the world into the world of ideas and the world of objects is the famous Platonic myth of the cave, in which people see not objects and other people but only their shadows on the wall of the cave. In this metaphor, the shadows projected on the wall of the cave correspond to the individual objects in the world, while the objects whose shadows are on the wall correspond to the ideas – which are more fundamental and real, in Plato’s view.\n\nThe cave for Plato is an allegory of our world, where people live, believing that the shadows on the walls of the caves are the only way to know reality. However, in reality, the shadows are just an illusion. Still, because of this illusion, it is difficult for people to pose critical questions about the existence of reality and overcome their “false consciousness.”\n\n\n\nGet the latest articles delivered to your inbox\nSign up to our Free Weekly Newsletter\n\nDeveloping Platonic ideas, philosophers of later times reached the concept of the transcendent and the “thing-in-itself.”\n\n\n2. The Concept of Introspection\n\n        \n          \n          Self-Portrait with Grey Felt Hat, Vincent van Gogh, 1887, via Van Gogh Museum\n          \n        \n      \n\nIntrospection is a way of achieving self-knowledge during which a person observes their internal reaction to events in the external world. Introspection is motivated by a fundamental human need to examine the self carefully, to explain to themselves why they believe what they believe and whether there is a possibility that their belief is wrong.\n\nThe founder of introspection as a method of inquiry is the British educator and philosopher John Locke, who, relying on the ideas of Rene Descartes, pointed out that there are only two direct sources of all knowledge: the objects of the external world and the human mind. In this regard, all significant psychological facts of consciousness are open to study only by the subject of knowledge itself. It may well be that “blue” for one person is not at all the same as “blue” for another.\n\nThe method of introspection helps to keep track of the stages of thinking, breaking down feelings into elements and providing a complete picture of the relationship between thoughts and actions. Introspection teaches us to think more abstractly and broadly, for example, to perceive a “big red apple” as a sensation of red, replaced by an impression of a round one and a trace of a taste sensation. But do not go too deep into introspection – excessive focus on tracking your own impressions can dull the perception of reality.\n\n\n\nIllustration to the book Mysterious Stranger by Mark Twain, Author unknown, via Project Gutenberg\nSolipsism is a philosophical concept according to which a person recognizes only their mind as the only reality that always exists and is always available. Mark Twain demonstrates the main message of solipsism in his story The Mysterious Stranger:\n\n“There is no God, no universe, no human race, no earthly life, no heaven, no hell. It is all a Dream, a grotesque and foolish dream. Nothing exists but you. And You are but a Thought – a vagrant Thought, a useless Thought, a homeless Thought, wandering forlorn among the empty eternities.”\n\nThe same idea, in general, is illustrated by the films Mr. Nobody, The Beginning, and The Matrix.\n\nAccording to solipsism, only a person’s perception of reality and their thoughts are available to them, while the entire external world is beyond the bounds of certainty. Therefore, the existence of things for a person will always be only a matter of faith, since if someone requires proof of their existence, a person will not be able to provide them.\n\nIn other words, no person can be sure of the existence of anything outside of their consciousness. Solipsism is not so much doubt about the existence of reality as a recognition of the importance of the role of one’s own mind. The concept of solipsism either needs to be assimilated as it is or to accept “solipsism in reverse,” that is, to give yourself a rational explanation of the relative external world and justify why this external world still exists.\n\n\n\n        \n          \n          St Augustine, Sandro Botticelli, 1480, via WikiArt\n          \n        \n      \n\nIf the world was created according to some higher plan, why is there so much absurdity and suffering in it? Most believers sooner or later begin to ask this question. Theodicy comes to the aid of the desperate. It is a religious and philosophical concept according to which God is unconditionally recognized as an absolute good, from which any responsibility for the presence of evil in the world is removed.\n\nLeibniz created this doctrine to conditionally “justify” God. The main question of this concept is: why does God not want to rid the world of misfortunes? The possible answers can be reduced to four: either God wants to rid the world of evil but cannot, or he can but does not want to, or he cannot and does not want to, or he can and wants to. The first three options do not correlate with the idea of God as the Absolute, and the last option does not explain the presence of evil in the world.\n\nThe problem of theodicy arises in any monotheistic religion where the responsibility for evil in the world should theoretically be assigned to God. In practice, laying responsibility on God is not possible since religions recognize God as a kind of ideal being who has the right to the presumption of innocence.\n\nOne of the main ideas of theodicy is that God created the best of all possible worlds, and, therefore, only the best is collected in it, and the presence of evil in this world is considered only as a consequence of the need for ethical diversity. Recognizing theodicy or not is a personal matter related to one’s faith, but it is certainly worth exploring the concept.\n\n\n\n        \n          \n          An Hindoo Woman Throwing Herself on the Funeral Pyre of Her Husband, Frederic Shoberl, via WikiMedia\n          \n        \n      \n\nLife would be much easier if good and evil were fixed, absolute concepts. But often, we are faced with the fact that what is good in one situation may be evil in another. We are approaching moral relativism, becoming less definite about what is good and what is bad. This ethical principle denies the dichotomous division of the concepts of “good” and “evil” and does not recognize the existence of mandatory, absolute moral norms and categories.\n\nMoral relativism, unlike moral absolutism, does not hold that there are absolute universal moral standards and principles. It is not morality that dominates the situation, but the situation over morality. That is, not just the fact of some action is important, but its context.\n\nThe philosophical doctrine of “permissiveness” recognizes each individual’s right to form their own system of values and their own ideas about the categories of good and evil and allows us to assert that morality is, in essence, a relative concept.\n\n6. Categorical Imperative or the Golden Rule of Morality\n\n        \n          \n          The Court of Death, Rembrandt Peale, 1820, via Detroit Institute of Arts\n          \n        \n      \n\n“Treat others how you want to be treated” – surely many of us have heard this phrase or its semblance at least once. We usually agree that it is perceived as something familiar and self-evident. However, this is not just a common expression or proverb; this phrase is similar to an important philosophical concept in ethics, which is called the “categorical imperative” or the “golden rule” of morality.\n\nThe term “categorical imperative” was introduced by German philosopher Immanuel Kant, who developed the concept of an ethics based in autonomy. According to this concept, moral principles always exist, do not depend on the environment, and must constantly connect with each other. The categorical imperative says that a person must use specific principles that guide their behavior.\n\nAccording to this ethical concept, a person must act according to the maxim, which, in his opinion, could become a universal law. Also, within the framework of this concept, Kant proposes not to consider the other person as a means to an end, but to treat them as the ultimate goal. Of course, such an approach will not save us from mistakes, but decisions become much more conscious if you think that every time you choose, you do so not only for yourself but for all of humanity.\n\n\n\n        \n          \n          Free Will Illustration, Jon Krause, via Harvard Magazine\n          \n        \n      \n\nIf we want to investigate free will, fate and predestination, we have to consider the concept of determinism – the philosophical doctrine of predestination, the interconnectedness of everything that is happening, and the presence of a unique cause for everything that exists. Everything is predetermined. Everything will happen according to a given pattern – this is the main postulate of determinism.\n\nFree will, according to this doctrine, does not exist, and in different interpretations of determinism, the fate of a person depends on various factors: either it is predetermined by God or by an extensive philosophically comprehended category of “nature.”\n\nWithin the framework of the doctrine of determinism, no events are considered random but are the consequence of a predetermined, but unknown to man, a chain of events. Determinism excludes belief in free will, in which all responsibility for actions lies with the person themselves, and forces the individual to entrust their fate entirely to the external world’s causality, regularity, and omnipotence. Because of this, determinism is a convenient idea for those who don’t want to take responsibility for themselves.\n\n8. Cogito Ergo Sum: I Think, Therefore I Am\n\n        \n          \n          René Descartes, Jonas Suyderhoff, 1650s, via National Gallery of Art\n          \n        \n      \n\n“I think, therefore I am” is a philosophical concept originating from the rationalist philosopher Rene Descartes, and a good starting point for those who doubt everything. This formula arose when Descartes was trying to find the primary, indisputable and absolute truth, based on which one can build a philosophical concept of absolute knowledge.\n\nDescartes questioned everything: the outside world, his feelings, God, and public opinion. The only thing that could not be questioned was one’s own existence since the very process of doubting one’s own existence was proof of this existence. Hence the formula appeared: “I doubt, therefore, I think; I think, therefore I am,” which was transformed into “I think, therefore I am” – this phrase became the metaphysical basis of modern philosophy. It proclaimed the dominant position of the Subject, around which it became possible to build reliable knowledge.\n\n\n\n        \n          \n          Friedrich Nietzsche, Edvard Munch, 1960, via WikiMedia\n          \n        \n      \n\n“God is dead. God remains dead. And we have killed him. How shall we comfort ourselves, the murderers of all murderers? What was holiest and mightiest of all that the world has yet owned has bled to death under our knives: who will wipe this blood off us?”\n\nIn saying “God is dead,” Nietzsche was not implying the death of God in a literal sense. He meant that in traditional society, the existence of God was a fact; he was in a single reality with people. But in the era of modernity, he ceased to be part of external reality, becoming an internal idea. It caused a crisis in the value system, which was previously based on the Christian worldview. It means that the time has come to revise this system – in fact, this is what the philosophy and culture of postmodernity are doing.\n\n10. Existential Crisis: A Contemporary Philosophical Concept\n\n        \n          \n          To Freedom, Benton Spruance, 1952, via Whitney Museum of American Art\n          \n        \n      \n\nExistentialism, one of the main philosophical currents of the 20th century, focuses on the uniqueness of human beings. It is also called the “philosophy of existence.” The forerunner of existentialism was the Danish philosopher Søren Kierkegaard. Back in the 19th century, he first formulated the concept of “existence,” opposing it to the “system” of the German idealist Hegel.\n\nAn existential crisis is a feeling of anxiety and anxiety associated with the loss of the meaning of life. Existential psychologists such as Irvin Yalom and Rollo May have widely studied this concept. Essentially, an existential crisis is the loss of the meaning of life.\n\nAn existential crisis can be provoked by a difficult situation in the world, uncertainty in the economic sphere, the illness of a loved one, a direct encounter with death, and great life upheavals. An existential crisis is always connected with how a person lives their life, how fully and deeply, and occurs either when this life is threatened – directly or indirectly, or in a situation where a life does not “suit” the person living it.\n\nThe notion of existential crisis resulted from the collapse of the traditional value system described above. It is generated by the idea that human existence has neither a predetermined purpose nor an objective meaning. It goes against our deepest need to believe that human life has value. But the absence of the original meaning does not mean the loss of meaning in general. According to the concept of existentialism, the value of life is manifested precisely in the way a person fulfills themselves, in the choices made by them, and in their actions.\n",
      "length": 14673,
      "excerpt": "",
      "siteName": null,
      "summary": "Philosophy is a complex and fascinating field of study that can sometimes seem daunting to beginners. Here are ten common philosophical concepts everyone should be familiar with, regardless of educational background. The concept of introspection is motivated by a fundamental human need to examine the self carefully, to explain why they believe what they believe.",
      "topics": [
        {
          "label": "Education & Reference",
          "score": 0.9864213466644287
        },
        {
          "label": "Science & Mathematics",
          "score": 0.9063573479652405
        },
        {
          "label": "Society & Culture",
          "score": 0.8997812867164612
        }
      ]
    }
  },
  "3799163788": {
    "item_id": "3799163788",
    "resolved_id": "3799163788",
    "given_url": "https://www.technologynetworks.com/neuroscience/news/amino-acid-supplements-ease-nerve-damage-in-diabetic-mice-369567",
    "given_title": "Amino Acid Supplements Ease Nerve Damage in Diabetic Mice | Technology Netw",
    "favorite": "0",
    "status": "0",
    "time_added": "1675709779",
    "time_updated": "1675709782",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 15,
    "resolved_title": "Amino Acid Supplements Ease Nerve Damage in Diabetic Mice",
    "resolved_url": "https://www.technologynetworks.com/neuroscience/news/amino-acid-supplements-ease-nerve-damage-in-diabetic-mice-369567",
    "excerpt": "Salk scientists discover that altered serine metabolism in diabetes leads to peripheral neuropathy—a finding that may provide a new way to identify people at high risk and a potential treatment option Approximately half of people with type 1 or type 2 diabetes experience peripheral neuropathy—w",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "0",
    "word_count": "745",
    "lang": "en",
    "time_to_read": 3,
    "top_image_url": "https://assets.technologynetworks.com/production/dynamic/images/content/369567/amino-acid-supplements-ease-nerve-damage-in-diabetic-mice-369567-640x360.jpg?cb=20230126",
    "tags": {
      "readinglist": {
        "item_id": "3799163788",
        "tag": "readinglist"
      }
    },
    "listen_duration_estimate": 288,
    "content": {
      "title": "Amino Acid Supplements Ease Nerve Damage in Diabetic Mice",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><div lang=\"en\">\n<p>Salk scientists discover that altered serine metabolism in diabetes leads to peripheral neuropathy—a finding that may provide a new way to identify people at high risk and a potential treatment option</p>\n\n<p>Approximately half of people with type 1 or type 2 diabetes experience peripheral neuropathy—weakness, numbness, and pain, primarily in the hands and feet. The condition occurs when high levels of sugar circulating in the blood damage peripheral nerves. Now, working with mice, Salk Institute researchers have identified another factor contributing to diabetes-associated peripheral neuropathy: altered amino acid metabolism.  <br></p>\n\n<p>The team found that diabetic mice with low levels of two related amino acids, serine and glycine, are at higher risk for peripheral neuropathy. What’s more, the researchers were able to alleviate neuropathy symptoms in diabetic mice by supplementing their diets with serine.</p>\n\n<p>The study, published January 25, 2023 in Nature, adds to growing evidence that some often-underappreciated, “non-essential” amino acids play important roles in the nervous system. The findings may provide a new way to identify people at high risk for peripheral neuropathy, as well as a potential treatment option.</p>\n\n<p>“We were surprised that dialing up and down a non-essential amino acid had such a profound effect on metabolism and diabetic complications,” says senior author <a href=\"https://www.salk.edu/scientist/christian-metallo/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-0\">Christian Metallo</a>, a professor in Salk’s Molecular and Cell Biology Laboratory. “It just goes to show that what we think of as dogma can change under different circumstances, such as in disease conditions.” Metallo led the study with first author Michal Handzlik, a postdoctoral researcher in his lab.</p>\n\n<p>Amino acids are the building blocks that make up proteins and specialized fat molecules called sphingolipids, which are abundant in the nervous system. Low levels of the amino acid serine force the body to incorporate a different amino acid in sphingolipids, which changes their structure. These atypical sphingolipids then accumulate, which may contribute to peripheral nerve damage. While the team observed this accumulation in diabetic mice, the same amino acid switch and sphingolipid changes occur in a rare human genetic disease marked by peripheral sensory neuropathy, indicating that the phenomenon is consistent across many species.</p>\n\n<p>To determine whether long-term, chronic serine deficiency drives peripheral neuropathy, Metallo’s team fed mice either control or serine-free diets in combination with either low-fat or high-fat diets for up to 12 months. The researchers were surprised to find that low serine, in combination with a high-fat diet, accelerated the onset of peripheral neuropathy in the mice. In contrast, serine supplementation in diabetic mice slowed the progression of peripheral neuropathy, and the mice fared better.</p>\n\n<p>The researchers also tested the compound myriocin, which inhibits the enzyme that switches out serine for another amino acid as sphingolipids are assembled. Myriocin treatment reduced peripheral neuropathy symptoms in mice fed a high-fat, serine-free diet. These findings underscore the importance of amino acid metabolism and sphingolipid production in the maintenance of a healthy peripheral nervous system.</p>\n\n<p>Serine deficiency has also been associated with various neurodegenerative disorders. For example, Metallo and collaborators previously found a link between altered serine and sphingolipid metabolism in patients with macular telangiectasia type 2, a condition that causes vision loss. In mice, reduced serine led to increased levels of atypical retinal sphingolipids and reduced vision. Serine is currently being tested in clinical trials for its safety and efficacy in treating macular telangiectasia and Alzheimer’s disease.</p>\n\n<p>Peripheral neuropathy is typically managed with dietary changes to reduce blood sugar levels, as well as pain relievers, physical therapy, and mobility aids, such as canes and wheelchairs. Foods naturally rich in serine include soybeans, nuts, eggs, chickpeas, lentils, meat, and fish, and serine supplements are inexpensive and available over the counter.</p>\n\n<p>Yet the researchers say it’s premature to advise people with diabetes to take serine supplements to prevent neuropathy.</p>\n\n<p>“You would likely need to take a lot to make a difference, and not everyone needs extra serine,” Metallo says. “We need more time to understand serine physiology in humans and explore potential downsides to supplementation.”</p>\n\n<p>To this end, Metallo and Handzlik are now developing a serine tolerance test, similar to a glucose tolerance test used to diagnose diabetes.</p>\n\n<p>“We want to identify those at highest risk for peripheral neuropathy so we can treat only those who might benefit most,” says Handzlik.</p>\n\n<p><b>Reference:</b> <span>Handzlik MK, Gengatharan JM, Frizzi KE, et al. Insulin-regulated serine and lipid metabolism drive peripheral neuropathy.</span> <i>Nature</i><span>. 2023. doi:</span> <a href=\"https://doi.org/10.1038/s41586-022-05637-6\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-1\"><span>10.1038/s41586-022-05637-6</span></a></p>\n\n<p><i><span>This article has been republished from the following <a href=\"https://www.salk.edu/news-release/supplementation-with-amino-acid-serine-eases-neuropathy-in-diabetic-mice/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-2\">materials.</a> Note: material may have been edited for length and content. For further information, please contact the cited source.</span></i></p>\n\n\n</div></article></div>",
      "textContent": "\nSalk scientists discover that altered serine metabolism in diabetes leads to peripheral neuropathy—a finding that may provide a new way to identify people at high risk and a potential treatment option\n\nApproximately half of people with type 1 or type 2 diabetes experience peripheral neuropathy—weakness, numbness, and pain, primarily in the hands and feet. The condition occurs when high levels of sugar circulating in the blood damage peripheral nerves. Now, working with mice, Salk Institute researchers have identified another factor contributing to diabetes-associated peripheral neuropathy: altered amino acid metabolism.  \n\nThe team found that diabetic mice with low levels of two related amino acids, serine and glycine, are at higher risk for peripheral neuropathy. What’s more, the researchers were able to alleviate neuropathy symptoms in diabetic mice by supplementing their diets with serine.\n\nThe study, published January 25, 2023 in Nature, adds to growing evidence that some often-underappreciated, “non-essential” amino acids play important roles in the nervous system. The findings may provide a new way to identify people at high risk for peripheral neuropathy, as well as a potential treatment option.\n\n“We were surprised that dialing up and down a non-essential amino acid had such a profound effect on metabolism and diabetic complications,” says senior author Christian Metallo, a professor in Salk’s Molecular and Cell Biology Laboratory. “It just goes to show that what we think of as dogma can change under different circumstances, such as in disease conditions.” Metallo led the study with first author Michal Handzlik, a postdoctoral researcher in his lab.\n\nAmino acids are the building blocks that make up proteins and specialized fat molecules called sphingolipids, which are abundant in the nervous system. Low levels of the amino acid serine force the body to incorporate a different amino acid in sphingolipids, which changes their structure. These atypical sphingolipids then accumulate, which may contribute to peripheral nerve damage. While the team observed this accumulation in diabetic mice, the same amino acid switch and sphingolipid changes occur in a rare human genetic disease marked by peripheral sensory neuropathy, indicating that the phenomenon is consistent across many species.\n\nTo determine whether long-term, chronic serine deficiency drives peripheral neuropathy, Metallo’s team fed mice either control or serine-free diets in combination with either low-fat or high-fat diets for up to 12 months. The researchers were surprised to find that low serine, in combination with a high-fat diet, accelerated the onset of peripheral neuropathy in the mice. In contrast, serine supplementation in diabetic mice slowed the progression of peripheral neuropathy, and the mice fared better.\n\nThe researchers also tested the compound myriocin, which inhibits the enzyme that switches out serine for another amino acid as sphingolipids are assembled. Myriocin treatment reduced peripheral neuropathy symptoms in mice fed a high-fat, serine-free diet. These findings underscore the importance of amino acid metabolism and sphingolipid production in the maintenance of a healthy peripheral nervous system.\n\nSerine deficiency has also been associated with various neurodegenerative disorders. For example, Metallo and collaborators previously found a link between altered serine and sphingolipid metabolism in patients with macular telangiectasia type 2, a condition that causes vision loss. In mice, reduced serine led to increased levels of atypical retinal sphingolipids and reduced vision. Serine is currently being tested in clinical trials for its safety and efficacy in treating macular telangiectasia and Alzheimer’s disease.\n\nPeripheral neuropathy is typically managed with dietary changes to reduce blood sugar levels, as well as pain relievers, physical therapy, and mobility aids, such as canes and wheelchairs. Foods naturally rich in serine include soybeans, nuts, eggs, chickpeas, lentils, meat, and fish, and serine supplements are inexpensive and available over the counter.\n\nYet the researchers say it’s premature to advise people with diabetes to take serine supplements to prevent neuropathy.\n\n“You would likely need to take a lot to make a difference, and not everyone needs extra serine,” Metallo says. “We need more time to understand serine physiology in humans and explore potential downsides to supplementation.”\n\nTo this end, Metallo and Handzlik are now developing a serine tolerance test, similar to a glucose tolerance test used to diagnose diabetes.\n\n“We want to identify those at highest risk for peripheral neuropathy so we can treat only those who might benefit most,” says Handzlik.\n\nReference: Handzlik MK, Gengatharan JM, Frizzi KE, et al. Insulin-regulated serine and lipid metabolism drive peripheral neuropathy. Nature. 2023. doi: 10.1038/s41586-022-05637-6\n\nThis article has been republished from the following materials. Note: material may have been edited for length and content. For further information, please contact the cited source.\n\n\n",
      "length": 5121,
      "excerpt": "Salk scientists discover that altered serine metabolism in diabetes leads to peripheral neuropathy—a finding that may provide a new way to identify people at high risk and a potential treatment option",
      "siteName": null,
      "summary": "Salk scientists discover that altered serine metabolism in diabetes leads to peripheral neuropathy. Finding that may provide a new way to identify people at high risk and a potential treatment option. Low levels of the amino acid serine force the body to incorporate a different amino acid in sphingolipids, which changes their structure.",
      "topics": [
        {
          "label": "Health",
          "score": 0.9970974922180176
        },
        {
          "label": "Science & Mathematics",
          "score": 0.9636068344116211
        },
        {
          "label": "Education & Reference",
          "score": 0.726840615272522
        }
      ]
    }
  },
  "3801448807": {
    "item_id": "3801448807",
    "resolved_id": "3801360613",
    "given_url": "https://magazine.sebastianraschka.com/p/ahead-of-ai-5-revaival-of-ideas?utm_source=substack&utm_medium=email",
    "given_title": "Ahead of AI #5: RevAIval of Ideas - by Sebastian Raschka",
    "favorite": "0",
    "status": "0",
    "time_added": "1675794733",
    "time_updated": "1675794739",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 14,
    "resolved_title": "Ahead of AI #5: RevAIval of Ideas",
    "resolved_url": "https://magazine.sebastianraschka.com/p/ahead-of-ai-5-revaival-of-ideas",
    "excerpt": "I hope you had a successful start to the new year, as did AI and deep learning research. In this edition of Ahead of AI #5, I wanted to showcase recent advancements in computer vision rather than simply covering the increasing popularity of large language models.",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "1",
    "word_count": "4623",
    "lang": "en",
    "time_to_read": 21,
    "top_image_url": "https://substackcdn.com/image/fetch/w_1200,h_600,c_limit,f_jpg,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d595462-d150-4e99-a58f-b651d8da610b_1014x576.png",
    "tags": {
      "readinglist": {
        "item_id": "3801448807",
        "tag": "readinglist"
      }
    },
    "authors": {
      "174510950": {
        "item_id": "3801448807",
        "author_id": "174510950",
        "name": "Sebastian Raschka",
        "url": "https://substack.com/profile/27393275-sebastian-raschka"
      }
    },
    "image": {
      "item_id": "3801448807",
      "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d595462-d150-4e99-a58f-b651d8da610b_1014x576.png",
      "width": "465",
      "height": "264"
    },
    "images": {
      "1": {
        "item_id": "3801448807",
        "image_id": "1",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d595462-d150-4e99-a58f-b651d8da610b_1014x576.png",
        "width": "465",
        "height": "264",
        "credit": "",
        "caption": "Source: Masked Autoencoders Are Scalable Vision Learners, https://arxiv.org/abs/2111.06377"
      },
      "2": {
        "item_id": "3801448807",
        "image_id": "2",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5ded48b-81f4-4d83-952f-db04b26d8a01_1240x1512.png",
        "width": "537",
        "height": "655",
        "credit": "masked",
        "caption": "Annotated figure from https://arxiv.org/abs/2301.03580 outlining the problems of regular convolutions for sparse"
      },
      "3": {
        "item_id": "3801448807",
        "image_id": "3",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2e8d956d-97d5-42f0-abd3-afa256a1fcff_1418x784.png",
        "width": "513",
        "height": "284",
        "credit": "",
        "caption": "Annotated figure from https://arxiv.org/abs/2301.03580 outlining the SparK CNN approach."
      },
      "4": {
        "item_id": "3801448807",
        "image_id": "4",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8afd6c1b-c952-44a0-82ea-686da994bbe6_1190x1146.png",
        "width": "543",
        "height": "523",
        "credit": "",
        "caption": "Source: Annotated figures from “ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders”, https://arxiv.org/abs/2301.00808"
      },
      "5": {
        "item_id": "3801448807",
        "image_id": "5",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c359b13-202d-413f-ac8b-5ac3e6a7a357_990x1042.png",
        "width": "377",
        "height": "397",
        "credit": "",
        "caption": "Source: https://arxiv.org/abs/2301.00808"
      },
      "6": {
        "item_id": "3801448807",
        "image_id": "6",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2423346c-7b72-4c38-bbc3-74b94a9270d3_834x766.png",
        "width": "403",
        "height": "370",
        "credit": "",
        "caption": "Source: https://arxiv.org/abs/2301.09515"
      },
      "7": {
        "item_id": "3801448807",
        "image_id": "7",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd21b8b3e-dacf-424a-b209-72157a09d931_1148x602.png",
        "width": "577",
        "height": "303",
        "credit": "",
        "caption": "https://lightning.ai/pages/courses/deep-learning-fundamentals/"
      },
      "8": {
        "item_id": "3801448807",
        "image_id": "8",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fef45bf44-524b-479c-841f-8d0a0f6c7f7c_1264x1252.png",
        "width": "573",
        "height": "568",
        "credit": "",
        "caption": ""
      },
      "9": {
        "item_id": "3801448807",
        "image_id": "9",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F19653deb-c4d4-408d-b51d-3ccc4bc9daf7_1242x822.png",
        "width": "435",
        "height": "288",
        "credit": "",
        "caption": ""
      },
      "10": {
        "item_id": "3801448807",
        "image_id": "10",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8fd0af6b-b056-463a-b26d-49e4a3f84514_802x494.png",
        "width": "379",
        "height": "233",
        "credit": "",
        "caption": ""
      },
      "11": {
        "item_id": "3801448807",
        "image_id": "11",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5676653a-6ccf-4dca-a942-a35af3c73b4f_678x438.png",
        "width": "370",
        "height": "239",
        "credit": "",
        "caption": ""
      },
      "12": {
        "item_id": "3801448807",
        "image_id": "12",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb955db2-25fd-4035-b963-d0f60fbdbbba_660x378.png",
        "width": "364",
        "height": "208",
        "credit": "",
        "caption": ""
      },
      "13": {
        "item_id": "3801448807",
        "image_id": "13",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f3cddba-7bbc-42ea-be62-1f5f84afc98d_1208x746.png",
        "width": "485",
        "height": "300",
        "credit": "",
        "caption": ""
      },
      "14": {
        "item_id": "3801448807",
        "image_id": "14",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff83407bf-ad3f-47ce-b8df-adbdd8f2f77a_1334x1062.png",
        "width": "457",
        "height": "364",
        "credit": "",
        "caption": ""
      },
      "15": {
        "item_id": "3801448807",
        "image_id": "15",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d81cfb4-555e-442c-a8e2-d43883faa7e1_1572x540.png",
        "width": "485",
        "height": "167",
        "credit": "",
        "caption": ""
      },
      "16": {
        "item_id": "3801448807",
        "image_id": "16",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf525de5-778f-42da-8e4f-9af7650b6baa_4207x2946.png",
        "width": "503",
        "height": "352",
        "credit": "",
        "caption": ""
      },
      "17": {
        "item_id": "3801448807",
        "image_id": "17",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0cee4743-7ee8-463d-8d92-9f14b303c592_4137x2687.png",
        "width": "441",
        "height": "287",
        "credit": "",
        "caption": ""
      },
      "18": {
        "item_id": "3801448807",
        "image_id": "18",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2148286e-22a8-465c-b8d5-70219d7f21dd_1711x2329.png",
        "width": "499",
        "height": "679",
        "credit": "",
        "caption": ""
      },
      "19": {
        "item_id": "3801448807",
        "image_id": "19",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d109817-853c-43c0-8bc4-311a5e78d6e4_1032x1554.jpeg",
        "width": "341",
        "height": "513",
        "credit": "",
        "caption": ""
      },
      "20": {
        "item_id": "3801448807",
        "image_id": "20",
        "src": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5299bf34-18da-4800-950a-90dbaea6cd6b_961x956.png",
        "width": "291",
        "height": "289",
        "credit": "",
        "caption": ""
      }
    },
    "listen_duration_estimate": 1790,
    "content": {
      "title": "",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><div lang=\"en\">\n<p>I hope you had a successful start to the new year, as did AI and deep learning research. In this edition of Ahead of AI #5, I wanted to showcase recent advancements in computer vision rather than simply covering the increasing popularity of large language models. This newsletter aims to revive ideas and take convolutional neural networks to new heights.</p>\n<p>But do not worry; I didn't forget about the large language model headlines this month ...</p>\n\n<p>This issue of Ahead of AI features:</p>\n<ul><li>\n<p>The latest advances for pretraining convolutional network architectures via self-supervised learning;</p>\n</li>\n<li>\n<p>The benefits of training LLMs from scratch;</p>\n</li>\n<li>\n<p>Open source highlights;</p>\n</li>\n<li>\n<p>Thirteen alternatives to supervised learning when you have limited training data.</p>\n</li>\n</ul><p><span>PS: If the newsletter appears truncated or clipped, that's because some email providers may truncate longer email messages. In this case, you can access the full article at</span> <a rel=\"noopener noreferrer\" href=\"http://magazine.sebastianraschka.com/\" target=\"_blank\" id=\"reader.external-link.num-0\">magazine.sebastianraschka.com</a><span>.</span></p>\n<h2><strong>Articles &amp; Trends</strong></h2>\n\n<p>We will open this section with a discussion of self-supervised learning for convolutional neural networks and the latest GAN vs. diffusion model comparisons. After our short excursion to computer vision, we will look at some of the benefits of training large language models from scratch and close this section with the latest headlines (since ChatGPT wants the final say).</p>\n<h2><strong>Next-Generation Convolutional Neural Networks</strong></h2>\n\n<p><span>Self-supervised learning, which lets us leverage large unlabeled datasets for supervised pretraining, has been one of the keys to success behind language and vision transformers. However, self-supervised learning techniques like masked auto-encoding don't necessarily work well for convolutional neural networks (CNNs). At their very best,</span> <a rel=\"noopener noreferrer\" href=\"https://arxiv.org/abs/2208.00173\" target=\"_blank\" id=\"reader.external-link.num-1\">they lack behind older self-supervised learning techniques such as contrastive learning</a><span>.</span></p>\n<div id=\"RIL_IMG_1\">\n        <figure>\n          <img alt=\"Source: Masked Autoencoders Are Scalable Vision Learners, https://arxiv.org/abs/2111.06377\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F3d595462-d150-4e99-a58f-b651d8da610b_1014x576.png\">\n          <figcaption>Source: Masked Autoencoders Are Scalable Vision Learners, https://arxiv.org/abs/2111.06377</figcaption>\n          \n        </figure>\n      </div>\n<p><span>So, how can we leverage successful pretraining techniques used in language and vision transformers to improve purely convolutional networks? The answer is</span> <em><a rel=\"noopener noreferrer\" href=\"https://arxiv.org/abs/1409.6070v1\" target=\"_blank\" id=\"reader.external-link.num-2\">sparse convolutions</a></em><span>, a type of convolution operation that only considers a subset of input feature maps or weights to reduce computation and memory usage while still capturing essential features in the data.</span></p>\n<p><strong>Applying self-supervised learning via masked autoencoding to CNNs</strong></p>\n<p>What's the problem with using state-of-the-art self-supervised learning techniques in CNNs? Traditional CNNs are not well-suited for handling irregular, randomly masked input images, which we encounter using this pretraining scheme.</p>\n<p>What are the issues with regular convolutions on masked pixels?</p>\n<ol><li>\n<p>Computations on masked pixels are redundant (inefficient);</p>\n</li>\n<li>\n<p>They disturb the data distribution of pixel values (point 2 in the figure below);</p>\n</li>\n<li>\n<p>Patterns on masked maps will vanish (point 3 in the figure below).</p>\n</li>\n</ol><div id=\"RIL_IMG_2\">\n        <figure>\n          <img alt=\"Annotated figure from https://arxiv.org/abs/2301.03580 outlining the problems of regular convolutions for sparse\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Fa5ded48b-81f4-4d83-952f-db04b26d8a01_1240x1512.png\">\n          <figcaption>Annotated figure from https://arxiv.org/abs/2301.03580 outlining the problems of regular convolutions for sparse</figcaption>\n          \n        </figure>\n      </div>\n<p><span>In the recent</span> <em><a rel=\"noopener noreferrer\" href=\"https://arxiv.org/abs/2301.03580\" target=\"_blank\" id=\"reader.external-link.num-3\">Designing BERT for convolutional networks</a></em> <span>paper, researchers proposed using sparse convolutions to address issues of CNNs with masked inputs — they proposed SparK (Sparse masKed modeling with hierarchy).</span></p>\n<div id=\"RIL_IMG_3\">\n        <figure>\n          <img alt=\"Annotated figure from https://arxiv.org/abs/2301.03580 outlining the SparK CNN approach.\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F2e8d956d-97d5-42f0-abd3-afa256a1fcff_1418x784.png\">\n          <figcaption>Annotated figure from https://arxiv.org/abs/2301.03580 outlining the SparK CNN approach.</figcaption>\n          \n        </figure>\n      </div>\n<p>SparK can be applied to any convolutional network. For instance, it has been used with ResNet and ConvNeXt, improving predictive performance by up to 1.7% on ImageNet when those purely convolutional networks are pretrained with 1.28 million unlabeled images.</p>\n<p>\n<figure>\n<figcaption><span>Annotated figure from</span> <a rel=\"noopener noreferrer\" href=\"https://arxiv.org/abs/2301.03580\" target=\"_blank\" id=\"reader.external-link.num-4\">https://arxiv.org/abs/2301.03580</a></figcaption></figure></p>\n<p><strong>Does it</strong> <em><strong>really</strong></em> <strong>work?</strong></p>\n<p>The numbers above look pretty convincing, but sometimes results in papers don't generalize well to other architectures or problems. So, it's always nice to consider additional points of evidence.</p>\n<p><span>In a recent independent paper, researchers proposed modifying the popular ConvNext architecture with sparse convolutions and global response normalization:</span> <a rel=\"noopener noreferrer\" href=\"https://arxiv.org/abs/2301.00808\" target=\"_blank\" id=\"reader.external-link.num-5\">ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</a><span>.</span></p>\n<p><span>This</span> <em>ConvNeXt v2</em> <span>paper co-designs a pure convolutional network for self-supervised via two mechanisms:</span></p>\n<ol><li>\n<p>using sparse convolutions for sparse patches with a high-mask ratio;</p>\n</li>\n<li>\n<p>adding a new global response normalization layer.</p>\n</li>\n</ol><div id=\"RIL_IMG_4\">\n        <figure>\n          <img alt=\"Source: Annotated figures from “ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders”, https://arxiv.org/abs/2301.00808\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F8afd6c1b-c952-44a0-82ea-686da994bbe6_1190x1146.png\">\n          <figcaption>Source: Annotated figures from “ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders”, https://arxiv.org/abs/2301.00808</figcaption>\n          \n        </figure>\n      </div>\n<p>(1) The use of sparse convolutions allows using high-mask ratios and boosts efficiency during training. Note that during fine-tuning, we can convert the sparse convolutional parts back into standard convolutions.</p>\n<p>(2) The global response normalization part is a new type of normalization layer that replaces batch normalization and layer normalization. Without global response normalization, the high-mask ratio via (1) results in feature collapse. The authors found that using global response normalization promotes feature diversity. And it was necessary to achieve good predictive performance. The global response normalization layer is a general technique, and it will be interesting to see whether it can also benefit other architectures in the future.</p>\n<p><span>Like its predecessor, (ConvNeXt, which we covered in</span> <a rel=\"noopener noreferrer\" href=\"https://magazine.sebastianraschka.com/p/ahead-of-ai-4-a-big-year-for-ai\" target=\"_blank\" id=\"reader.external-link.num-6\">Ahead of AI #4</a><span>) we can use ConvNeXt v2 for image classification, object detection, and instance segmentation.</span></p>\n<div id=\"RIL_IMG_5\">\n        <figure>\n          <img alt=\"Source: https://arxiv.org/abs/2301.00808\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F3c359b13-202d-413f-ac8b-5ac3e6a7a357_990x1042.png\">\n          <figcaption>Source: https://arxiv.org/abs/2301.00808</figcaption>\n          \n        </figure>\n      </div>\n<h2><strong>Remember GANs?</strong></h2>\n\n<p>Last year, I jokingly claimed that GANs were dead, but this month, GANs made a strong comeback.</p>\n<p><span>Researchers have created a GAN, StyleGAN-T, that surpasses distilled diffusion models as the leading text-to-image synthesis method in terms of quality and speed, according to the paper</span> <em><a rel=\"noopener noreferrer\" href=\"https://arxiv.org/abs/2301.09515\" target=\"_blank\" id=\"reader.external-link.num-7\">StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis</a></em><span>.</span></p>\n<div id=\"RIL_IMG_6\">\n        <figure>\n          <img alt=\"Source: https://arxiv.org/abs/2301.09515\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F2423346c-7b72-4c38-bbc3-74b94a9270d3_834x766.png\">\n          <figcaption>Source: https://arxiv.org/abs/2301.09515</figcaption>\n          \n        </figure>\n      </div>\n<p>I still believe that the diffusion models are the future for generative modeling in the computer vision domain, we shouldn't overlook GANs just yet. And competition is good for business anyways.</p>\n<h2><strong>Training a Large Language Model (LLM) On A Single GPU?</strong></h2>\n\n<p>Yes, that's possible! While contemporary PyTorch open-source libraries make it easy to train large models on multiple GPUs, we often don't have access to extensive hardware resources. The question is whether we (still) can train a large deep learning model on a single GPU like in the old times.</p>\n<p><span>In the paper</span> <em><a rel=\"noopener noreferrer\" href=\"https://arxiv.org/abs/2212.14034\" target=\"_blank\" id=\"reader.external-link.num-8\">Cramming: Training a Language Model on a Single GPU in One Day</a></em><span>, the researchers trained a masked language model / encoder-style LLM (here: BERT) for 24h on 1 GPU. For comparison, the original</span> <a rel=\"noopener noreferrer\" href=\"https://arxiv.org/abs/1810.04805\" target=\"_blank\" id=\"reader.external-link.num-9\">2018 BERT paper</a> <span>trained it on 16 TPUs for four days.</span></p>\n<p><span>The impressive outcome of this</span> <em>cramming</em> <span>project was that the researchers were able to train BERT with a 78.6 average performance (compared to 80.9) — the larger, the better.</span></p>\n<p>What were some of the performance-squeezing tricks?</p>\n<ul><li>\n<p>Using automated operator fusion &amp; 32/16-bit mixed precision training;</p>\n</li>\n<li>\n<p>disabling biases in QKV attention matrices and fully-connected layers;</p>\n</li>\n<li>\n<p>Decreasing the input length from 512 -&gt; 128 tokens;</p>\n</li>\n</ul><p>And what didn't work?</p>\n<ul><li>\n<p>There were no benefits in replacing the original multi-head self-attention mechanism with FLASH attention or Fourier attention;</p>\n</li>\n<li>\n<p>There was no advantage after changing GELU activations to something else;</p>\n</li>\n<li>\n<p>Shrinking the number of attention heads is bad; keeping the original 12 attention heads is essential to maintain finetuning performance.</p>\n</li>\n</ul><p>From a predictive performance standpoint, they furthermore observed that</p>\n<ul><li>\n<p>a triangular one-cycle learning rate schedules work best;</p>\n</li>\n<li>\n<p>dropout was not needed during pretraining due to the extensive training dataset and 1-epoch training schedule;</p>\n</li>\n<li>\n<p>increasing the vocabulary size past 32k does not improve GLUE performance (but MNLI improves).</p>\n</li>\n</ul><p>\n<figure>\n<figcaption>Annotated figure from https://arxiv.org/abs/2212.14034</figcaption></figure></p>\n<p>Now, all that being said, why don't we simply train smaller models if we have limited resources? To answer this question with some parting advice from this paper: while smaller models have higher throughput, smaller models also learn less efficiently. Thus, larger models do not require more time to train for reaching a specific predictive performance threshold.</p>\n<p><strong>Encoder-Style LLMs?</strong></p>\n<p><span>Let's talk a bit more about the encoder-style models used in the</span> <em>cramming</em> <span>paper above.</span></p>\n<p><span>The recent buzz around large language models is entirely around decoder-style LLMs such as</span> <a rel=\"noopener noreferrer\" href=\"https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\" target=\"_blank\" id=\"reader.external-link.num-10\">PalM</a><span>,</span> <a rel=\"noopener noreferrer\" href=\"https://arxiv.org/abs/2203.15556\" target=\"_blank\" id=\"reader.external-link.num-11\">Chinchilla</a><span>, and the GPT family that learn to generate text based on being pretrained via next-word prediction. They are sometimes also called</span> <em>autoregressive</em> <span>and</span> <em>unidirectional</em> <span>as they process text from left to right, one token at a time.</span></p>\n<p>In contrast, encoder-style LLMs like BERT are pretrained via masked language modeling — we hide or mask tokens that the LLM has to predict. Since the LLM sees the complete sentence — except for the masked words, of course. This is also referred to as a bidirectional LLM.</p>\n<p>As a rule of thumb, decoder-style LLMs are usually better for generative modeling, whereas encoder-style LLMs are better for predictive modeling (think of text classification).</p>\n\n<p>While generative modeling is usually considered as more exciting and \"magical,\" that's typically what we see on social media and in newspaper headlines.</p>\n<p>However, I'd argue that most real-world business problems revolve around and rely upon predictive modeling.</p>\n<p><span>Anyways, given that there are now hundreds of different large language transformers out there, it's sometimes easy to lose track. In this case, I highly recommend bookmarking Xavier Amatriain's comprehensive transformer model summaries and family tree:</span> <a rel=\"noopener noreferrer\" href=\"https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/\" target=\"_blank\" id=\"reader.external-link.num-12\">Transformer models: an introduction and catalog — 2023 Edition</a><span>.</span></p>\n<p><strong>Which GPU?</strong></p>\n<p><span>By the way, if you are wondering which GPU gives you the best bang for the buck, I recommend checking out Tim Dettmer's recently updated</span> <a rel=\"noopener noreferrer\" href=\"https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/\" target=\"_blank\" id=\"reader.external-link.num-13\">The Best GPUs for Deep Learning in 2023</a> <span>guide.</span></p>\n<p>Also, to find the optimal LLM size for your compute budget, check out the handy new widget at</p>\n<p>https://howmanyparams.com</p>\n\n<h2><strong>Why do we need to train LLMs from scratch anyways?</strong></h2>\n\n<p>And why do you need to pretrain LLMs, anyway, given that many pretrained models are available off the shelf? Consider research/study purposes, or you may want to adapt them to new languages or domains (e.g., think of protein or DNA sequences).</p>\n<p>Since language is somewhat universal, the typical workflow is to take a model that was pretrained on a large, general language corpus and then fine-tune it on a target domain — for example, finance articles if the target task is sentiment classification for stock market analysis purposes. However, if large amounts of domain-specific texts exist, it may make sense to also use that data for pretraining.</p>\n<p><span>For example, in a</span> <a rel=\"noopener noreferrer\" href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4337182\" target=\"_blank\" id=\"reader.external-link.num-14\">recent paper</a><span>, a researcher reported that</span> <a rel=\"noopener noreferrer\" href=\"https://arxiv.org/abs/1908.10063\" target=\"_blank\" id=\"reader.external-link.num-15\">FinBERT</a><span>, a BERT model that was further pretrained on a collection of 1.8M finance news articles, outperformed all other models for sentiment analysis on finance domains.</span></p>\n<p><strong>Learning the language of life</strong></p>\n<p><span>Another example of why pretraining sometimes makes sense is the recent</span> <em><a rel=\"noopener noreferrer\" href=\"https://www.nature.com/articles/s41587-022-01618-2\" target=\"_blank\" id=\"reader.external-link.num-16\">Deep neural language modeling enables functional protein generation across families</a></em> <span>paper. Here, researchers trained a 1.2 billion parameter transformer on 280 million proteins sequence to generate new protein sequences (protein sequences consist of one-letter amino acid strings, such as \"DIQMTQSPA...\".</span></p>\n<p>\n<figure>\n<figcaption><span>Annotated figure from</span> <a rel=\"noopener noreferrer\" href=\"https://www.nature.com/articles/s41587-022-01618-2\" target=\"_blank\" id=\"reader.external-link.num-17\">https://www.nature.com/articles/s41587-022-01618-2</a></figcaption></figure></p>\n<p><span>The LLM was able to generate realistic protein sequences that were not more than 30% similar to existing proteins. But what's really remarkable is that the researchers didn't</span> <em>just</em> <span>train an LLM on amino acid sequences. They went further by synthesizing complete genes and expressing the proteins in reality. These artificial proteins showed the same functionality as natural proteins.</span></p>\n<p><strong>Training LLMs from scratch</strong></p>\n<p>A shameless plug: if you are interested in training LLMs from scratch, we recently developed and shared a 50-lines-of-code recipe to do just that.</p>\n<p>\n<figure>\n<figcaption>https://lightning.ai</figcaption></figure></p>\n<h2><strong>Headlines</strong></h2>\n\n<p><span>Many noteworthy things happened in January since the buzz around large language models is still in full swing. Since I can't include it all, but it may still be interesting and relevant, I am adding this new</span> <em>Headlines</em> <span>section with short taglines of tech-related newsworthy highlights — my apologies if this is a bit OpenAI- and LLM-heavy this month.</span></p>\n<p><strong>Academia and research</strong></p>\n<ul><li>\n<p><a rel=\"noopener noreferrer\" href=\"https://openreview.net/group?id=ICLR.cc/2023/Conference#all-submissions\" target=\"_blank\" id=\"reader.external-link.num-18\">ICLR 2023 paper decisions</a> <span>are out</span></p>\n</li>\n<li>\n<p><span>NeurIPS call for papers</span> <a rel=\"noopener noreferrer\" href=\"https://neurips.cc/Conferences/2023/CallForPapers\" target=\"_blank\" id=\"reader.external-link.num-19\">announced</a> <span>(abstracts are due May 11)</span></p>\n</li>\n<li>\n<p><a rel=\"noopener noreferrer\" href=\"https://twitter.com/roydanroy/status/1621679333180948480?s=20&amp;t=LPtCqGqmQ-gQoTx7uW3F9g\" target=\"_blank\" id=\"reader.external-link.num-20\">ICML leaked</a> <span>~10,000 submitted papers for people to download prior to peer-review</span></p>\n</li>\n<li>\n<p><span>Springer Nature \"</span><a rel=\"noopener noreferrer\" href=\"https://www.theverge.com/2023/1/26/23570967/chatgpt-author-scientific-papers-springer-nature-ban\" target=\"_blank\" id=\"reader.external-link.num-21\">says it has no problem with AI being used to help write research — as long as its use is properly disclosed.</a><span>\"</span></p>\n</li>\n<li>\n<p><span>Science policies state that \"</span><a rel=\"noopener noreferrer\" href=\"https://www.science.org/doi/10.1126/science.adg7879\" target=\"_blank\" id=\"reader.external-link.num-22\">text generated by ChatGPT (or any other AI tools) cannot be used in the work, nor can figures, images, or graphics be the products of such tools.</a><span>\"</span></p>\n</li>\n</ul><p><strong>ChatGPT</strong></p>\n<ul><li>\n<p><a rel=\"noopener noreferrer\" href=\"https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/\" target=\"_blank\" id=\"reader.external-link.num-23\">Microsoft extends</a> <span>multi-billion dollar partnership with OpenAI</span></p>\n</li>\n<li>\n<p><span>Microsoft also begun integrating ChatGPT into its core products,</span> <a rel=\"noopener noreferrer\" href=\"https://www.microsoft.com/en-us/microsoft-365/blog/2023/02/01/microsoft-teams-premium-cut-costs-and-add-ai-powered-productivity/?utm_source=substack&amp;utm_medium=email\" target=\"_blank\" id=\"reader.external-link.num-24\">starting with Teams Premium</a></p>\n</li>\n<li>\n<p><a rel=\"noopener noreferrer\" href=\"https://help.openai.com/en/articles/6825453-chatgpt-release-notes\" target=\"_blank\" id=\"reader.external-link.num-25\">ChatGPT was updated</a> <span>with \"improved factuality and mathematical capabilities\" (but as I've shown</span> <a rel=\"noopener noreferrer\" href=\"https://www.linkedin.com/posts/sebastianraschka_deeplearning-ai-chatgpt-activity-7026180540701573120-i9iV?utm_source=share&amp;utm_medium=member_desktop\" target=\"_blank\" id=\"reader.external-link.num-26\">here</a> <span>it still struggles with basic addition and multiplication)</span></p>\n</li>\n<li>\n<p><span>OpenAI releases</span> <a rel=\"noopener noreferrer\" href=\"https://platform.openai.com/ai-text-classifier\" target=\"_blank\" id=\"reader.external-link.num-27\">AI Text Classifier</a> <span>to detect AI-generated text (I have a short blog post about other detection mechanisms</span> <a rel=\"noopener noreferrer\" href=\"https://sebastianraschka.com/blog/2023/detect-ai.html\" target=\"_blank\" id=\"reader.external-link.num-28\">here on my blog</a><span>)</span></p>\n</li>\n<li>\n<p><a rel=\"noopener noreferrer\" href=\"https://techcrunch.com/2023/02/01/openai-launches-chatgpt-plus-starting-at-20-per-month/\" target=\"_blank\" id=\"reader.external-link.num-29\">OpenAI introduces $20 monthly subscription</a> <span>service for ChatGPT</span></p>\n</li>\n<li>\n<p><span>According to report by Semafor,</span> <a rel=\"noopener noreferrer\" href=\"https://watcher.guru/news/openai-is-training-chatgpt-to-eventually-replace-software-engineers\" target=\"_blank\" id=\"reader.external-link.num-30\">OpenAI is training a ChatGPT model to execute mundane coding tasks</a> <span>(the headline reads \"to replace software engineers\").</span></p>\n</li>\n<li>\n<p><span>\"</span><a rel=\"noopener noreferrer\" href=\"https://www.bloomberg.com/news/articles/2023-02-02/google-to-make-ai-language-models-available-soon-pichai-says\" target=\"_blank\" id=\"reader.external-link.num-31\">Google CEO Says</a> <span>Its ChatGPT Rival Coming Soon as a ‘Companion’ to Search\"</span></p>\n</li>\n<li>\n<p><a rel=\"noopener noreferrer\" href=\"https://venturebeat.com/ai/google-invests-300-million-in-anthropic-as-race-to-compete-with-chatgpt-heats-up/\" target=\"_blank\" id=\"reader.external-link.num-32\">Google invests $300 million in Anthropic</a><span>, which is creating its LLM-based chatbot called</span> <em>Claude</em><span>, in response to the alleged competition with ChatGPT</span></p>\n</li>\n<li>\n<p><a rel=\"noopener noreferrer\" href=\"https://time.com/6246119/demis-hassabis-deepmind-interview/\" target=\"_blank\" id=\"reader.external-link.num-33\">According to DeepMind CEO Demis Hassabis</a><span>, \"DeepMind is also considering releasing its own chatbot, called Sparrow, for a 'private beta' some time in 2023\"</span></p>\n</li>\n</ul><p><strong>Stable Diffusion and generative image models</strong></p>\n<ul><li>\n<p><span>Getty Images</span> <a rel=\"noopener noreferrer\" href=\"https://www.theverge.com/2022/9/21/23364696/getty-images-ai-ban-generated-artwork-illustration-copyright\" target=\"_blank\" id=\"reader.external-link.num-34\">bans</a> <span>AI-generated content and is</span> <a rel=\"noopener noreferrer\" href=\"https://www.theverge.com/2023/1/17/23558516/ai-art-copyright-stable-diffusion-getty-images-lawsuit\" target=\"_blank\" id=\"reader.external-link.num-35\">suing Stability</a> <span>AI for scraping content.</span></p>\n</li>\n<li>\n<p><span>Shutterstock</span> <a rel=\"noopener noreferrer\" href=\"https://www.prnewswire.com/news-releases/shutterstock-introduces-generative-ai-to-its-all-in-one-creative-platform-301729904.html\" target=\"_blank\" id=\"reader.external-link.num-36\">adds text-to-image AI generation</a> <span>and</span> <a rel=\"noopener noreferrer\" href=\"https://support.submit.shutterstock.com/s/article/Shutterstock-ai-and-Computer-Vision-Contributor-FAQ?language=en_US\" target=\"_blank\" id=\"reader.external-link.num-37\">adds a revenue share compensation plan</a> <span>for creators whose content was used to train AI.</span></p>\n</li>\n</ul><h2><strong>Deep Learning Fundamentals, Unit 4</strong></h2>\n\n<p><span>I am happy to share that</span> <a rel=\"noopener noreferrer\" href=\"https://lightning.ai/pages/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/logistic-regression-for-multiple-classes-part-1-5/\" target=\"_blank\" id=\"reader.external-link.num-38\">Unit 4</a> <span>of my</span> <em><a rel=\"noopener noreferrer\" href=\"https://lightning.ai/pages/courses/deep-learning-fundamentals/\" target=\"_blank\" id=\"reader.external-link.num-39\">Deep Learning Fundamentals course</a></em> <span>is now available! In Unit 4, we are finally tackling the training of multilayer neural networks using PyTorch. And we will discuss some design decisions, such as random weight initialization.</span></p>\n<div id=\"RIL_IMG_7\">\n        <figure>\n          <img alt=\"https://lightning.ai/pages/courses/deep-learning-fundamentals/\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Fd21b8b3e-dacf-424a-b209-72157a09d931_1148x602.png\">\n          <figcaption>https://lightning.ai/pages/courses/deep-learning-fundamentals/</figcaption>\n          \n        </figure>\n      </div>\n<h2><strong>Open Source Highlights</strong></h2>\n\n<p><strong>CIFAR10 hyperlightspeedbench</strong></p>\n<p><span>The</span> <a rel=\"noopener noreferrer\" href=\"https://github.com/tysam-code/hlb-CIFAR10/blob/main/main.py\" target=\"_blank\" id=\"reader.external-link.num-40\">CIFAR10 hyperlightspeedbench repository</a> <span>provides rapid-experimentation-friendly PyTorch code for training convolutional neural networks on a single GPU. It enables training a model to 94% on CIFAR-10 in less than 10 seconds on a single A100.</span></p>\n<p><strong>Scaling PyTorch models with Lightning 1.9 and Fabric</strong></p>\n<p><a rel=\"noopener noreferrer\" href=\"https://github.com/Lightning-AI/lightning/releases/tag/1.9.0\" target=\"_blank\" id=\"reader.external-link.num-41\">Lightning 1.9 was released</a> <span>with many changes. One of the highlights is the new Fabric class, a more light-weight alternative to the PyTorch Trainer class and utilities.</span></p>\n<p>Like the Trainer, Fabric makes it straightforward to scale a PyTorch model with just a few lines of code and run it on distributed devices. But compared to using the Trainer class, the training loop and optimization logic remain under your full control.</p>\n<p><span>You can read more in the</span> <a rel=\"noopener noreferrer\" href=\"https://pytorch-lightning.readthedocs.io/en/stable/fabric/fabric.html\" target=\"_blank\" id=\"reader.external-link.num-42\">Fabric (Beta) documentation here</a><span>.</span></p>\n<div id=\"RIL_IMG_8\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Fef45bf44-524b-479c-841f-8d0a0f6c7f7c_1264x1252.png\">\n          \n          \n        </figure>\n      </div>\n<p><strong>nanoGPT — an educational code base for GPT</strong></p>\n<p><span>Andrej Karpathy shared</span> <a rel=\"noopener noreferrer\" href=\"https://github.com/karpathy/nanoGPT\" target=\"_blank\" id=\"reader.external-link.num-43\">nanoGPT</a><span>, a rewrite for the popular</span> <a rel=\"noopener noreferrer\" href=\"https://github.com/karpathy/minGPT\" target=\"_blank\" id=\"reader.external-link.num-44\">minGPT</a> <span>codebase that aims to illustrate the inner workings of GPT-2 (the decoder-style large language models) in less than 600 lines of code for the model and training loop combined.</span></p>\n<p><strong>Ruff — an extremely fast Python linter</strong></p>\n<p><span>The</span> <a rel=\"noopener noreferrer\" href=\"https://github.com/charliermarsh/ruff\" target=\"_blank\" id=\"reader.external-link.num-45\">Ruff linter</a> <span>— written in rust — is 10x-100x faster than existing linters, including the popular</span> <a rel=\"noopener noreferrer\" href=\"https://flake8.pycqa.org/en/latest/\" target=\"_blank\" id=\"reader.external-link.num-46\">Flake8</a> <span>package.</span></p>\n<p>Code linters are helpful because they check your code for potential errors, inconsistencies, and adherence to coding standards. Code linters also enforce consistent coding styles, which can be especially helpful for projects with multiple contributors. However, working with large code bases, those linters can sometimes be a tad slow (especially if you use lower-end hardware to check GitHub submissions). In this case, Ruff is worth a try — it can be installed via pip without hassle!</p>\n<h2><strong>Machine Learning Questions &amp; Answers</strong></h2>\n\n<p><strong>Question:</strong> <span>Suppose we plotted a learning curve and found that the machine learning model overfits and could benefit from more training data. Name different approaches for dealing with limited labeled data in supervised machine learning settings.</span></p>\n<div id=\"RIL_IMG_9\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F19653deb-c4d4-408d-b51d-3ccc4bc9daf7_1242x822.png\">\n          \n          \n        </figure>\n      </div>\n<p><strong>Answer:</strong></p>\n<p>Next to collecting more data, there are several methods more or less related to regular supervised learning that we can use in limited-labeled data regimes.</p>\n<p><strong>1) Label more data</strong></p>\n<p>Collecting additional training examples is often the best way to improve the performance of a model. However, this is often not feasible in practice. Listed below are various alternative approaches.</p>\n<p><strong>2) Bootstrapping the data</strong></p>\n<p>It can be helpful to \"bootstrap\" the data by generating modified (augmented) or artificial (synthetic) training examples to boost the performance of the predictive model. (Details are omitted for the sake of brevity in this newsletter.)</p>\n<p><span>Of course, improving the quality of data can also lead to improved predictive performance of a model. (Details abou</span> <em>Data-Centric AI</em> <span>are again omitted for brevity.)</span></p>\n<p><strong>3) Transfer learning</strong></p>\n<p>Transfer learning describes training a model on a general dataset (e.g., ImageNet) and then fine-tuning the pretrained target dataset (e.g., a specific dataset consisting of different bird species). Transfer learning is usually done in the context of deep learning, where model weights can be updated. This is in contrast to tree-based methods since most decision tree algorithms are nonparametric models that do not support iterative training or parameter updates.</p>\n<div id=\"RIL_IMG_10\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F8fd0af6b-b056-463a-b26d-49e4a3f84514_802x494.png\">\n          \n          \n        </figure>\n      </div>\n<p><strong>4) Self-supervised learning</strong></p>\n<p>Similar to transfer learning, self-supervised learning, the model is pretrained on a different task before it is fine-tuned to a target task for which only limited data exists. However, in contrast to transfer learning, self-supervised learning usually relies on label information that can be directly and automatically extracted from unlabeled data. Hence, self-supervised learning is also often called unsupervised pretraining. Common examples include \"next word\" (e.g., used in GPT) or \"masked word\" (e.g., used in BERT) prediction in language modeling. Or, an intuitive example from computer vision includes inpainting: predicting the missing part of an image that was randomly removed.</p>\n<div id=\"RIL_IMG_11\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F5676653a-6ccf-4dca-a942-a35af3c73b4f_678x438.png\">\n          \n          \n        </figure>\n      </div>\n<p><strong>5) Active learning</strong></p>\n<p>In active learning, we typically involve manual labelers or users for feedback during the learning process. However, instead of labeling the entire dataset upfront, active learning includes a prioritization scheme for suggesting unlabeled data points for labeling that maximize the machine learning model's performance.</p>\n<p><span>The name</span> <em>active learning</em> <span>refers to the fact that the model is</span> <em>actively</em> <span>selecting data for labeling in this process. For example, the simplest form of active learning selects data points with high prediction uncertainty for labeling by a human annotator (also referred to as an</span> <em>oracle</em><span>).</span></p>\n<div id=\"RIL_IMG_12\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Fdb955db2-25fd-4035-b963-d0f60fbdbbba_660x378.png\">\n          \n          \n        </figure>\n      </div>\n<p><strong>6) Few-shot learning</strong></p>\n<p>In a few-shot learning scenario, we often deal with extremely small datasets where we usually only have a handful of examples per class. In research contexts, 1-shot (1 example per class) and 5-shot (5 examples per class) are very common. An extreme case of few-shot learning is zero-shot learning, where no labels are provided. A recently popular example of zero-shot learning is GPT-3 and related language models. Here, the user has to provide all the necessary information via the input prompt, as illustrated in the figure below.</p>\n<div id=\"RIL_IMG_13\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F8f3cddba-7bbc-42ea-be62-1f5f84afc98d_1208x746.png\">\n          \n          \n        </figure>\n      </div>\n<p><strong>7) Meta-learning</strong></p>\n<p>We can think of meta-learning as \"learning to learn\" — we develop methods that learn how machine learning algorithms can best learn from data. Over the years, the machine learning community developed several approaches for meta-learning. To further complicate matters, meta-learning can refer to different processes.</p>\n<p><span>Meta-learning is one of the main subcategories of few-shot learning (mentioned above). Here, the focus is on learning a</span> <em>good</em> <span>feature extraction module. The feature extraction module converts support and query images into vector representations. These vector representations are optimized for determining the predicted class of the query example via comparisons with the training examples in the support set.</span></p>\n<p>Another branch of meta-learning, unrelated to the few-shot learning approach above, is focused on extracting meta-data (also called meta-features) from datasets for supervised learning tasks. The meta-features are descriptions of the dataset itself. For example, these can include the number of features and statistics of the different features (kurtosis, range, mean, etc.).</p>\n<p>The extracted meta-features provide information for selecting a machine learning algorithm for the given dataset at hand. Using this approach, we can narrow down the algorithm and hyperparameter search spaces, which helps reduce overfitting when the dataset is small.</p>\n<p><strong>8) Weakly supervised learning</strong></p>\n<p><span>Weakly supervised learning is a procedure where we use an external label source to generate labels for an unlabeled dataset. Often, the labels created by a weakly supervised labeling function are more noisy or inaccurate than those produced by a human or domain expert; hence, the term</span> <em>weakly</em> <span>supervised.</span></p>\n<p>Often, we can develop or adopt a rule-based classifier to create the labels in weakly supervised learning — these rules usually only cover a subset of the unlabeled dataset.</p>\n<div id=\"RIL_IMG_14\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Ff83407bf-ad3f-47ce-b8df-adbdd8f2f77a_1334x1062.png\">\n          \n          \n        </figure>\n      </div>\n<p>Imagine the context of email spam classification as an example of a rule-based approach for data labeling. In weak supervision, we could design a rule-based classifier based on the keyword \"SALE\" in the email subject header line to identify a subset of spam emails. Note that while we may use this rule to label certain emails as spam-positive, we should not apply this rule to label emails without SALE as non-spam but leave those either unlabeled or apply a different rule to these.</p>\n<p>In short, weakly supervised learning is an approach for increasing the number of labeled instances in the training set. Hence, other techniques, such as semi-supervised, transfer, active, and zero-shot learning, are fully compatible with weakly supervised learning.</p>\n<p><strong>9) Semi-supervised learning</strong></p>\n<p><span>Semi-supervised learning is closely related to weakly supervised learning described above: we create labels for unlabeled instances in the dataset. The main difference between weakly supervised and semi-supervised learning is</span> <em>how</em> <span>we create the labels (Semi-supervised learning is sometimes referred to as a subcategory of weakly supervised learning and vice versa.)</span></p>\n<p>In weak supervision, we create labels using an external labeling function that is often noisy, inaccurate or only covers a subset of the data. In semi-supervision, we do not use an external label function but leverage the structure of the data itself.</p>\n<div id=\"RIL_IMG_15\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F3d81cfb4-555e-442c-a8e2-d43883faa7e1_1572x540.png\">\n          \n          \n        </figure>\n      </div>\n<p>In semi-supervised learning, we can, for example, label additional data points based on the density of neighboring labeled data points, as illustrated in the figure below.</p>\n<p>While we can apply weak supervision to an entirely unlabeled dataset, semi-supervised learning requires at least a portion of the data to be labeled. In practice, it is possible first to apply weak supervision to label a subset of the data and then use semi-supervised learning to label instances that were not captured by the labeling functions.</p>\n<p><strong>10) Self-training</strong></p>\n<p><span>Self-training is a category that falls somewhere between semi-supervised learning and weakly supervised learning. In self-training, we train a model or adopt an existing model to label the dataset. This model is also referred to as a</span> <em>pseudo-labeler</em><span>.</span></p>\n<p>Since the model used in self-training does not guarantee accurate labels, self-training is related to weakly supervised learning. Moreover, while we use or adopt a machine learning model for this pseudo-labeling, self-training is also related to semi-supervised learning.</p>\n<p><strong>11) Multi-task learning</strong></p>\n<p><span>Multi-task learning trains neural networks on multiple, ideally related tasks. For example, suppose we are training a classifier to detect spam emails; here, spam classification is the main task. In multi-task learning, we can add one or more related tasks the model has to solve. These additional tasks are also referred to as</span> <em>auxiliary tasks</em><span>. If the main task is email spam classification, an auxiliary task could be classifying the email's topic or language.</span></p>\n<p>Typically, multi-task learning is implemented via multiple loss functions that have to be optimized simultaneously — one loss function for each task. The auxiliary tasks serve as an inductive bias, guiding the model to prioritize hypotheses that can explain multiple tasks. This approach often results in models that perform better on unseen data.</p>\n\n<p>The figure above illustrates the difference between hard and soft parameter sharing. In hard parameter sharing, only the output layers are task-specific, while all tasks share the same hidden layers and neural network backbone architecture. In contrast, soft parameter sharing uses separate neural networks for each task, but regularization techniques such as distance minimization between parameter layers are applied to encourage similarity among the networks.</p>\n<p><strong>12) Multi-modal learning</strong></p>\n<p>While multi-task learning involves training a model with multiple tasks and loss functions, multi-modal learning focuses on incorporating multiple types of input data.</p>\n<p>Common examples of multi-modal learning are architectures that take both image and text data as input. Depending on the task, we may employ a matching loss that forces the embedding vectors between related images and text to be similar, as shown in the figure below.</p>\n<div id=\"RIL_IMG_16\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252Fcf525de5-778f-42da-8e4f-9af7650b6baa_4207x2946.png\">\n          \n          \n        </figure>\n      </div>\n<p>The figure above shows image and text encoders as separate components. The image encoder can be a convolutional backbone or a vision transformer, and the language encoder can be a recurrent neural network or language transformer. However, it's common nowadays to use a single transformer-based module that can simultaneously process image and text data.</p>\n<p>Optimizing a matching loss, as shown in the previous figure, can be useful for learning embeddings that can be applied to various tasks, such as image classification or summarization. However, it is also possible to directly optimize the target loss, like classification or regression, as the figure below illustrates.</p>\n<div id=\"RIL_IMG_17\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F0cee4743-7ee8-463d-8d92-9f14b303c592_4137x2687.png\">\n          \n          \n        </figure>\n      </div>\n<p>Intuitively, models that combine data from different modalities generally perform better than uni-modal models because they can leverage more information. Moreover, recent research suggests that the key to the success of multi-modal learning is the improved quality of the latent space representation.</p>\n<p><strong>13) Inductive biases</strong></p>\n<p>Choosing models with stronger inductive biases can help to lower data requirements by making assumptions about the structure of the data. For example, due to their inductive biases, convolutional networks require less data than vision transformers as discussed in Q13.</p>\n<p><strong>Which techniques should we use?</strong></p>\n<p>Now that we covered several techniques for lowering the data requirements, which ones should we use?</p>\n<p><span>Collecting more data and techniques such as data augmentation and feature engineering are compatible with all the methods discussed above. Also, multi-task learning and multi-modal inputs can be used with the other learning strategies outlined above. If the model suffers from overfitting, techniques from other Q &amp; As (</span><em>Reducing overfitting via model modifications</em><span>, and</span> <em>Reducing overfitting via dataset modifications</em><span>) should also be included.</span></p>\n<p>How about active learning, few-shot learning, transfer learning, self-supervised learning, semi-supervised learning, and weakly supervised learning? Which technique(s) to try highly depends on the context, and the figure below provides an overview that can be used for guidance.</p>\n<div id=\"RIL_IMG_18\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F2148286e-22a8-465c-b8d5-70219d7f21dd_1711x2329.png\">\n          \n          \n        </figure>\n      </div>\n<p>The black boxes are not terminal nodes but arch back to \"Evaluate model performance\" (the arrows were omitted to avoid visual clutter).</p>\n<h2><strong>Machine Learning Q &amp; AI</strong></h2>\n\n<p><span>If you liked the Q&amp;A above and want to read more of these, this was actually an excerpt from my new book, Machine Learning Q &amp; AI, which is</span> <a rel=\"noopener noreferrer\" href=\"https://leanpub.com/machine-learning-q-and-ai/\" target=\"_blank\" id=\"reader.external-link.num-47\">available on Leanpub</a><span>!</span></p>\n<div id=\"RIL_IMG_19\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F8d109817-853c-43c0-8bc4-311a5e78d6e4_1032x1554.jpeg\">\n          \n          \n        </figure>\n      </div>\n<p><span>There is a temporary</span> <code>ahead-of-ai</code> <span>coupon code for readers of Ahead of AI to get 33% off:</span> <a rel=\"noopener noreferrer\" href=\"https://leanpub.com/machine-learning-q-and-ai/c/ahead-of-ai\" target=\"_blank\" id=\"reader.external-link.num-48\">https://leanpub.com/machine-learning-q-and-ai/c/ahead-of-ai</a> <span>(the coupon is valid until February 15).</span></p>\n<h2><strong>Notable Quote</strong></h2>\n\n<blockquote>\n<p><span>The best analogy that I‘ve found for AI is that it’s like a calculator for reading and writing. —</span> <a rel=\"noopener noreferrer\" href=\"https://twitter.com/naval/status/1615248290781745153?s=20&amp;t=XUHYiNcjFll218cjrkhAJg\" target=\"_blank\" id=\"reader.external-link.num-49\">Naval</a></p>\n</blockquote>\n<p>This quote appeals to me, similar to the notion of \"a computer as a bicycle for the mind.\" However, upon further reflection, the analogy falls short. Calculators are deterministic and precise, unlike today's AI models. A better comparison would be to view large language models as a thesaurus, but for entire sentences and paragraphs instead of just individual words.</p>\n<h2><strong>Study &amp; Productivity Tips</strong></h2>\n\n<p>One of the best time commitments when working through textbooks or courses is to do the exercises and quizzes! Sure, often, it feels more productive when we read along — because it's much quicker. But these activities are important for several reasons:</p>\n<ul><li>\n<p>We are assessing our understanding by identifying knowledge gaps. Sometimes, when we struggle with quizzes and exercises, we may have to review the material several times until it sinks in. While it sometimes feels less satisfying than completing the next chapter or unit, it's important if we want to ensure that we get the most out of the material. Learning is a marathon, not a sprint.</p>\n</li>\n<li>\n<p>Active learning. Doing exercises and quizzes actively engages us more, and it promotes a deeper understanding of the material compared to reading or listening passively.</p>\n</li>\n<li>\n<p>Motivation! Above, I mentioned that doing exercises and quizzes can sometimes feel less satisfying than progressing to the next chapter or unit. I suspect that's because we are constantly trained to try to do more and more things. And getting through a book or course quicker may mean we can check it off sooner — who doesn't have an endless pile of things to do? However, completing exercises and quizzes can give us a better sense of accomplishment and motivate us to continue our studies.</p>\n</li>\n<li>\n<p>Real-world readiness. Exercises and quizzes sometimes simulate real-world scenarios helping us to put the learned material into action — often, that's why we are learning the material in the first place!</p>\n</li>\n</ul><p>Making good exercises and quizzes is not easy, and I always tell my students that</p>\n<blockquote>\n<p>making the exam is harder than taking the exam.</p>\n</blockquote>\n<p>I will try my best to add those to any future courses and books I create.</p>\n<h2><strong>Machine Learning Humor</strong></h2>\n\n<div id=\"RIL_IMG_20\">\n        <figure>\n          <img alt=\"\" src=\"https://pocket-image-cache.com//filters:format(jpg):extract_focal()/https%3A%2F%2Fsubstackcdn.com%2Fimage%2Ffetch%2Ff_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep%2Fhttps%253A%252F%252Fsubstack-post-media.s3.amazonaws.com%252Fpublic%252Fimages%252F5299bf34-18da-4800-950a-90dbaea6cd6b_961x956.png\">\n          \n          \n        </figure>\n      </div>\n<blockquote>\n<p><em>Everything gets better with time.</em></p>\n<p>— Said one training epoch to the other.</p>\n</blockquote>\n\n<p><span>Thank you to those who have reached out asking how they can support Ahead of AI. While this newsletter is free and unabbreviated, there is a</span> <a rel=\"noopener noreferrer\" href=\"https://magazine.sebastianraschka.com/subscribe?utm_medium=web&amp;utm_source=magaziney-home-page\" target=\"_blank\" id=\"reader.external-link.num-50\">paid subscription option</a> <span>on Substack for those who would like to support it.</span></p>\n<p><span>Finally, if you are interested in more AI-related news, musings, and educational material but don't want to wait until the next newsletter issue, you can follow me on</span> <a rel=\"noopener noreferrer\" href=\"https://twitter.com/rasbt\" target=\"_blank\" id=\"reader.external-link.num-51\">Twitter</a><span>,</span> <a rel=\"noopener noreferrer\" href=\"https://www.linkedin.com/in/sebastianraschka/\" target=\"_blank\" id=\"reader.external-link.num-52\">LinkedIn</a><span>, or check out</span> <a rel=\"noopener noreferrer\" href=\"https://sebastianraschka.com/books/\" target=\"_blank\" id=\"reader.external-link.num-53\">my books</a><span>.</span></p>\n\n</div></article></div>",
      "textContent": "\nI hope you had a successful start to the new year, as did AI and deep learning research. In this edition of Ahead of AI #5, I wanted to showcase recent advancements in computer vision rather than simply covering the increasing popularity of large language models. This newsletter aims to revive ideas and take convolutional neural networks to new heights.\nBut do not worry; I didn't forget about the large language model headlines this month ...\n\nThis issue of Ahead of AI features:\n\nThe latest advances for pretraining convolutional network architectures via self-supervised learning;\n\n\nThe benefits of training LLMs from scratch;\n\n\nOpen source highlights;\n\n\nThirteen alternatives to supervised learning when you have limited training data.\n\nPS: If the newsletter appears truncated or clipped, that's because some email providers may truncate longer email messages. In this case, you can access the full article at magazine.sebastianraschka.com.\nArticles & Trends\n\nWe will open this section with a discussion of self-supervised learning for convolutional neural networks and the latest GAN vs. diffusion model comparisons. After our short excursion to computer vision, we will look at some of the benefits of training large language models from scratch and close this section with the latest headlines (since ChatGPT wants the final say).\nNext-Generation Convolutional Neural Networks\n\nSelf-supervised learning, which lets us leverage large unlabeled datasets for supervised pretraining, has been one of the keys to success behind language and vision transformers. However, self-supervised learning techniques like masked auto-encoding don't necessarily work well for convolutional neural networks (CNNs). At their very best, they lack behind older self-supervised learning techniques such as contrastive learning.\n\n        \n          \n          Source: Masked Autoencoders Are Scalable Vision Learners, https://arxiv.org/abs/2111.06377\n          \n        \n      \nSo, how can we leverage successful pretraining techniques used in language and vision transformers to improve purely convolutional networks? The answer is sparse convolutions, a type of convolution operation that only considers a subset of input feature maps or weights to reduce computation and memory usage while still capturing essential features in the data.\nApplying self-supervised learning via masked autoencoding to CNNs\nWhat's the problem with using state-of-the-art self-supervised learning techniques in CNNs? Traditional CNNs are not well-suited for handling irregular, randomly masked input images, which we encounter using this pretraining scheme.\nWhat are the issues with regular convolutions on masked pixels?\n\nComputations on masked pixels are redundant (inefficient);\n\n\nThey disturb the data distribution of pixel values (point 2 in the figure below);\n\n\nPatterns on masked maps will vanish (point 3 in the figure below).\n\n\n        \n          \n          Annotated figure from https://arxiv.org/abs/2301.03580 outlining the problems of regular convolutions for sparse\n          \n        \n      \nIn the recent Designing BERT for convolutional networks paper, researchers proposed using sparse convolutions to address issues of CNNs with masked inputs — they proposed SparK (Sparse masKed modeling with hierarchy).\n\n        \n          \n          Annotated figure from https://arxiv.org/abs/2301.03580 outlining the SparK CNN approach.\n          \n        \n      \nSparK can be applied to any convolutional network. For instance, it has been used with ResNet and ConvNeXt, improving predictive performance by up to 1.7% on ImageNet when those purely convolutional networks are pretrained with 1.28 million unlabeled images.\n\n\nAnnotated figure from https://arxiv.org/abs/2301.03580\nDoes it really work?\nThe numbers above look pretty convincing, but sometimes results in papers don't generalize well to other architectures or problems. So, it's always nice to consider additional points of evidence.\nIn a recent independent paper, researchers proposed modifying the popular ConvNext architecture with sparse convolutions and global response normalization: ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders.\nThis ConvNeXt v2 paper co-designs a pure convolutional network for self-supervised via two mechanisms:\n\nusing sparse convolutions for sparse patches with a high-mask ratio;\n\n\nadding a new global response normalization layer.\n\n\n        \n          \n          Source: Annotated figures from “ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders”, https://arxiv.org/abs/2301.00808\n          \n        \n      \n(1) The use of sparse convolutions allows using high-mask ratios and boosts efficiency during training. Note that during fine-tuning, we can convert the sparse convolutional parts back into standard convolutions.\n(2) The global response normalization part is a new type of normalization layer that replaces batch normalization and layer normalization. Without global response normalization, the high-mask ratio via (1) results in feature collapse. The authors found that using global response normalization promotes feature diversity. And it was necessary to achieve good predictive performance. The global response normalization layer is a general technique, and it will be interesting to see whether it can also benefit other architectures in the future.\nLike its predecessor, (ConvNeXt, which we covered in Ahead of AI #4) we can use ConvNeXt v2 for image classification, object detection, and instance segmentation.\n\n        \n          \n          Source: https://arxiv.org/abs/2301.00808\n          \n        \n      \nRemember GANs?\n\nLast year, I jokingly claimed that GANs were dead, but this month, GANs made a strong comeback.\nResearchers have created a GAN, StyleGAN-T, that surpasses distilled diffusion models as the leading text-to-image synthesis method in terms of quality and speed, according to the paper StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis.\n\n        \n          \n          Source: https://arxiv.org/abs/2301.09515\n          \n        \n      \nI still believe that the diffusion models are the future for generative modeling in the computer vision domain, we shouldn't overlook GANs just yet. And competition is good for business anyways.\nTraining a Large Language Model (LLM) On A Single GPU?\n\nYes, that's possible! While contemporary PyTorch open-source libraries make it easy to train large models on multiple GPUs, we often don't have access to extensive hardware resources. The question is whether we (still) can train a large deep learning model on a single GPU like in the old times.\nIn the paper Cramming: Training a Language Model on a Single GPU in One Day, the researchers trained a masked language model / encoder-style LLM (here: BERT) for 24h on 1 GPU. For comparison, the original 2018 BERT paper trained it on 16 TPUs for four days.\nThe impressive outcome of this cramming project was that the researchers were able to train BERT with a 78.6 average performance (compared to 80.9) — the larger, the better.\nWhat were some of the performance-squeezing tricks?\n\nUsing automated operator fusion & 32/16-bit mixed precision training;\n\n\ndisabling biases in QKV attention matrices and fully-connected layers;\n\n\nDecreasing the input length from 512 -> 128 tokens;\n\nAnd what didn't work?\n\nThere were no benefits in replacing the original multi-head self-attention mechanism with FLASH attention or Fourier attention;\n\n\nThere was no advantage after changing GELU activations to something else;\n\n\nShrinking the number of attention heads is bad; keeping the original 12 attention heads is essential to maintain finetuning performance.\n\nFrom a predictive performance standpoint, they furthermore observed that\n\na triangular one-cycle learning rate schedules work best;\n\n\ndropout was not needed during pretraining due to the extensive training dataset and 1-epoch training schedule;\n\n\nincreasing the vocabulary size past 32k does not improve GLUE performance (but MNLI improves).\n\n\n\nAnnotated figure from https://arxiv.org/abs/2212.14034\nNow, all that being said, why don't we simply train smaller models if we have limited resources? To answer this question with some parting advice from this paper: while smaller models have higher throughput, smaller models also learn less efficiently. Thus, larger models do not require more time to train for reaching a specific predictive performance threshold.\nEncoder-Style LLMs?\nLet's talk a bit more about the encoder-style models used in the cramming paper above.\nThe recent buzz around large language models is entirely around decoder-style LLMs such as PalM, Chinchilla, and the GPT family that learn to generate text based on being pretrained via next-word prediction. They are sometimes also called autoregressive and unidirectional as they process text from left to right, one token at a time.\nIn contrast, encoder-style LLMs like BERT are pretrained via masked language modeling — we hide or mask tokens that the LLM has to predict. Since the LLM sees the complete sentence — except for the masked words, of course. This is also referred to as a bidirectional LLM.\nAs a rule of thumb, decoder-style LLMs are usually better for generative modeling, whereas encoder-style LLMs are better for predictive modeling (think of text classification).\n\nWhile generative modeling is usually considered as more exciting and \"magical,\" that's typically what we see on social media and in newspaper headlines.\nHowever, I'd argue that most real-world business problems revolve around and rely upon predictive modeling.\nAnyways, given that there are now hundreds of different large language transformers out there, it's sometimes easy to lose track. In this case, I highly recommend bookmarking Xavier Amatriain's comprehensive transformer model summaries and family tree: Transformer models: an introduction and catalog — 2023 Edition.\nWhich GPU?\nBy the way, if you are wondering which GPU gives you the best bang for the buck, I recommend checking out Tim Dettmer's recently updated The Best GPUs for Deep Learning in 2023 guide.\nAlso, to find the optimal LLM size for your compute budget, check out the handy new widget at\nhttps://howmanyparams.com\n\nWhy do we need to train LLMs from scratch anyways?\n\nAnd why do you need to pretrain LLMs, anyway, given that many pretrained models are available off the shelf? Consider research/study purposes, or you may want to adapt them to new languages or domains (e.g., think of protein or DNA sequences).\nSince language is somewhat universal, the typical workflow is to take a model that was pretrained on a large, general language corpus and then fine-tune it on a target domain — for example, finance articles if the target task is sentiment classification for stock market analysis purposes. However, if large amounts of domain-specific texts exist, it may make sense to also use that data for pretraining.\nFor example, in a recent paper, a researcher reported that FinBERT, a BERT model that was further pretrained on a collection of 1.8M finance news articles, outperformed all other models for sentiment analysis on finance domains.\nLearning the language of life\nAnother example of why pretraining sometimes makes sense is the recent Deep neural language modeling enables functional protein generation across families paper. Here, researchers trained a 1.2 billion parameter transformer on 280 million proteins sequence to generate new protein sequences (protein sequences consist of one-letter amino acid strings, such as \"DIQMTQSPA...\".\n\n\nAnnotated figure from https://www.nature.com/articles/s41587-022-01618-2\nThe LLM was able to generate realistic protein sequences that were not more than 30% similar to existing proteins. But what's really remarkable is that the researchers didn't just train an LLM on amino acid sequences. They went further by synthesizing complete genes and expressing the proteins in reality. These artificial proteins showed the same functionality as natural proteins.\nTraining LLMs from scratch\nA shameless plug: if you are interested in training LLMs from scratch, we recently developed and shared a 50-lines-of-code recipe to do just that.\n\n\nhttps://lightning.ai\nHeadlines\n\nMany noteworthy things happened in January since the buzz around large language models is still in full swing. Since I can't include it all, but it may still be interesting and relevant, I am adding this new Headlines section with short taglines of tech-related newsworthy highlights — my apologies if this is a bit OpenAI- and LLM-heavy this month.\nAcademia and research\n\nICLR 2023 paper decisions are out\n\n\nNeurIPS call for papers announced (abstracts are due May 11)\n\n\nICML leaked ~10,000 submitted papers for people to download prior to peer-review\n\n\nSpringer Nature \"says it has no problem with AI being used to help write research — as long as its use is properly disclosed.\"\n\n\nScience policies state that \"text generated by ChatGPT (or any other AI tools) cannot be used in the work, nor can figures, images, or graphics be the products of such tools.\"\n\nChatGPT\n\nMicrosoft extends multi-billion dollar partnership with OpenAI\n\n\nMicrosoft also begun integrating ChatGPT into its core products, starting with Teams Premium\n\n\nChatGPT was updated with \"improved factuality and mathematical capabilities\" (but as I've shown here it still struggles with basic addition and multiplication)\n\n\nOpenAI releases AI Text Classifier to detect AI-generated text (I have a short blog post about other detection mechanisms here on my blog)\n\n\nOpenAI introduces $20 monthly subscription service for ChatGPT\n\n\nAccording to report by Semafor, OpenAI is training a ChatGPT model to execute mundane coding tasks (the headline reads \"to replace software engineers\").\n\n\n\"Google CEO Says Its ChatGPT Rival Coming Soon as a ‘Companion’ to Search\"\n\n\nGoogle invests $300 million in Anthropic, which is creating its LLM-based chatbot called Claude, in response to the alleged competition with ChatGPT\n\n\nAccording to DeepMind CEO Demis Hassabis, \"DeepMind is also considering releasing its own chatbot, called Sparrow, for a 'private beta' some time in 2023\"\n\nStable Diffusion and generative image models\n\nGetty Images bans AI-generated content and is suing Stability AI for scraping content.\n\n\nShutterstock adds text-to-image AI generation and adds a revenue share compensation plan for creators whose content was used to train AI.\n\nDeep Learning Fundamentals, Unit 4\n\nI am happy to share that Unit 4 of my Deep Learning Fundamentals course is now available! In Unit 4, we are finally tackling the training of multilayer neural networks using PyTorch. And we will discuss some design decisions, such as random weight initialization.\n\n        \n          \n          https://lightning.ai/pages/courses/deep-learning-fundamentals/\n          \n        \n      \nOpen Source Highlights\n\nCIFAR10 hyperlightspeedbench\nThe CIFAR10 hyperlightspeedbench repository provides rapid-experimentation-friendly PyTorch code for training convolutional neural networks on a single GPU. It enables training a model to 94% on CIFAR-10 in less than 10 seconds on a single A100.\nScaling PyTorch models with Lightning 1.9 and Fabric\nLightning 1.9 was released with many changes. One of the highlights is the new Fabric class, a more light-weight alternative to the PyTorch Trainer class and utilities.\nLike the Trainer, Fabric makes it straightforward to scale a PyTorch model with just a few lines of code and run it on distributed devices. But compared to using the Trainer class, the training loop and optimization logic remain under your full control.\nYou can read more in the Fabric (Beta) documentation here.\n\n        \n          \n          \n          \n        \n      \nnanoGPT — an educational code base for GPT\nAndrej Karpathy shared nanoGPT, a rewrite for the popular minGPT codebase that aims to illustrate the inner workings of GPT-2 (the decoder-style large language models) in less than 600 lines of code for the model and training loop combined.\nRuff — an extremely fast Python linter\nThe Ruff linter — written in rust — is 10x-100x faster than existing linters, including the popular Flake8 package.\nCode linters are helpful because they check your code for potential errors, inconsistencies, and adherence to coding standards. Code linters also enforce consistent coding styles, which can be especially helpful for projects with multiple contributors. However, working with large code bases, those linters can sometimes be a tad slow (especially if you use lower-end hardware to check GitHub submissions). In this case, Ruff is worth a try — it can be installed via pip without hassle!\nMachine Learning Questions & Answers\n\nQuestion: Suppose we plotted a learning curve and found that the machine learning model overfits and could benefit from more training data. Name different approaches for dealing with limited labeled data in supervised machine learning settings.\n\n        \n          \n          \n          \n        \n      \nAnswer:\nNext to collecting more data, there are several methods more or less related to regular supervised learning that we can use in limited-labeled data regimes.\n1) Label more data\nCollecting additional training examples is often the best way to improve the performance of a model. However, this is often not feasible in practice. Listed below are various alternative approaches.\n2) Bootstrapping the data\nIt can be helpful to \"bootstrap\" the data by generating modified (augmented) or artificial (synthetic) training examples to boost the performance of the predictive model. (Details are omitted for the sake of brevity in this newsletter.)\nOf course, improving the quality of data can also lead to improved predictive performance of a model. (Details abou Data-Centric AI are again omitted for brevity.)\n3) Transfer learning\nTransfer learning describes training a model on a general dataset (e.g., ImageNet) and then fine-tuning the pretrained target dataset (e.g., a specific dataset consisting of different bird species). Transfer learning is usually done in the context of deep learning, where model weights can be updated. This is in contrast to tree-based methods since most decision tree algorithms are nonparametric models that do not support iterative training or parameter updates.\n\n        \n          \n          \n          \n        \n      \n4) Self-supervised learning\nSimilar to transfer learning, self-supervised learning, the model is pretrained on a different task before it is fine-tuned to a target task for which only limited data exists. However, in contrast to transfer learning, self-supervised learning usually relies on label information that can be directly and automatically extracted from unlabeled data. Hence, self-supervised learning is also often called unsupervised pretraining. Common examples include \"next word\" (e.g., used in GPT) or \"masked word\" (e.g., used in BERT) prediction in language modeling. Or, an intuitive example from computer vision includes inpainting: predicting the missing part of an image that was randomly removed.\n\n        \n          \n          \n          \n        \n      \n5) Active learning\nIn active learning, we typically involve manual labelers or users for feedback during the learning process. However, instead of labeling the entire dataset upfront, active learning includes a prioritization scheme for suggesting unlabeled data points for labeling that maximize the machine learning model's performance.\nThe name active learning refers to the fact that the model is actively selecting data for labeling in this process. For example, the simplest form of active learning selects data points with high prediction uncertainty for labeling by a human annotator (also referred to as an oracle).\n\n        \n          \n          \n          \n        \n      \n6) Few-shot learning\nIn a few-shot learning scenario, we often deal with extremely small datasets where we usually only have a handful of examples per class. In research contexts, 1-shot (1 example per class) and 5-shot (5 examples per class) are very common. An extreme case of few-shot learning is zero-shot learning, where no labels are provided. A recently popular example of zero-shot learning is GPT-3 and related language models. Here, the user has to provide all the necessary information via the input prompt, as illustrated in the figure below.\n\n        \n          \n          \n          \n        \n      \n7) Meta-learning\nWe can think of meta-learning as \"learning to learn\" — we develop methods that learn how machine learning algorithms can best learn from data. Over the years, the machine learning community developed several approaches for meta-learning. To further complicate matters, meta-learning can refer to different processes.\nMeta-learning is one of the main subcategories of few-shot learning (mentioned above). Here, the focus is on learning a good feature extraction module. The feature extraction module converts support and query images into vector representations. These vector representations are optimized for determining the predicted class of the query example via comparisons with the training examples in the support set.\nAnother branch of meta-learning, unrelated to the few-shot learning approach above, is focused on extracting meta-data (also called meta-features) from datasets for supervised learning tasks. The meta-features are descriptions of the dataset itself. For example, these can include the number of features and statistics of the different features (kurtosis, range, mean, etc.).\nThe extracted meta-features provide information for selecting a machine learning algorithm for the given dataset at hand. Using this approach, we can narrow down the algorithm and hyperparameter search spaces, which helps reduce overfitting when the dataset is small.\n8) Weakly supervised learning\nWeakly supervised learning is a procedure where we use an external label source to generate labels for an unlabeled dataset. Often, the labels created by a weakly supervised labeling function are more noisy or inaccurate than those produced by a human or domain expert; hence, the term weakly supervised.\nOften, we can develop or adopt a rule-based classifier to create the labels in weakly supervised learning — these rules usually only cover a subset of the unlabeled dataset.\n\n        \n          \n          \n          \n        \n      \nImagine the context of email spam classification as an example of a rule-based approach for data labeling. In weak supervision, we could design a rule-based classifier based on the keyword \"SALE\" in the email subject header line to identify a subset of spam emails. Note that while we may use this rule to label certain emails as spam-positive, we should not apply this rule to label emails without SALE as non-spam but leave those either unlabeled or apply a different rule to these.\nIn short, weakly supervised learning is an approach for increasing the number of labeled instances in the training set. Hence, other techniques, such as semi-supervised, transfer, active, and zero-shot learning, are fully compatible with weakly supervised learning.\n9) Semi-supervised learning\nSemi-supervised learning is closely related to weakly supervised learning described above: we create labels for unlabeled instances in the dataset. The main difference between weakly supervised and semi-supervised learning is how we create the labels (Semi-supervised learning is sometimes referred to as a subcategory of weakly supervised learning and vice versa.)\nIn weak supervision, we create labels using an external labeling function that is often noisy, inaccurate or only covers a subset of the data. In semi-supervision, we do not use an external label function but leverage the structure of the data itself.\n\n        \n          \n          \n          \n        \n      \nIn semi-supervised learning, we can, for example, label additional data points based on the density of neighboring labeled data points, as illustrated in the figure below.\nWhile we can apply weak supervision to an entirely unlabeled dataset, semi-supervised learning requires at least a portion of the data to be labeled. In practice, it is possible first to apply weak supervision to label a subset of the data and then use semi-supervised learning to label instances that were not captured by the labeling functions.\n10) Self-training\nSelf-training is a category that falls somewhere between semi-supervised learning and weakly supervised learning. In self-training, we train a model or adopt an existing model to label the dataset. This model is also referred to as a pseudo-labeler.\nSince the model used in self-training does not guarantee accurate labels, self-training is related to weakly supervised learning. Moreover, while we use or adopt a machine learning model for this pseudo-labeling, self-training is also related to semi-supervised learning.\n11) Multi-task learning\nMulti-task learning trains neural networks on multiple, ideally related tasks. For example, suppose we are training a classifier to detect spam emails; here, spam classification is the main task. In multi-task learning, we can add one or more related tasks the model has to solve. These additional tasks are also referred to as auxiliary tasks. If the main task is email spam classification, an auxiliary task could be classifying the email's topic or language.\nTypically, multi-task learning is implemented via multiple loss functions that have to be optimized simultaneously — one loss function for each task. The auxiliary tasks serve as an inductive bias, guiding the model to prioritize hypotheses that can explain multiple tasks. This approach often results in models that perform better on unseen data.\n\nThe figure above illustrates the difference between hard and soft parameter sharing. In hard parameter sharing, only the output layers are task-specific, while all tasks share the same hidden layers and neural network backbone architecture. In contrast, soft parameter sharing uses separate neural networks for each task, but regularization techniques such as distance minimization between parameter layers are applied to encourage similarity among the networks.\n12) Multi-modal learning\nWhile multi-task learning involves training a model with multiple tasks and loss functions, multi-modal learning focuses on incorporating multiple types of input data.\nCommon examples of multi-modal learning are architectures that take both image and text data as input. Depending on the task, we may employ a matching loss that forces the embedding vectors between related images and text to be similar, as shown in the figure below.\n\n        \n          \n          \n          \n        \n      \nThe figure above shows image and text encoders as separate components. The image encoder can be a convolutional backbone or a vision transformer, and the language encoder can be a recurrent neural network or language transformer. However, it's common nowadays to use a single transformer-based module that can simultaneously process image and text data.\nOptimizing a matching loss, as shown in the previous figure, can be useful for learning embeddings that can be applied to various tasks, such as image classification or summarization. However, it is also possible to directly optimize the target loss, like classification or regression, as the figure below illustrates.\n\n        \n          \n          \n          \n        \n      \nIntuitively, models that combine data from different modalities generally perform better than uni-modal models because they can leverage more information. Moreover, recent research suggests that the key to the success of multi-modal learning is the improved quality of the latent space representation.\n13) Inductive biases\nChoosing models with stronger inductive biases can help to lower data requirements by making assumptions about the structure of the data. For example, due to their inductive biases, convolutional networks require less data than vision transformers as discussed in Q13.\nWhich techniques should we use?\nNow that we covered several techniques for lowering the data requirements, which ones should we use?\nCollecting more data and techniques such as data augmentation and feature engineering are compatible with all the methods discussed above. Also, multi-task learning and multi-modal inputs can be used with the other learning strategies outlined above. If the model suffers from overfitting, techniques from other Q & As (Reducing overfitting via model modifications, and Reducing overfitting via dataset modifications) should also be included.\nHow about active learning, few-shot learning, transfer learning, self-supervised learning, semi-supervised learning, and weakly supervised learning? Which technique(s) to try highly depends on the context, and the figure below provides an overview that can be used for guidance.\n\n        \n          \n          \n          \n        \n      \nThe black boxes are not terminal nodes but arch back to \"Evaluate model performance\" (the arrows were omitted to avoid visual clutter).\nMachine Learning Q & AI\n\nIf you liked the Q&A above and want to read more of these, this was actually an excerpt from my new book, Machine Learning Q & AI, which is available on Leanpub!\n\n        \n          \n          \n          \n        \n      \nThere is a temporary ahead-of-ai coupon code for readers of Ahead of AI to get 33% off: https://leanpub.com/machine-learning-q-and-ai/c/ahead-of-ai (the coupon is valid until February 15).\nNotable Quote\n\n\nThe best analogy that I‘ve found for AI is that it’s like a calculator for reading and writing. — Naval\n\nThis quote appeals to me, similar to the notion of \"a computer as a bicycle for the mind.\" However, upon further reflection, the analogy falls short. Calculators are deterministic and precise, unlike today's AI models. A better comparison would be to view large language models as a thesaurus, but for entire sentences and paragraphs instead of just individual words.\nStudy & Productivity Tips\n\nOne of the best time commitments when working through textbooks or courses is to do the exercises and quizzes! Sure, often, it feels more productive when we read along — because it's much quicker. But these activities are important for several reasons:\n\nWe are assessing our understanding by identifying knowledge gaps. Sometimes, when we struggle with quizzes and exercises, we may have to review the material several times until it sinks in. While it sometimes feels less satisfying than completing the next chapter or unit, it's important if we want to ensure that we get the most out of the material. Learning is a marathon, not a sprint.\n\n\nActive learning. Doing exercises and quizzes actively engages us more, and it promotes a deeper understanding of the material compared to reading or listening passively.\n\n\nMotivation! Above, I mentioned that doing exercises and quizzes can sometimes feel less satisfying than progressing to the next chapter or unit. I suspect that's because we are constantly trained to try to do more and more things. And getting through a book or course quicker may mean we can check it off sooner — who doesn't have an endless pile of things to do? However, completing exercises and quizzes can give us a better sense of accomplishment and motivate us to continue our studies.\n\n\nReal-world readiness. Exercises and quizzes sometimes simulate real-world scenarios helping us to put the learned material into action — often, that's why we are learning the material in the first place!\n\nMaking good exercises and quizzes is not easy, and I always tell my students that\n\nmaking the exam is harder than taking the exam.\n\nI will try my best to add those to any future courses and books I create.\nMachine Learning Humor\n\n\n        \n          \n          \n          \n        \n      \n\nEverything gets better with time.\n— Said one training epoch to the other.\n\n\nThank you to those who have reached out asking how they can support Ahead of AI. While this newsletter is free and unabbreviated, there is a paid subscription option on Substack for those who would like to support it.\nFinally, if you are interested in more AI-related news, musings, and educational material but don't want to wait until the next newsletter issue, you can follow me on Twitter, LinkedIn, or check out my books.\n\n",
      "length": 32425,
      "excerpt": "I hope you had a successful start to the new year, as did AI and deep learning research. In this edition of Ahead of AI #5, I wanted to showcase recent advancements in computer vision rather than simply covering the increasing popularity of large language models. This newsletter aims to revive ideas and take convolutional neural networks to new heights.",
      "siteName": null,
      "summary": "This issue of Ahead of AI features: The latest advances for pretraining convolutional network architectures via self-supervised learning. The benefits of training large language models from scratch and close this section with the latest headlines. If the newsletter appears truncated or clipped, that's because some email providers may truncate longer email messages.",
      "topics": [
        {
          "label": "Computers & Internet",
          "score": 0.9941692352294922
        },
        {
          "label": "Science & Mathematics",
          "score": 0.9471293091773987
        },
        {
          "label": "Education & Reference",
          "score": 0.9314823746681213
        }
      ]
    }
  },
  "3801983319": {
    "item_id": "3801983319",
    "resolved_id": "3801983319",
    "given_url": "https://fortune.com/well/2023/02/07/cold-shower-health-benefits/",
    "given_title": "Are cold showers good for you? 3 main benefits | Fortune",
    "favorite": "0",
    "status": "0",
    "time_added": "1676356128",
    "time_updated": "1676356134",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 5,
    "resolved_title": "Cold showers are the hot new thing and research suggests that health benefits outweigh the discomfort",
    "resolved_url": "https://fortune.com/well/2023/02/07/cold-shower-health-benefits/",
    "excerpt": "Billionaire Twitter founder and Square CEO, Jack Dorsey, not only wakes up at 5 a.m., he also starts his day with an ice-cold bath. WHOOP founder and CEO, Will Ahmed, told Fortune that cold showers and meditation are part of his morning routine.",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "0",
    "word_count": "596",
    "lang": "en",
    "time_to_read": 3,
    "amp_url": "https://fortune.com/well/2023/02/07/cold-shower-health-benefits/amp/",
    "top_image_url": "https://content.fortune.com/wp-content/uploads/2023/02/GettyImages-1408896585-e1675283722998.jpg?resize=1200,600",
    "tags": {
      "readinglist": {
        "item_id": "3801983319",
        "tag": "readinglist"
      }
    },
    "authors": {
      "169847557": {
        "item_id": "3801983319",
        "author_id": "169847557",
        "name": "L'Oreal Thompson Payton",
        "url": "https://fortune.com/author/loreal-thompson-payton/"
      }
    },
    "domain_metadata": {
      "name": "Fortune",
      "logo": "https://logo.clearbit.com/fortune.com?size=800",
      "greyscale_logo": "https://logo.clearbit.com/fortune.com?size=800&greyscale=true"
    },
    "listen_duration_estimate": 231,
    "content": {
      "title": "Cold showers are the hot new thing and research suggests that health benefits outweigh the discomfort",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><div>\n\n\n\n<p>Billionaire <a rel=\"noopener noreferrer\" href=\"https://fortune.com/company/twitter/\" target=\"_blank\" id=\"reader.external-link.num-0\">Twitter</a> founder and Square CEO, Jack Dorsey, not only <a rel=\"noopener noreferrer\" href=\"https://fortune.com/well/2023/01/29/waking-up-at-5-a-m-every-day-could-improve-your-life-heres-how-to-make-it-work-for-you/\" target=\"_blank\" id=\"reader.external-link.num-1\">wakes up at 5 a.m.</a>, he also starts his day with an <a rel=\"noopener noreferrer\" href=\"https://www.yahoo.com/now/silicon-valley-stoic-glimpse-jack-122000360.html?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAA2EutnJFDjpgAehj6BnO_2Zk91mmhAVq02Rp6jG2B5GOid5VE550HryFtV-y5gN9FJ7EduzKsdQQR9Kw82SXtBopE2k7U8eDZ7EKIkQ02xRUT9kkSAt3GtU4xBoDuVwJi5mhCQC7qWiN9prV_5KsUZZYJs4fwH31xn8LWVJ0qFO\" target=\"_blank\" id=\"reader.external-link.num-2\">ice-cold bath</a>. WHOOP founder and CEO, Will Ahmed, told <em>Fortune</em> that <a rel=\"noopener noreferrer\" href=\"https://fortune.com/well/2022/12/04/ceo-whoop-will-ahmed-wellness-habits/\" target=\"_blank\" id=\"reader.external-link.num-3\">cold showers and meditation</a> are part of his morning routine. Meanwhile, <a rel=\"noopener noreferrer\" href=\"https://fortune.com/well/2022/09/09/matteo-franceschetti-ceo-eight-sleep-stays-energized/\" target=\"_blank\" id=\"reader.external-link.num-4\">Matteo Franceschetti</a>, cofounder and CEO of Eight Sleep, swears by thermal shock as part of his nighttime routine—starting with a sauna or hot bath, followed by an ice bath or cold shower.&nbsp;</p>\n\n\n\n<p>But what does science say? As it turns out, there are real physical and mental benefits to starting your day with a cold shower, which is defined as a shower with a water temperature below 70 degrees Fahrenheit.</p>\n\n\n\n<p>Here are three reasons why you might want to incorporate a cold shower into your daily routine:</p>\n\n\n\n<h2><strong>Improve your physical health&nbsp;</strong></h2>\n\n\n\n<p>A study published in <a rel=\"noopener noreferrer\" href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5025014/\" target=\"_blank\" id=\"reader.external-link.num-5\"><em>PLOS One</em></a> showed that people who regularly took cold showers saw a 29% reduction in sickness-related absences from work—though it didn’t reduce the number of days a person was sick. Another <a rel=\"noopener noreferrer\" href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5025014/\" target=\"_blank\" id=\"reader.external-link.num-6\">study</a> noted that cold showers contribute to the “improvement of the immune system, cardiovascular circulation and vitality,” although proven associations remain unclear. There is also data that suggests showers may <a rel=\"noopener noreferrer\" href=\"https://health.clevelandclinic.org/are-cold-showers-good-for-you/\" target=\"_blank\" id=\"reader.external-link.num-7\">decrease inflammation</a>, <a rel=\"noopener noreferrer\" href=\"https://journals.physiology.org/doi/abs/10.1152/jappl.1986.61.5.1656\" target=\"_blank\" id=\"reader.external-link.num-8\">increase longevity</a>, and <a rel=\"noopener noreferrer\" href=\"https://bjsm.bmj.com/content/44/3/179\" target=\"_blank\" id=\"reader.external-link.num-9\">improve your metabolism</a>.&nbsp;</p>\n\n\n\n<h2><strong>Improve your mood</strong></h2>\n\n\n\n<p>In a <a rel=\"noopener noreferrer\" href=\"https://www.sciencedirect.com/science/article/abs/pii/S030698770700566X\" target=\"_blank\" id=\"reader.external-link.num-10\">clinical trial</a>, researchers found that taking a cold shower for up to five minutes once or twice daily can help relieve symptoms of depression. Cold showers can also help clear your mind and combat overwhelm by decreasing your heart rate. Short on time or already took your shower for the day? Splashing your face with cold water, taking a walk on a cold day, or holding an ice cube can also help, according to <a rel=\"noopener noreferrer\" href=\"https://dialecticalbehaviortherapy.com/distress-tolerance/tipp/\" target=\"_blank\" id=\"reader.external-link.num-11\">Dialectical Behavior Therapy</a>, which is a type of psychotherapy especially designed for people who experience emotions intensely.</p>\n\n\n\n<h2><strong>Increase your willpower</strong></h2>\n\n\n\n<p>In Dr. Kelly McGonigal’s book, <a rel=\"noopener noreferrer\" href=\"http://kellymcgonigal.com/books\" target=\"_blank\" id=\"reader.external-link.num-12\"><em>The Willpower Instinct</em></a>, the health psychologist explains that doing uncomfortable activities, such as taking a cold shower, helps build willpower and the neural pathways that contribute to resilience.</p>\n\n\n\n<p>Athlete Joel Runyon backs this theory up in his popular <a rel=\"noopener noreferrer\" href=\"https://www.youtube.com/watch?v=Gb0h8ZKvJW4\" target=\"_blank\" id=\"reader.external-link.num-13\">TED talk</a> about “Cold Shower Therapy.”</p>\n\n\n\n<p>“After you’ve stared at the shower head and conquered the cold in the shower that morning, you begin to recognize that hesitation,” he wrote in a <a rel=\"noopener noreferrer\" href=\"https://impossiblehq.com/cold-shower-therapy/\" target=\"_blank\" id=\"reader.external-link.num-14\">blog post</a>. “You realize the hesitation you have when it comes to the project/business/workout is the exact same fear you see in the shower. And you realize you’ve already beaten it once that day and that the only way it can actually hurt you is by stopping you from doing what you need to do.”</p>\n\n\n\n<h2><strong>Getting started with cold showers</strong></h2>\n\n\n\n<p>While cold water immersion is all the rage among athletes and influencers, such as the Dutch motivational speaker behind the popular <a rel=\"noopener noreferrer\" href=\"https://www.wimhofmethod.com/benefits-of-cold-showers\" target=\"_blank\" id=\"reader.external-link.num-15\">Wim Hof Method</a>, too much cold exposure can cause your blood pressure to drop.</p>\n\n\n\n<p>If you want to give it a try <a rel=\"noopener noreferrer\" href=\"https://fortune.com/well/2022/06/11/do-ice-baths-have-health-benefits/\" target=\"_blank\" id=\"reader.external-link.num-16\">experts recommend starting slow and small</a>—by either gradually decreasing the temperature of your shower, or finishing your shower with a blast of cold water, as Dr. Dominic King, a sports medicine physician in the department of orthopaedic surgery at the Cleveland Clinic, previously told <em>Fortune</em>. </p>\n\n\n\n<p>You can start by taking a cold five-minute shower and slowly work your way up to 10 minutes. It’s also best to take a cold shower in the morning to set yourself up for success for the remainder of the day.&nbsp;</p></div><p>Learn how to navigate and strengthen trust in your business with The Trust Factor, a weekly newsletter examining what leaders need to succeed. <a rel=\"noopener noreferrer\" href=\"https://mynewsletters.fortune.com/trust-factor/\" target=\"_blank\" id=\"reader.external-link.num-17\">Sign up here.</a></p></div>",
      "textContent": "\n\n\n\nBillionaire Twitter founder and Square CEO, Jack Dorsey, not only wakes up at 5 a.m., he also starts his day with an ice-cold bath. WHOOP founder and CEO, Will Ahmed, told Fortune that cold showers and meditation are part of his morning routine. Meanwhile, Matteo Franceschetti, cofounder and CEO of Eight Sleep, swears by thermal shock as part of his nighttime routine—starting with a sauna or hot bath, followed by an ice bath or cold shower. \n\n\n\nBut what does science say? As it turns out, there are real physical and mental benefits to starting your day with a cold shower, which is defined as a shower with a water temperature below 70 degrees Fahrenheit.\n\n\n\nHere are three reasons why you might want to incorporate a cold shower into your daily routine:\n\n\n\nImprove your physical health \n\n\n\nA study published in PLOS One showed that people who regularly took cold showers saw a 29% reduction in sickness-related absences from work—though it didn’t reduce the number of days a person was sick. Another study noted that cold showers contribute to the “improvement of the immune system, cardiovascular circulation and vitality,” although proven associations remain unclear. There is also data that suggests showers may decrease inflammation, increase longevity, and improve your metabolism. \n\n\n\nImprove your mood\n\n\n\nIn a clinical trial, researchers found that taking a cold shower for up to five minutes once or twice daily can help relieve symptoms of depression. Cold showers can also help clear your mind and combat overwhelm by decreasing your heart rate. Short on time or already took your shower for the day? Splashing your face with cold water, taking a walk on a cold day, or holding an ice cube can also help, according to Dialectical Behavior Therapy, which is a type of psychotherapy especially designed for people who experience emotions intensely.\n\n\n\nIncrease your willpower\n\n\n\nIn Dr. Kelly McGonigal’s book, The Willpower Instinct, the health psychologist explains that doing uncomfortable activities, such as taking a cold shower, helps build willpower and the neural pathways that contribute to resilience.\n\n\n\nAthlete Joel Runyon backs this theory up in his popular TED talk about “Cold Shower Therapy.”\n\n\n\n“After you’ve stared at the shower head and conquered the cold in the shower that morning, you begin to recognize that hesitation,” he wrote in a blog post. “You realize the hesitation you have when it comes to the project/business/workout is the exact same fear you see in the shower. And you realize you’ve already beaten it once that day and that the only way it can actually hurt you is by stopping you from doing what you need to do.”\n\n\n\nGetting started with cold showers\n\n\n\nWhile cold water immersion is all the rage among athletes and influencers, such as the Dutch motivational speaker behind the popular Wim Hof Method, too much cold exposure can cause your blood pressure to drop.\n\n\n\nIf you want to give it a try experts recommend starting slow and small—by either gradually decreasing the temperature of your shower, or finishing your shower with a blast of cold water, as Dr. Dominic King, a sports medicine physician in the department of orthopaedic surgery at the Cleveland Clinic, previously told Fortune. \n\n\n\nYou can start by taking a cold five-minute shower and slowly work your way up to 10 minutes. It’s also best to take a cold shower in the morning to set yourself up for success for the remainder of the day. Learn how to navigate and strengthen trust in your business with The Trust Factor, a weekly newsletter examining what leaders need to succeed. Sign up here.",
      "length": 3616,
      "excerpt": "Billionaire Twitter founder and Square CEO, Jack Dorsey, not only wakes up at 5 a.m., he also starts his day with an ice-cold bath. WHOOP founder and CEO, Will Ahmed, told Fortune that cold showers and meditation are part of his morning routine. Meanwhile, Matteo Franceschetti, cofounder and CEO of Eight Sleep, swears by thermal shock as part of his nighttime routine—starting with a sauna or hot bath, followed by an ice bath or cold shower.",
      "siteName": null,
      "summary": "There are real physical and mental benefits to starting your day with a cold shower. People who regularly took cold showers saw a 29% reduction in sickness-related absences from work. A clinical trial found that taking a cold showers for up to five minutes once or twice daily can help relieve symptoms of depression.",
      "topics": [
        {
          "label": "Health",
          "score": 0.9934311509132385
        },
        {
          "label": "Business & Finance",
          "score": 0.769397497177124
        },
        {
          "label": "Science & Mathematics",
          "score": 0.7315102219581604
        }
      ]
    }
  },
  "3802004841": {
    "item_id": "3802004841",
    "resolved_id": "3802004841",
    "given_url": "https://www.insider.com/high-protein-foods-dietitian-personal-trainer-buys-build-muscle-weight-2023-2",
    "given_title": "5 High-Protein Foods a Dietitian and Personal Trainer Always Buys",
    "favorite": "0",
    "status": "0",
    "time_added": "1676269816",
    "time_updated": "1676269824",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 8,
    "resolved_title": "5 high-protein foods a dietitian and personal trainer always keeps in her kitchen to build muscle and stay feeling full",
    "resolved_url": "https://www.insider.com/high-protein-foods-dietitian-personal-trainer-buys-build-muscle-weight-2023-2",
    "excerpt": "Registered dietitian and personal trainer Kara Mockler shared the five protein-rich foods she always has in her kitchen to help her feel and perform her best. Mockler eats a high-protein diet, aiming for 0.8-1.2 grams of protein per pound of bodyweight (or 1.6-2.",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "1",
    "word_count": "487",
    "lang": "en",
    "amp_url": "https://www.insider.com/high-protein-foods-dietitian-personal-trainer-buys-build-muscle-weight-2023-2?amp",
    "top_image_url": "https://i.insider.com/63e12a00428e0c0019f7afbd?width=1200&format=jpeg",
    "tags": {
      "readinglist": {
        "item_id": "3802004841",
        "tag": "readinglist"
      }
    },
    "authors": {
      "10260378": {
        "item_id": "3802004841",
        "author_id": "10260378",
        "name": "Rachel Hosie",
        "url": ""
      }
    },
    "image": {
      "item_id": "3802004841",
      "src": "https://i.insider.com/63e129e9c6bb0f0019bfce00?width=700",
      "width": "0",
      "height": "0"
    },
    "images": {
      "1": {
        "item_id": "3802004841",
        "image_id": "1",
        "src": "https://i.insider.com/63e129e9c6bb0f0019bfce00?width=700",
        "width": "0",
        "height": "0",
        "credit": "",
        "caption": "Greek yogurt is a staple food for dietitian and personal trainer Kara Mockler.\n                          \n                              RP Strength"
      }
    },
    "listen_duration_estimate": 189,
    "content": {
      "title": "5 high-protein foods a dietitian and personal trainer always keeps in her kitchen to build muscle and stay feeling full",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><div lang=\"en\"><p>Registered dietitian and personal trainer <a rel=\"noopener noreferrer\" href=\"https://www.instagram.com/karamockler.rp/\" target=\"_blank\" id=\"reader.external-link.num-0\">Kara Mockler</a> shared the five protein-rich foods she always has in her kitchen to help her feel and perform her best.</p><p>Mockler eats a <a rel=\"noopener noreferrer\" href=\"https://www.insider.com/high-protein-daily-diet-to-lose-body-fat-keep-muscle-2021-8\" target=\"_blank\" id=\"reader.external-link.num-1\">high-protein diet</a>, aiming for 0.8-1.2 grams of protein per pound of bodyweight (or 1.6-2.4 grams per kilogram) daily, for multiple reasons, she said.</p><p>\"<a rel=\"noopener noreferrer\" href=\"https://www.insider.com/high-protein-food-swaps-fat-loss-muscle-gain-2022-1\" target=\"_blank\" id=\"reader.external-link.num-2\">Protein</a> is utilized all throughout the body to not only <a rel=\"noopener noreferrer\" href=\"https://www.insider.com/best-exercises-for-building-muscle-in-gym-correct-technique-2020-12\" target=\"_blank\" id=\"reader.external-link.num-3\">build muscle</a> but also hair, skin, and pretty much <a rel=\"noopener noreferrer\" href=\"https://www.insider.com/physical-signs-need-eat-more-protein-weak-hair-nails-hunger-2022-7\" target=\"_blank\" id=\"reader.external-link.num-4\">every other cell in our bodies</a>,\" Mockler said.</p><p>It's important to <a rel=\"noopener noreferrer\" href=\"https://www.insider.com/protein-timing-after-workout-training-dietitian-advice-2022-3\" target=\"_blank\" id=\"reader.external-link.num-5\">spread your protein intake </a>evenly over the course of the day, rather than eating one large high-protein meal, she said.</p>\n                        \n                        <p>\"Eating protein consistently throughout the day allows our bodies to continually rebuild cells as they constantly turnover,\" Mockler said.</p><p>Another reason protein is important in the diet is because <a rel=\"noopener noreferrer\" href=\"https://www.insider.com/foods-rich-in-protein-help-me-feel-full-2018-9\" target=\"_blank\" id=\"reader.external-link.num-6\">it's satiating</a>.</p><p>\"Protein makes us feel full, which is especially useful for weight management,\" Mockler said, so protein is important for anyone wanting to <a rel=\"noopener noreferrer\" href=\"https://www.insider.com/weight-loss-work-out-eat-calorie-deficit-not-losing-fat-2022-4\" target=\"_blank\" id=\"reader.external-link.num-7\">lose fat</a>.</p><p>High-protein foods help Mockler hit her daily target.</p>\n                        \n                        <h2>1. Greek yogurt</h2><p>Greek yogurt, or similar dairy products like Icelandic Skyr, is significantly higher in protein than other types of yogurt, and Mockler finds it filling and delicious thanks to the different flavor options on offer.</p><p>\"I use Greek yogurt in <a rel=\"noopener noreferrer\" href=\"https://www.businessinsider.com/smoothie-mistake-spiking-blood-sugar-healthy-recipes-dietitian-2023-1?r=US&amp;IR=T\" target=\"_blank\" id=\"reader.external-link.num-8\">smoothies</a>, to make a parfait, and on its own,\" she said. \"You can also swap out things like sour cream or ranch dressing for plain non-fat Greek yogurt.\" Greek yogurt is generally lower in fat and higher in protein than those foods.</p><h2>2. Eggs and egg whites</h2><p>Mockler makes omelets with a mix of whole eggs and egg whites from a carton.</p><p>\"This keeps the total fat content a bit lower (all of the fat of the egg is within the yolk) and increases the protein content with extra egg whites,\" she said. \"That way I still get the vitamins and minerals from the nutrient-packed yolk but without as much fat.\"</p>\n                        \n                        <h2>3. Lean meats</h2><p>Mockler regularly buys chicken breast, lean ground turkey, and lean ground beef, she said.</p><p>\"I'll throw the chicken breast in the slow cooker and then shred it for a few days' worth of protein,\" she said. \"I love adding lean ground meats in with pasta sauces to have over wholegrain noodles or using them in burrito bowls.\"</p><h2>4. Shelled edamame beans </h2><p>Edamame is a great <a rel=\"noopener noreferrer\" href=\"https://www.insider.com/guides/health/diet-nutrition/vegan-protein\" target=\"_blank\" id=\"reader.external-link.num-9\">vegan protein option</a>, Mockler said, and she likes to buy bags of frozen beans for convenience.</p><p>\"It's also a <a rel=\"noopener noreferrer\" href=\"https://www.insider.com/difference-between-incomplete-complete-protein-2019-1\" target=\"_blank\" id=\"reader.external-link.num-10\">complete protein</a> meaning it contains all of the essential amino acids, which not all vegan and vegetarian options do,\" she said.</p>\n                        \n                        <p>One cup of shelled edamame beans contains 18 grams of protein and 16 grams of <a rel=\"noopener noreferrer\" href=\"https://www.insider.com/tracked-fiber-intake-harder-than-expected-to-meet-daily-goal-2021-11\" target=\"_blank\" id=\"reader.external-link.num-11\">fiber</a>, Mockler said.</p><h2>5. Ready-to-drink protein shakes </h2><p>Mockler tries to get her protein from whole foods, but <a rel=\"noopener noreferrer\" href=\"https://www.insider.com/foods-to-boost-protein-intake-without-shakes-powder-dietitian-approved-2022-12\" target=\"_blank\" id=\"reader.external-link.num-12\">protein shakes</a> can be convenient when she's on-the-go or doesn't have time for a sit-down meal, she said.</p><p>\"There are dairy-based options and non-dairy options so everyone can find something that works for their diet preferences,\" Mockler said.</p>\n                    </div></article></div>",
      "textContent": "Registered dietitian and personal trainer Kara Mockler shared the five protein-rich foods she always has in her kitchen to help her feel and perform her best.Mockler eats a high-protein diet, aiming for 0.8-1.2 grams of protein per pound of bodyweight (or 1.6-2.4 grams per kilogram) daily, for multiple reasons, she said.\"Protein is utilized all throughout the body to not only build muscle but also hair, skin, and pretty much every other cell in our bodies,\" Mockler said.It's important to spread your protein intake evenly over the course of the day, rather than eating one large high-protein meal, she said.\n                        \n                        \"Eating protein consistently throughout the day allows our bodies to continually rebuild cells as they constantly turnover,\" Mockler said.Another reason protein is important in the diet is because it's satiating.\"Protein makes us feel full, which is especially useful for weight management,\" Mockler said, so protein is important for anyone wanting to lose fat.High-protein foods help Mockler hit her daily target.\n                        \n                        1. Greek yogurtGreek yogurt, or similar dairy products like Icelandic Skyr, is significantly higher in protein than other types of yogurt, and Mockler finds it filling and delicious thanks to the different flavor options on offer.\"I use Greek yogurt in smoothies, to make a parfait, and on its own,\" she said. \"You can also swap out things like sour cream or ranch dressing for plain non-fat Greek yogurt.\" Greek yogurt is generally lower in fat and higher in protein than those foods.2. Eggs and egg whitesMockler makes omelets with a mix of whole eggs and egg whites from a carton.\"This keeps the total fat content a bit lower (all of the fat of the egg is within the yolk) and increases the protein content with extra egg whites,\" she said. \"That way I still get the vitamins and minerals from the nutrient-packed yolk but without as much fat.\"\n                        \n                        3. Lean meatsMockler regularly buys chicken breast, lean ground turkey, and lean ground beef, she said.\"I'll throw the chicken breast in the slow cooker and then shred it for a few days' worth of protein,\" she said. \"I love adding lean ground meats in with pasta sauces to have over wholegrain noodles or using them in burrito bowls.\"4. Shelled edamame beans Edamame is a great vegan protein option, Mockler said, and she likes to buy bags of frozen beans for convenience.\"It's also a complete protein meaning it contains all of the essential amino acids, which not all vegan and vegetarian options do,\" she said.\n                        \n                        One cup of shelled edamame beans contains 18 grams of protein and 16 grams of fiber, Mockler said.5. Ready-to-drink protein shakes Mockler tries to get her protein from whole foods, but protein shakes can be convenient when she's on-the-go or doesn't have time for a sit-down meal, she said.\"There are dairy-based options and non-dairy options so everyone can find something that works for their diet preferences,\" Mockler said.\n                    ",
      "length": 3135,
      "excerpt": "Registered dietitian and personal trainer Kara Mockler shared the five protein-rich foods she always has in her kitchen to help her feel and perform her best.",
      "siteName": null,
      "summary": "Kara Mockler eats a high-protein diet, aiming for 0.8-1.2 grams of protein per pound of bodyweight. Greek yogurt is significantly higher in protein than other types of yogurt. Shelled edamame beans are a great vegan protein option.",
      "topics": [
        {
          "label": "Health",
          "score": 0.9976840019226074
        },
        {
          "label": "Sports",
          "score": 0.8753977417945862
        },
        {
          "label": "Science & Mathematics",
          "score": 0.7362837791442871
        }
      ]
    }
  },
  "3804148715": {
    "item_id": "3804148715",
    "resolved_id": "3804148715",
    "given_url": "https://phys.org/news/2023-02-autophagy-molecular-self-eating.html",
    "given_title": "",
    "favorite": "0",
    "status": "0",
    "time_added": "1676269858",
    "time_updated": "1676269863",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 7,
    "resolved_title": "Autophagy: The molecular regulation of self-eating",
    "resolved_url": "https://phys.org/news/2023-02-autophagy-molecular-self-eating.html",
    "excerpt": "Cellular stress and homeostasis in the tree of life. Credit: Picchianti/GMI. Autophagy, or \"self-eating\", is an essential cellular quality control mechanism that clears the cell of protein aggregates and damaged organelles.",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "1",
    "word_count": "890",
    "lang": "en",
    "time_to_read": 4,
    "amp_url": "https://phys.org/news/2023-02-autophagy-molecular-self-eating.amp",
    "top_image_url": "https://scx2.b-cdn.net/gfx/news/hires/2023/autophagy-the-molecula.jpg",
    "tags": {
      "readinglist": {
        "item_id": "3804148715",
        "tag": "readinglist"
      }
    },
    "authors": {
      "134017333": {
        "item_id": "3804148715",
        "author_id": "134017333",
        "name": "Gregor Mendel Institute",
        "url": ""
      }
    },
    "image": {
      "item_id": "3804148715",
      "src": "https://scx1.b-cdn.net/csz/news/800a/2023/autophagy-the-molecula.jpg",
      "width": "800",
      "height": "530"
    },
    "images": {
      "1": {
        "item_id": "3804148715",
        "image_id": "1",
        "src": "https://scx1.b-cdn.net/csz/news/800a/2023/autophagy-the-molecula.jpg",
        "width": "800",
        "height": "530",
        "credit": "",
        "caption": ""
      }
    },
    "domain_metadata": {
      "name": "Phys.org",
      "logo": "https://logo.clearbit.com/phys.org?size=800",
      "greyscale_logo": "https://logo.clearbit.com/phys.org?size=800&greyscale=true"
    },
    "listen_duration_estimate": 345,
    "content": {
      "title": "Autophagy: The molecular regulation of self-eating",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><div lang=\"en\">\n<div>\n<figure><figcaption>Cellular stress and homeostasis in the tree of life. Credit: Picchianti/GMI.</figcaption></figure></div>\n<p>Autophagy, or \"self-eating\", is an essential cellular quality control mechanism that clears the cell of protein aggregates and damaged organelles. This mechanism is inactive under normal conditions and only triggered upon persistent cellular stress.</p>\n<p>Researchers from the Gregor Mendel Institute of Molecular Plant Biology (GMI) of the Austrian Academy of Sciences and the Max Perutz Labs uncovered a <a rel=\"noopener noreferrer\" href=\"https://phys.org/tags/molecular+switch/\" target=\"_blank\" id=\"reader.external-link.num-0\">molecular switch</a> that regulates autophagy in plants. Combining evolutionary analysis with a mechanistic experimental approach, they demonstrated that this regulatory mechanism is conserved in eukaryotes. The findings were published on February 10th in <i>The EMBO Journal</i>.</p>\n<p>Persistent cellular stress, resulting from disturbances of cellular homeostasis, impairs cell fitness and lifespan. Cellular stress may develop, for example, when ribosomes collide while translating faulty mRNAs. As a result, cells get overburdened with unfinished and improperly formed protein products that form toxic protein aggregates.</p>\n<p>During cellular stress, cells can call on an arsenal of quality control (QC) mechanisms to restore homeostasis. Cells experiencing prolonged stress in the endoplasmic reticulum (ER), the cellular center for protein synthesis and transport, initiate an ER-specific autophagic pathway called \"ER-phagy\" to selectively remove damaged ER.</p>\n<p>When ribosomes collide on the ER, another QC pathway, called \"UFMylation,\" cooperates with ER-phagy to get rid of incompletely synthesized proteins at the ER membrane. UFMylation is an enigmatic QC pathway based on a protein post-translational modification that resembles ubiquitin and its functions are still being deciphered.</p>\n<p>Now, a team of researchers at the Vienna BioCenter uncovered an ancient molecular switch that regulates ER-phagy. Using a combination of evolutionary biology and mechanistic experimentation, the researchers demonstrated that the competition between two ubiquitin-like molecules, UFM1 and ATG8, creates a molecular switch in the master regulator C53, thus initiating ER-phagy.</p>\n<h2>UFMylation and ER-phagy: Bridging the pathways through similar, but distinct binding</h2>\n<p>\"Our previous work suggested that C53 could link the two quality control mechanisms, ER-phagy and UFMylation. However, the molecular nature of this bridge remained unclear,\" says the co-corresponding author and GMI group leader Yasin Dagdas. In the work in question, published in 2020, the scientists showed that C53 interacted with the protein ATG8, a ubiquitin-like player in the autophagy pathway, through non-canonical ATG8 Interacting Motif (AIM) sequences in C53's intrinsically disordered region.</p>\n<p>The researchers named these non-canonical AIMs \"shuffled AIMs\" (sAIMs). They also demonstrated that UFM1, the ubiquitin-like molecule that is attached as a chemical modification to protein substrates, competes with ATG8 for C53 binding. The C53 intrinsically disordered region contains three sAIM motifs and one canonical AIM (cAIM).</p>\n<p>\"Now, using Nuclear Magnetic Resonance spectroscopy, we showed that C53 sAIM1 and sAIM2 were UFM1's preferred binding motifs. On the other hand, ATG8 had a considerably higher preference for the cAIM motif on C53, as expected for a canonical binding sequence. Yet, ATG8 also interacted with sAIM1 and sAIM2, albeit with a lesser affinity,\" says Elif Karagöz, co-corresponding author and Max Perutz Labs group leader.</p>\n\n<h2>Tinkering with the binding shifts the balance between the pathways</h2>\n<p>Having uncovered the binding preferences of UFM1 and ATG8 in the intrinsically disordered region of C53, the team sought to test their function by substituting the sAIM motifs in C53 with canonical cAIM sequences. By introducing these mutations in Arabidopsis thaliana, the researchers effectively strengthened the binding affinity of ATG8 to C53 and impaired UFM1's binding.</p>\n<p>This led to constant firing through the C53 autophagy pathway and greatly sensitized the plants to ER stress. Thus, the team demonstrated that sAIMs are essential for regulating C53-mediated ER-phagy and thereby ER stress tolerance.</p>\n<h2>UFMylation is highly conserved in eukaryotes</h2>\n<p>The team analyzed the evolutionary path of C53, sAIMs, and UFMylation components with the help of Thomas A. Richards' lab at the University of Oxford. They demonstrated that C53-mediated autophagy was conserved among eukaryotes and that C53 co-evolved with the UFMylation pathway.</p>\n<p>Molecular remnants or the existence of related proteins indicated that fungi, some algae, and some eukaryotic parasites were subject to a secondary loss of UFMylation and/or C53. \"Our results show that C53 is very much linked to UFMylation, suggesting a highly conserved functional link. This applies to sAIMs in particular: In species that have lost UFM1, their C53 also lost its sAIMs,\" says Dagdas.</p>\n<p>With the help of Silvia Ramundo's lab at GMI, the researchers went further and demonstrated that the unicellular algae Chlamydomonas reinhardtii possesses a functional UFMylation pathway. This finding counters previous claims that the UFMylation pathway was linked to the evolution of multicellularity.</p>\n<h2>A powerful QC mechanism regulated by an ancient molecular switch</h2>\n<p>\"Taken together, our findings indicate that the non-canonical ATG8 interacting motifs evolved to allow another ubiquitin-like protein, UFM1, to bind C53 and keep it inactive under homeostatic conditions,\" says Dagdas. This mechanism is essential to prevent cells from \"eating\" healthy cellular components.</p>\n<p>Finally, seeing as fungi and some eukaryotic parasites have lost the UFMylation pathway at a more recent time in evolution, Dagdas believes that these organisms must have evolved analogous mechanisms to fulfill the same function, namely maintaining ER homeostasis. \"Identifying such mechanisms in fungi, but also in parasites affecting plants, animals, and even humans would open up potential translational avenues for new drugs,\" concludes Dagdas.</p>\n<p><strong>More information:</strong> Lorenzo Picchianti et al, Shuffled ATG8 interacting motifs form an ancestral bridge between UFMylation and autophagy, <i>The EMBO Journal</i> (2023). <a href=\"https://dx.doi.org/10.15252/embj.2022112053\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-1\">DOI: 10.15252/embj.2022112053</a></p></div></article></div>",
      "textContent": "\n\nCellular stress and homeostasis in the tree of life. Credit: Picchianti/GMI.\nAutophagy, or \"self-eating\", is an essential cellular quality control mechanism that clears the cell of protein aggregates and damaged organelles. This mechanism is inactive under normal conditions and only triggered upon persistent cellular stress.\nResearchers from the Gregor Mendel Institute of Molecular Plant Biology (GMI) of the Austrian Academy of Sciences and the Max Perutz Labs uncovered a molecular switch that regulates autophagy in plants. Combining evolutionary analysis with a mechanistic experimental approach, they demonstrated that this regulatory mechanism is conserved in eukaryotes. The findings were published on February 10th in The EMBO Journal.\nPersistent cellular stress, resulting from disturbances of cellular homeostasis, impairs cell fitness and lifespan. Cellular stress may develop, for example, when ribosomes collide while translating faulty mRNAs. As a result, cells get overburdened with unfinished and improperly formed protein products that form toxic protein aggregates.\nDuring cellular stress, cells can call on an arsenal of quality control (QC) mechanisms to restore homeostasis. Cells experiencing prolonged stress in the endoplasmic reticulum (ER), the cellular center for protein synthesis and transport, initiate an ER-specific autophagic pathway called \"ER-phagy\" to selectively remove damaged ER.\nWhen ribosomes collide on the ER, another QC pathway, called \"UFMylation,\" cooperates with ER-phagy to get rid of incompletely synthesized proteins at the ER membrane. UFMylation is an enigmatic QC pathway based on a protein post-translational modification that resembles ubiquitin and its functions are still being deciphered.\nNow, a team of researchers at the Vienna BioCenter uncovered an ancient molecular switch that regulates ER-phagy. Using a combination of evolutionary biology and mechanistic experimentation, the researchers demonstrated that the competition between two ubiquitin-like molecules, UFM1 and ATG8, creates a molecular switch in the master regulator C53, thus initiating ER-phagy.\nUFMylation and ER-phagy: Bridging the pathways through similar, but distinct binding\n\"Our previous work suggested that C53 could link the two quality control mechanisms, ER-phagy and UFMylation. However, the molecular nature of this bridge remained unclear,\" says the co-corresponding author and GMI group leader Yasin Dagdas. In the work in question, published in 2020, the scientists showed that C53 interacted with the protein ATG8, a ubiquitin-like player in the autophagy pathway, through non-canonical ATG8 Interacting Motif (AIM) sequences in C53's intrinsically disordered region.\nThe researchers named these non-canonical AIMs \"shuffled AIMs\" (sAIMs). They also demonstrated that UFM1, the ubiquitin-like molecule that is attached as a chemical modification to protein substrates, competes with ATG8 for C53 binding. The C53 intrinsically disordered region contains three sAIM motifs and one canonical AIM (cAIM).\n\"Now, using Nuclear Magnetic Resonance spectroscopy, we showed that C53 sAIM1 and sAIM2 were UFM1's preferred binding motifs. On the other hand, ATG8 had a considerably higher preference for the cAIM motif on C53, as expected for a canonical binding sequence. Yet, ATG8 also interacted with sAIM1 and sAIM2, albeit with a lesser affinity,\" says Elif Karagöz, co-corresponding author and Max Perutz Labs group leader.\n\nTinkering with the binding shifts the balance between the pathways\nHaving uncovered the binding preferences of UFM1 and ATG8 in the intrinsically disordered region of C53, the team sought to test their function by substituting the sAIM motifs in C53 with canonical cAIM sequences. By introducing these mutations in Arabidopsis thaliana, the researchers effectively strengthened the binding affinity of ATG8 to C53 and impaired UFM1's binding.\nThis led to constant firing through the C53 autophagy pathway and greatly sensitized the plants to ER stress. Thus, the team demonstrated that sAIMs are essential for regulating C53-mediated ER-phagy and thereby ER stress tolerance.\nUFMylation is highly conserved in eukaryotes\nThe team analyzed the evolutionary path of C53, sAIMs, and UFMylation components with the help of Thomas A. Richards' lab at the University of Oxford. They demonstrated that C53-mediated autophagy was conserved among eukaryotes and that C53 co-evolved with the UFMylation pathway.\nMolecular remnants or the existence of related proteins indicated that fungi, some algae, and some eukaryotic parasites were subject to a secondary loss of UFMylation and/or C53. \"Our results show that C53 is very much linked to UFMylation, suggesting a highly conserved functional link. This applies to sAIMs in particular: In species that have lost UFM1, their C53 also lost its sAIMs,\" says Dagdas.\nWith the help of Silvia Ramundo's lab at GMI, the researchers went further and demonstrated that the unicellular algae Chlamydomonas reinhardtii possesses a functional UFMylation pathway. This finding counters previous claims that the UFMylation pathway was linked to the evolution of multicellularity.\nA powerful QC mechanism regulated by an ancient molecular switch\n\"Taken together, our findings indicate that the non-canonical ATG8 interacting motifs evolved to allow another ubiquitin-like protein, UFM1, to bind C53 and keep it inactive under homeostatic conditions,\" says Dagdas. This mechanism is essential to prevent cells from \"eating\" healthy cellular components.\nFinally, seeing as fungi and some eukaryotic parasites have lost the UFMylation pathway at a more recent time in evolution, Dagdas believes that these organisms must have evolved analogous mechanisms to fulfill the same function, namely maintaining ER homeostasis. \"Identifying such mechanisms in fungi, but also in parasites affecting plants, animals, and even humans would open up potential translational avenues for new drugs,\" concludes Dagdas.\nMore information: Lorenzo Picchianti et al, Shuffled ATG8 interacting motifs form an ancestral bridge between UFMylation and autophagy, The EMBO Journal (2023). DOI: 10.15252/embj.2022112053",
      "length": 6186,
      "excerpt": "Autophagy, or \"self-eating\", is an essential cellular quality control mechanism that clears the cell of protein aggregates and damaged organelles. This mechanism is inactive under normal conditions and only triggered upon persistent cellular stress.",
      "siteName": null,
      "summary": "Researchers from the Gregor Mendel Institute of Molecular Plant Biology (GMI) of the Austrian Academy of Sciences and the Max Perutz Labs uncovered a molecular switch that regulates autophagy in plants. Combining evolutionary analysis with a mechanistic experimental approach, they demonstrated that this regulatory mechanism is conserved in eukaryotes.",
      "topics": [
        {
          "label": "Science & Mathematics",
          "score": 0.9966766834259033
        },
        {
          "label": "Education & Reference",
          "score": 0.8845393657684326
        },
        {
          "label": "Business & Finance",
          "score": 0.5920519232749939
        }
      ]
    }
  },
  "3805162646": {
    "item_id": "3805162646",
    "resolved_id": "3805162646",
    "given_url": "https://www.zdnet.com/article/i-wanted-to-try-the-new-bing-with-chatgpt-then-microsoft-went-all-microsoft/",
    "given_title": "I wanted to try the new Bing with ChatGPT. Then, Microsoft went all Microso",
    "favorite": "0",
    "status": "0",
    "time_added": "1676356031",
    "time_updated": "1676356037",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 6,
    "resolved_title": "I wanted to try the new Bing with ChatGPT. Then, Microsoft went all Microsoft",
    "resolved_url": "https://www.zdnet.com/article/i-wanted-to-try-the-new-bing-with-chatgpt-then-microsoft-went-all-microsoft/",
    "excerpt": "This is Microsoft's moment. At last it can stick it to companies that, once upon a time, laughed in its face.",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "0",
    "word_count": "681",
    "lang": "en",
    "time_to_read": 3,
    "amp_url": "https://www.zdnet.com/google-amp/article/i-wanted-to-try-the-new-bing-with-chatgpt-then-microsoft-went-all-microsoft/",
    "top_image_url": "https://www.zdnet.com/a/img/resize/b983bf5762a8f94c5b2d252402ee4a741d778a15/2023/02/10/e283a6d5-ae14-4b64-b5cf-97bd226ecddb/gettyimages-1246861884.jpg?auto=webp&fit=crop&height=675&width=1200",
    "tags": {
      "readinglist": {
        "item_id": "3805162646",
        "tag": "readinglist"
      }
    },
    "authors": {
      "1362": {
        "item_id": "3805162646",
        "author_id": "1362",
        "name": "Chris Matyszczyk",
        "url": ""
      }
    },
    "domain_metadata": {
      "name": "ZDNet",
      "logo": "https://logo.clearbit.com/zdnet.com?size=800",
      "greyscale_logo": "https://logo.clearbit.com/zdnet.com?size=800&greyscale=true"
    },
    "listen_duration_estimate": 264,
    "content": {
      "title": "I wanted to try the new Bing with ChatGPT. Then, Microsoft went all Microsoft",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><div lang=\"en\">\n<figure>\n<figcaption><div>\n<p>I can almost touch it.</p>\n</div>\n<span>NurPhoto/Getty Images</span></figcaption></figure><p>This is Microsoft's moment.</p>\n<p>At last it can stick it to companies that, once upon a time, laughed in its face.</p>\n<p>Like Google, Apple, and, well, Nokia.</p>\n\n<p>Microsoft CEO Satya Nadella has leaped on the latest tech wizardry -- or, perhaps, the latest wildly imperfect tech nonsense to enchant simple minds -- and brought it to Microsoft's red-headed stepchild, Bing.</p>\n\n<p>Yes, <a rel=\"noopener noreferrer\" href=\"https://www.zdnet.com/article/microsoft-taps-chatgpt-to-improve-bing-and-the-edge-browser/\" target=\"_blank\" id=\"reader.external-link.num-0\">new, improved Bing</a>. Now with ChatGPT.</p>\n<p>Naturally, Microsoft couldn't let everyone try it at once. It's entirely understandable. <span><a href=\"https://www.zdnet.com/article/chatgpt-productivity-hacks-five-ways-to-use-chatbots-to-make-your-life-easier/\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-1\"><span>ChatGPT</span></a></span> itself seems often overloaded.</p>\n<p>I still wanted to see whether my paltry status as someone who gives Microsoft money to use Word, PowerPoint, and other highly revolutionary products actually mattered.</p>\n<p>I rushed to Bing. Not a sentence I've ever written before, I think.</p>\n<p><strong>Also:</strong> <a rel=\"noopener noreferrer\" href=\"https://www.zdnet.com/article/microsoft-taps-chatgpt-to-improve-bing-and-the-edge-browser/\" target=\"_blank\" id=\"reader.external-link.num-2\"><strong>'More powerful than ChatGPT': Microsoft unveils AI-improved Bing search and Edge browser</strong></a></p>\n<p>This chatbox-like superior intelligence thing is, I feared, little more than than the latest tech industry windbaggery. You know, like self-driving cars and making the world a better place. Still, I was willing to be open. Open enough. <a rel=\"noopener noreferrer\" href=\"https://www.zdnet.com/article/openais-chatgpt-is-scary-good-at-my-job-but-it-cant-replace-me-yet/\" target=\"_blank\" id=\"reader.external-link.num-3\">OpenAI</a> enough.</p>\n\n<p>I wasn't sure whether I'd get any joy when I clicked on the chat button, so I was prepared for disappointment. And that's what I got.</p>\n<p>\"Chat mode is only available when you have access to the new Bing,\" said the message. I was enticed, however, to join the waitlist for Microsoft's new \"AI-powered answer engine.\"</p>\n<figure>\n<figcaption><div>\n<p>So exciting.</p>\n</div>\n<span>Screenshot by Chris Matyszczyk/ZDNET</span></figcaption></figure><p>With barely suppressed glee, I aggressively pressed home my willingness to join the waitlist.</p>\n<p>Surely, once I'd handed over my email address, Microsoft would say: \"Yo, Chris. How's it going? Thank you for all the money you send us every month. We'll put you somewhere near the front of the line.\"</p>\n\n<p>Instead, Microsoft came over all Microsoft. You know, the one that you adored so infinitely back in happy days of pearly Gates.</p>\n\n<p>For this was Microsoft's next little touch of prestidigitation.</p>\n<p>\"On the waitlist?\" mused Microsoft. \"Access the new Bing even faster.\"</p>\n<p>But I haven't accessed the new Bing at all because I'm, you know, on the waitlist.</p>\n<p><strong>Also:</strong> </p>\n<p>Here it was, though -- surely -- the recognition I'd been waiting for. The next sentence, you see, read: \"Get ahead in the line when you complete the following...\"</p>\n<p>Don't tell me, I have to enter how much I pay Microsoft every month? Oh, not quite.</p>\n<p>Here was secret waitlist passage No. 1: \"Set Microsoft defaults on your PC.\"</p>\n<p>These were my instant feelings: \"Dearest Microsoft, are you really trying to use the entrance of a supposedly entrancing new Bing to force me to make Microsoft my default everything?\"</p>\n\n<p>This was shortly followed by: \"Dearest Microsoft, would you just take a run and jump?\"</p>\n<p>I had no evidence that making Microsoft my everything-default would speed me to the no-doubt monstrously superb new Bing.</p>\n<p>Yet there was Microsoft, again, grinning like a used car salesperson of the rather sniveling kind.</p>\n<p>Oh, there was another option to supposedly garner the favor of the Bing bouncers. I could scan a QR code and download the Bing app.</p>\n\n<p><strong>Also:</strong> <a rel=\"noopener noreferrer\" href=\"https://www.zdnet.com/article/how-to-use-chatgpt/\" target=\"_blank\" id=\"reader.external-link.num-4\"><strong>How to get started using ChatGPT</strong></a></p>\n<p>Trying to maintain a modicum of decorum, I succumbed. I opened the Bing app. Did it instantly send me ahead of those on the waitlist? Not as far as I could see.</p>\n<p>It did, though, inform me that it could see I was on the waitlist.</p>\n\n<p>Please absolve me of my occasional intolerance, but this felt just a tinge sordid. And on such a happy day.</p>\n<p>I know you'll tell me one rarely gets something for nothing. I know you'll sigh and say, \"Everyone does this.\"</p>\n<p>But if you really are the company that once called Google \"Scroogle,\" maybe it's worth opening your arms to the whole of humanity without attaching irritating strings to their beings.</p>\n<p><strong>Also:</strong> <a rel=\"noopener noreferrer\" href=\"https://www.zdnet.com/article/what-is-google-bard-heres-everything-you-need-to-know/\" target=\"_blank\" id=\"reader.external-link.num-5\"><strong>What is Google Bard? Here's everything you need to know</strong></a></p>\n<p>Be the new, gloriously open, work-with-anybody Microsoft. It might even make people like you all the more.</p>\n<p>Or at least forget how you <a rel=\"noopener noreferrer\" href=\"https://www.zdnet.com/article/microsoft-edge-is-malware-says-angry-windows-7-user/\" target=\"_blank\" id=\"reader.external-link.num-6\">tried to force everyone to use Microsoft Edge</a>.</p>\n</div></article></div>",
      "textContent": "\n\n\nI can almost touch it.\n\nNurPhoto/Getty ImagesThis is Microsoft's moment.\nAt last it can stick it to companies that, once upon a time, laughed in its face.\nLike Google, Apple, and, well, Nokia.\n\nMicrosoft CEO Satya Nadella has leaped on the latest tech wizardry -- or, perhaps, the latest wildly imperfect tech nonsense to enchant simple minds -- and brought it to Microsoft's red-headed stepchild, Bing.\n\nYes, new, improved Bing. Now with ChatGPT.\nNaturally, Microsoft couldn't let everyone try it at once. It's entirely understandable. ChatGPT itself seems often overloaded.\nI still wanted to see whether my paltry status as someone who gives Microsoft money to use Word, PowerPoint, and other highly revolutionary products actually mattered.\nI rushed to Bing. Not a sentence I've ever written before, I think.\nAlso: 'More powerful than ChatGPT': Microsoft unveils AI-improved Bing search and Edge browser\nThis chatbox-like superior intelligence thing is, I feared, little more than than the latest tech industry windbaggery. You know, like self-driving cars and making the world a better place. Still, I was willing to be open. Open enough. OpenAI enough.\n\nI wasn't sure whether I'd get any joy when I clicked on the chat button, so I was prepared for disappointment. And that's what I got.\n\"Chat mode is only available when you have access to the new Bing,\" said the message. I was enticed, however, to join the waitlist for Microsoft's new \"AI-powered answer engine.\"\n\n\nSo exciting.\n\nScreenshot by Chris Matyszczyk/ZDNETWith barely suppressed glee, I aggressively pressed home my willingness to join the waitlist.\nSurely, once I'd handed over my email address, Microsoft would say: \"Yo, Chris. How's it going? Thank you for all the money you send us every month. We'll put you somewhere near the front of the line.\"\n\nInstead, Microsoft came over all Microsoft. You know, the one that you adored so infinitely back in happy days of pearly Gates.\n\nFor this was Microsoft's next little touch of prestidigitation.\n\"On the waitlist?\" mused Microsoft. \"Access the new Bing even faster.\"\nBut I haven't accessed the new Bing at all because I'm, you know, on the waitlist.\nAlso: \nHere it was, though -- surely -- the recognition I'd been waiting for. The next sentence, you see, read: \"Get ahead in the line when you complete the following...\"\nDon't tell me, I have to enter how much I pay Microsoft every month? Oh, not quite.\nHere was secret waitlist passage No. 1: \"Set Microsoft defaults on your PC.\"\nThese were my instant feelings: \"Dearest Microsoft, are you really trying to use the entrance of a supposedly entrancing new Bing to force me to make Microsoft my default everything?\"\n\nThis was shortly followed by: \"Dearest Microsoft, would you just take a run and jump?\"\nI had no evidence that making Microsoft my everything-default would speed me to the no-doubt monstrously superb new Bing.\nYet there was Microsoft, again, grinning like a used car salesperson of the rather sniveling kind.\nOh, there was another option to supposedly garner the favor of the Bing bouncers. I could scan a QR code and download the Bing app.\n\nAlso: How to get started using ChatGPT\nTrying to maintain a modicum of decorum, I succumbed. I opened the Bing app. Did it instantly send me ahead of those on the waitlist? Not as far as I could see.\nIt did, though, inform me that it could see I was on the waitlist.\n\nPlease absolve me of my occasional intolerance, but this felt just a tinge sordid. And on such a happy day.\nI know you'll tell me one rarely gets something for nothing. I know you'll sigh and say, \"Everyone does this.\"\nBut if you really are the company that once called Google \"Scroogle,\" maybe it's worth opening your arms to the whole of humanity without attaching irritating strings to their beings.\nAlso: What is Google Bard? Here's everything you need to know\nBe the new, gloriously open, work-with-anybody Microsoft. It might even make people like you all the more.\nOr at least forget how you tried to force everyone to use Microsoft Edge.\n",
      "length": 4043,
      "excerpt": "I can almost touch it.",
      "siteName": null,
      "summary": "Microsoft CEO Satya Nadella has leaped on the latest tech wizardry. He brought it to Microsoft's stepchild, Bing. The chatbox-like superior intelligence thing is, I feared, little more than than tech industry windbaggery. Still, I was willing to be open.",
      "topics": [
        {
          "label": "Computers & Internet",
          "score": 0.9926115870475769
        },
        {
          "label": "Business & Finance",
          "score": 0.967966616153717
        },
        {
          "label": "Education & Reference",
          "score": 0.5387672185897827
        }
      ]
    }
  },
  "3807303856": {
    "item_id": "3807303856",
    "resolved_id": "3807213852",
    "given_url": "https://link.medium.com/Lma7OlUBrxb",
    "given_title": "",
    "favorite": "0",
    "status": "0",
    "time_added": "1676483382",
    "time_updated": "1676483392",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 4,
    "resolved_title": "Is there always a tradeoff between bias and variance?",
    "resolved_url": "https://kozyrkov.medium.com/is-there-always-a-tradeoff-between-bias-and-variance-5ca44398a552",
    "excerpt": "Should you read this article? If you understand all the words in the next section, then no. If you don’t care to understand them, then also no. If you want the bolded bits explained, then yes. It isn’t.",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "0",
    "word_count": "771",
    "lang": "en",
    "time_to_read": 4,
    "top_image_url": "https://miro.medium.com/max/600/1*NDqbOdm56alWHuLGE3d0Tg.png",
    "tags": {
      "readinglist": {
        "item_id": "3807303856",
        "tag": "readinglist"
      }
    },
    "authors": {
      "143764983": {
        "item_id": "3807303856",
        "author_id": "143764983",
        "name": "Cassie Kozyrkov",
        "url": "https://kozyrkov.medium.com"
      }
    },
    "listen_duration_estimate": 298,
    "content": {
      "title": "",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><div lang=\"en\"><h2>The bias-variance tradeoff, part 1 of 3</h2><p>Should you read this article? If you understand all the words in the next section, then no. If you don’t care to understand them, then also no. If you want the bolded bits explained, then yes.</p><h2>The bias-variance tradeoff</h2><p><em>“The bias-variance tradeoff”</em> is a popular phrase you’ll hear in the context of <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_emperor\" target=\"_blank\" id=\"reader.external-link.num-0\">ML/AI</a>. If you’re a <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_statistics\" target=\"_blank\" id=\"reader.external-link.num-1\">statistician</a>, you might think it’s about summarizing this formula:</p><p><strong>MSE = Bias² + Variance</strong></p><p>It isn’t.</p><p>Well, it’s loosely related, but the phrase actually refers to a <em>practical recipe</em> for how to pick a model’s <strong>complexity sweet spot</strong>. It’s most useful when you’re tuning a <strong>regularization</strong> <strong>hyperparameter</strong>.</p><figure></figure><p><strong><em>Note: </em></strong><em>If you’ve never heard of the MSE, you might need a bit of help with some of the jargon. When you hit a new term you want explained in more detail, you can follow the links to my other articles where I introduce the words I’m using.</em></p><h2>Understanding the basics</h2><p>The mean squared error (<a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_babymse\" target=\"_blank\" id=\"reader.external-link.num-2\">MSE</a>) is the most popular (and vanilla) choice for a model’s <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_emperorm\" target=\"_blank\" id=\"reader.external-link.num-3\">loss function</a> and it tends to be the first one you’re taught. You’ll likely take a whole bunch of <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_statistics\" target=\"_blank\" id=\"reader.external-link.num-4\">stats</a> classes before it occurs to anyone to tell you that you’re welcome to minimize other loss functions if you like. (But let’s be real: <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_msefav\" target=\"_blank\" id=\"reader.external-link.num-5\">parabolae are super easy to optimize</a>. Remember <em>d/dx</em> <em>x²</em>? 2<em>x</em>. That convenience is enough to keep most of you loyal to the MSE.)</p><p>Once you learn about the MSE, it’s usually mere <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_lemur\" target=\"_blank\" id=\"reader.external-link.num-6\">moments</a> until someone mentions the bias and variance formula:</p><blockquote><p>MSE = Bias² + Variance</p></blockquote><p><a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_lemur\" target=\"_blank\" id=\"reader.external-link.num-7\">I did it too</a> and, like a garden variety <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_datascim\" target=\"_blank\" id=\"reader.external-link.num-8\">data science</a> jerk, left the proof as <a rel=\"noopener noreferrer\" href=\"https://twitter.com/kareem_carr/status/1535402776276217859?t=R60xEOxxTYkBq7mkj8CvnQ&amp;s=09\" target=\"_blank\" id=\"reader.external-link.num-9\">homework for the interested reader</a>.</p><p>Let’s make amends — if you’d like me to derive it for you while making snide comments in the margins, take a small detour to <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_mseformula\" target=\"_blank\" id=\"reader.external-link.num-10\">here</a>. If you choose to skip the mathy stuff, then you’ll have to put up with my jazz hands and just take my word for it.</p><h2>Positive vibes only</h2><p>Want me to tell the key thing to you bluntly? Notice that the formula consists of two terms that <strong>can’t be negative</strong>.</p><p>The quantity (MSE) you’re trying to optimize when you fit your predictive <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_emperor\" target=\"_blank\" id=\"reader.external-link.num-11\">ML/AI models</a> can be decomposed into <strong>always-positive terms</strong> that involve bias only and variance only.</p><blockquote><p>MSE = Bias² + Variance = (Bias)² + (Standard Deviation)²</p></blockquote><p>Even more bluntly? Okay, sure.</p><p>A better model has a lower MSE. E stands for <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_msefav\" target=\"_blank\" id=\"reader.external-link.num-12\">error</a> and fewer errors are better, so the <em>best </em>model has a zero MSE: it makes no mistakes. That means it also has <em>no bias</em> and <em>no variance</em>.</p><figure><figcaption>Photo by <a rel=\"noopener noreferrer\" href=\"https://unsplash.com/de/@laurarain?utm_source=medium&amp;utm_medium=referral\" target=\"_blank\" id=\"reader.external-link.num-13\">Laura Crowe</a> on <a rel=\"noopener noreferrer\" href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\" target=\"_blank\" id=\"reader.external-link.num-14\">Unsplash</a></figcaption></figure><p>Instead of perfect models, let’s look at going from good to better. <strong>If you’re truly able to improve your model (in terms of MSE), there’s no need for a tradeoff between bias and variance.</strong> If you became a better archer, you became a better archer. No tradeoff. (You probably needed more practice — <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_hist\" target=\"_blank\" id=\"reader.external-link.num-15\">data</a>! — to get there.)</p><blockquote><p>As Tolstoy would say, all perfect models are alike, but each unhappy model can be unhappy in its own way.</p></blockquote><h2>All perfect models are alike</h2><p>As <a rel=\"noopener noreferrer\" href=\"https://en.wikipedia.org/wiki/Anna_Karenina_principle\" target=\"_blank\" id=\"reader.external-link.num-16\">Tolstoy</a> would say, all perfect models are alike, but each unhappy model (for a given MSE) can be unhappy in its own way. You can get two equally rubbish yet different models with the same MSE: one model can have really good (low) bias but high variance while the other can have really good (low) variance but high bias, and yet both can have the same MSE (overall score).</p><figure></figure><p>If we measure the performance of an archer by MSE, we’re saying that decreasing an archer’s standard deviation is worth the same as an equivalent decrease in bias. We’re saying we’re indifferent between the two. (Wait, what if <em>you’re </em>not indifferent between them? Then the MSE might not be your best choice here. Don’t like the MSE’s way of scoring performance? Not a problem. Make your own <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_emperor\" target=\"_blank\" id=\"reader.external-link.num-17\">loss function</a>.)</p><p>Now that we’ve set the table, head over to <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_bivar2\" target=\"_blank\" id=\"reader.external-link.num-18\">Part 2</a> where we dig into the heart of the matter: Is there an actual tradeoff? (Yes! But not where you might think.) And what does overfitting have to do with it? (Hint: everything!)</p><h2>Thanks for reading! How about a YouTube course?</h2><p>If you had fun here and you’re looking for an entire applied AI course designed to be fun for beginners and experts alike, here’s the one I made for your amusement:</p><figure></figure><h2>Looking for hands-on ML/AI tutorials?</h2><p>Here are some of my favorite 10 minute walkthroughs:</p><ul><li><a rel=\"noopener noreferrer\" href=\"https://console.cloud.google.com/?walkthrough_id=automl_quickstart\" target=\"_blank\" id=\"reader.external-link.num-19\">AutoML</a></li><li><a rel=\"noopener noreferrer\" href=\"https://bit.ly/kozvertex\" target=\"_blank\" id=\"reader.external-link.num-20\">Vertex AI</a></li><li><a rel=\"noopener noreferrer\" href=\"https://bit.ly/kozvertexnotebooks\" target=\"_blank\" id=\"reader.external-link.num-21\">AI notebooks</a></li><li><a rel=\"noopener noreferrer\" href=\"https://bit.ly/kozvertextables\" target=\"_blank\" id=\"reader.external-link.num-22\">ML for tabular data</a></li><li><a rel=\"noopener noreferrer\" href=\"https://bit.ly/kozvertextext\" target=\"_blank\" id=\"reader.external-link.num-23\">Text classification</a></li><li><a rel=\"noopener noreferrer\" href=\"https://bit.ly/kozverteximage\" target=\"_blank\" id=\"reader.external-link.num-24\">Image classification</a></li><li><a rel=\"noopener noreferrer\" href=\"https://bit.ly/kozvertexvideo\" target=\"_blank\" id=\"reader.external-link.num-25\">Video classification</a></li></ul></div></article></div>",
      "textContent": "The bias-variance tradeoff, part 1 of 3Should you read this article? If you understand all the words in the next section, then no. If you don’t care to understand them, then also no. If you want the bolded bits explained, then yes.The bias-variance tradeoff“The bias-variance tradeoff” is a popular phrase you’ll hear in the context of ML/AI. If you’re a statistician, you might think it’s about summarizing this formula:MSE = Bias² + VarianceIt isn’t.Well, it’s loosely related, but the phrase actually refers to a practical recipe for how to pick a model’s complexity sweet spot. It’s most useful when you’re tuning a regularization hyperparameter.Note: If you’ve never heard of the MSE, you might need a bit of help with some of the jargon. When you hit a new term you want explained in more detail, you can follow the links to my other articles where I introduce the words I’m using.Understanding the basicsThe mean squared error (MSE) is the most popular (and vanilla) choice for a model’s loss function and it tends to be the first one you’re taught. You’ll likely take a whole bunch of stats classes before it occurs to anyone to tell you that you’re welcome to minimize other loss functions if you like. (But let’s be real: parabolae are super easy to optimize. Remember d/dx x²? 2x. That convenience is enough to keep most of you loyal to the MSE.)Once you learn about the MSE, it’s usually mere moments until someone mentions the bias and variance formula:MSE = Bias² + VarianceI did it too and, like a garden variety data science jerk, left the proof as homework for the interested reader.Let’s make amends — if you’d like me to derive it for you while making snide comments in the margins, take a small detour to here. If you choose to skip the mathy stuff, then you’ll have to put up with my jazz hands and just take my word for it.Positive vibes onlyWant me to tell the key thing to you bluntly? Notice that the formula consists of two terms that can’t be negative.The quantity (MSE) you’re trying to optimize when you fit your predictive ML/AI models can be decomposed into always-positive terms that involve bias only and variance only.MSE = Bias² + Variance = (Bias)² + (Standard Deviation)²Even more bluntly? Okay, sure.A better model has a lower MSE. E stands for error and fewer errors are better, so the best model has a zero MSE: it makes no mistakes. That means it also has no bias and no variance.Photo by Laura Crowe on UnsplashInstead of perfect models, let’s look at going from good to better. If you’re truly able to improve your model (in terms of MSE), there’s no need for a tradeoff between bias and variance. If you became a better archer, you became a better archer. No tradeoff. (You probably needed more practice — data! — to get there.)As Tolstoy would say, all perfect models are alike, but each unhappy model can be unhappy in its own way.All perfect models are alikeAs Tolstoy would say, all perfect models are alike, but each unhappy model (for a given MSE) can be unhappy in its own way. You can get two equally rubbish yet different models with the same MSE: one model can have really good (low) bias but high variance while the other can have really good (low) variance but high bias, and yet both can have the same MSE (overall score).If we measure the performance of an archer by MSE, we’re saying that decreasing an archer’s standard deviation is worth the same as an equivalent decrease in bias. We’re saying we’re indifferent between the two. (Wait, what if you’re not indifferent between them? Then the MSE might not be your best choice here. Don’t like the MSE’s way of scoring performance? Not a problem. Make your own loss function.)Now that we’ve set the table, head over to Part 2 where we dig into the heart of the matter: Is there an actual tradeoff? (Yes! But not where you might think.) And what does overfitting have to do with it? (Hint: everything!)Thanks for reading! How about a YouTube course?If you had fun here and you’re looking for an entire applied AI course designed to be fun for beginners and experts alike, here’s the one I made for your amusement:Looking for hands-on ML/AI tutorials?Here are some of my favorite 10 minute walkthroughs:AutoMLVertex AIAI notebooksML for tabular dataText classificationImage classificationVideo classification",
      "length": 4315,
      "excerpt": "Should you read this article? If you understand all the words in the next section, then no. If you don’t care to understand them, then also no. If you want the bolded bits explained, then yes.",
      "siteName": null,
      "summary": "The bias-variance tradeoff is a popular phrase you’ll hear in the context of ML/AI. The phrase actually refers to a recipe for how to pick a model’s complexity sweet spot. If you don’t understand all the words in the next section, then no. But if you want the bolded bits explained, then yes.",
      "topics": [
        {
          "label": "Science & Mathematics",
          "score": 0.9963969588279724
        },
        {
          "label": "Education & Reference",
          "score": 0.959370493888855
        },
        {
          "label": "Computers & Internet",
          "score": 0.8699377775192261
        }
      ]
    }
  },
  "3807304089": {
    "item_id": "3807304089",
    "resolved_id": "3807304103",
    "given_url": "https://link.medium.com/wZCdJhWBrxb",
    "given_title": "",
    "favorite": "0",
    "status": "0",
    "time_added": "1676483407",
    "time_updated": "1676483412",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 3,
    "resolved_title": "Overfitting, underfitting, and regularization",
    "resolved_url": "https://kozyrkov.medium.com/overfitting-underfitting-and-regularization-7f83dd998a62",
    "excerpt": "All perfect models are alike, but each unhappy model can be unhappy in its own way. To make the most of this article, I suggest taking a look at Part 1 to make sure you’re well-situated to absorb this one.",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "0",
    "word_count": "720",
    "lang": "en",
    "time_to_read": 3,
    "top_image_url": "https://miro.medium.com/max/1200/1*MAeZVKPPAtMonrs6x0ebBQ.png",
    "tags": {
      "readinglist": {
        "item_id": "3807304089",
        "tag": "readinglist"
      }
    },
    "authors": {
      "143764983": {
        "item_id": "3807304089",
        "author_id": "143764983",
        "name": "Cassie Kozyrkov",
        "url": "https://kozyrkov.medium.com"
      }
    },
    "listen_duration_estimate": 279,
    "content": {
      "title": "",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><div lang=\"en\"><h2>The bias-variance tradeoff, part 2 of 3</h2><p>In <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_bivar1\" target=\"_blank\" id=\"reader.external-link.num-0\">Part 1</a>, we covered much of the basic terminology as well as a few key insights about the bias-variance formula (<strong>MSE = Bias² + Variance</strong>), including this misquote from <a rel=\"noopener noreferrer\" href=\"https://www.goodreads.com/work/quotes/2507928\" target=\"_blank\" id=\"reader.external-link.num-1\">Anna Karenina</a>:</p><blockquote><p>All perfect models are alike, but each unhappy model can be unhappy in its own way.</p></blockquote><p>To make the most of <em>this </em>article, I suggest taking a look at <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_bivar1\" target=\"_blank\" id=\"reader.external-link.num-2\">Part 1</a> to make sure you’re well-situated to absorb this one.</p><figure></figure><h2>What does overfitting/underfitting have to do with it?</h2><p>Let’s say you have a <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_emperorm\" target=\"_blank\" id=\"reader.external-link.num-3\">model</a> that is as good as you’re going to get for the information you have.</p><p>To have an even better model, you need better <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_hist\" target=\"_blank\" id=\"reader.external-link.num-4\">data</a>. In other words, more data (quantity) or <em>more relevant</em> data (quality).</p><p>When I say as <em>good </em>as you’re going to get, I mean in “good” terms of <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_msefav\" target=\"_blank\" id=\"reader.external-link.num-5\">MSE</a> performance on data your model hasn’t seen before. (It’s supposed to <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_parrot\" target=\"_blank\" id=\"reader.external-link.num-6\"><em>pre</em>dict</a>, not <em>post</em>dict.) You’ve done a perfect job of getting what you can from the information you have — the rest is error you can’t do anything about with your information.</p><blockquote><p>Reality = Best Model + Unavoidable Error</p></blockquote><p>But here’s the problem… we’ve jumped ahead; <strong>you don’t have this model yet.</strong></p><p>All you have is a pile of old data to learn this model from. Eventually, if you’re smart, you’ll <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_idiot\" target=\"_blank\" id=\"reader.external-link.num-7\">validate this model</a> on data it hasn’t seen before, but first you have to learn the model by finding useful patterns in data and trying to inch closer and closer to the stated objective: an MSE that’s as low as possible.</p><p>Unfortunately, during the <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_mrbean\" target=\"_blank\" id=\"reader.external-link.num-8\">learning process</a>, you don’t get to observe the MSE you’re after (the one that comes from reality). You only get to compute a shoddy version from your current training dataset.</p><figure><figcaption>Photo by <a rel=\"noopener noreferrer\" href=\"https://unsplash.com/@ninjason?utm_source=medium&amp;utm_medium=referral\" target=\"_blank\" id=\"reader.external-link.num-9\">Jason Leung</a> on <a rel=\"noopener noreferrer\" href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\" target=\"_blank\" id=\"reader.external-link.num-10\">Unsplash</a></figcaption></figure><p>Oh, and also, in this example “you” are not a human, you’re an <a rel=\"noopener noreferrer\" href=\"http://bit.ly/mfml_045\" target=\"_blank\" id=\"reader.external-link.num-11\">optimization algorithm</a> that was told by your human boss to twiddle the dials in the model’s settings until the MSE is as low as it will go.</p><p>You say, <em>“Sweet! I can do this!! Boss, if you give me an extremely flexible model with lots of settings to fiddle (</em><a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_emperor\" target=\"_blank\" id=\"reader.external-link.num-12\"><em>neural networks</em></a><em>, anyone?), I can give you a perfect training MSE. No bias and no variance.”</em></p><p>The way to get a better training MSE than the true model’s test MSE is to fit all the noise (errors you have no predictively-useful information about) along with the signal. How do you achieve this little miracle? By making the model more complicated. Connecting the dots, essentially.</p><p>This is called <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_sydd\" target=\"_blank\" id=\"reader.external-link.num-13\">overfitting</a>. Such a model has an excellent training MSE but a whopper of a variance when you try to use it for anything practical. That’s what you get for trying to cheat by creating a solution with <a rel=\"noopener noreferrer\" href=\"http://bit.ly/mfml_049\" target=\"_blank\" id=\"reader.external-link.num-14\">more complexity than your information supports</a>.</p><p>The boss is too smart for your tricks. Knowing that a <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_emperor\" target=\"_blank\" id=\"reader.external-link.num-15\">flexible</a>, complicated model allows you to score too well on your <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_mrbean\" target=\"_blank\" id=\"reader.external-link.num-16\">training set</a>, the boss changes the scoring function to penalize complexity. This is called <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_059\" target=\"_blank\" id=\"reader.external-link.num-17\">regularization</a>. (Frankly, I wish we had more regularization of engineers’ antics, to stop them from doing complicated things for complexity’s sake.)</p><figure></figure><p>Regularization essentially says, <em>“Each extra bit of complexity is going to cost you, so don’t do it unless it improves the fit by at least this amount…”</em></p><p>If the boss regularizes too much — getting tyrannical about simplicity — your performance review is going to go terribly unless you oversimplify the model, so that’s what you end up doing.</p><p>This is called <a rel=\"noopener noreferrer\" href=\"http://bit.ly/mfml_050\" target=\"_blank\" id=\"reader.external-link.num-18\">underfitting</a>. Such a model has an excellent training score (mostly because of all the simplicity bonuses it won) but a whopper of a bias in reality. That’s what you get for insisting that solutions should be simpler than your problem requires.</p><p>And with that, we’re ready for <a rel=\"noopener noreferrer\" href=\"http://bit.ly/quaesita_bivar3\" target=\"_blank\" id=\"reader.external-link.num-19\">Part 3</a>, where we bring it all together and cram the bias-variance tradeoff into a convenient nutshell for you.</p><h2>Thanks for reading! How about a YouTube course?</h2><p>If you had fun here and you’re looking for an entire applied AI course designed to be fun for beginners and experts alike, here’s the one I made for your amusement:</p><figure></figure><h2>Looking for hands-on ML/AI tutorials?</h2><p>Here are some of my favorite 10 minute walkthroughs:</p><ul><li><a rel=\"noopener noreferrer\" href=\"https://console.cloud.google.com/?walkthrough_id=automl_quickstart\" target=\"_blank\" id=\"reader.external-link.num-20\">AutoML</a></li><li><a rel=\"noopener noreferrer\" href=\"https://bit.ly/kozvertex\" target=\"_blank\" id=\"reader.external-link.num-21\">Vertex AI</a></li><li><a rel=\"noopener noreferrer\" href=\"https://bit.ly/kozvertexnotebooks\" target=\"_blank\" id=\"reader.external-link.num-22\">AI notebooks</a></li><li><a rel=\"noopener noreferrer\" href=\"https://bit.ly/kozvertextables\" target=\"_blank\" id=\"reader.external-link.num-23\">ML for tabular data</a></li><li><a rel=\"noopener noreferrer\" href=\"https://bit.ly/kozvertextext\" target=\"_blank\" id=\"reader.external-link.num-24\">Text classification</a></li><li><a rel=\"noopener noreferrer\" href=\"https://bit.ly/kozverteximage\" target=\"_blank\" id=\"reader.external-link.num-25\">Image classification</a></li><li><a rel=\"noopener noreferrer\" href=\"https://bit.ly/kozvertexvideo\" target=\"_blank\" id=\"reader.external-link.num-26\">Video classification</a></li></ul></div></article></div>",
      "textContent": "The bias-variance tradeoff, part 2 of 3In Part 1, we covered much of the basic terminology as well as a few key insights about the bias-variance formula (MSE = Bias² + Variance), including this misquote from Anna Karenina:All perfect models are alike, but each unhappy model can be unhappy in its own way.To make the most of this article, I suggest taking a look at Part 1 to make sure you’re well-situated to absorb this one.What does overfitting/underfitting have to do with it?Let’s say you have a model that is as good as you’re going to get for the information you have.To have an even better model, you need better data. In other words, more data (quantity) or more relevant data (quality).When I say as good as you’re going to get, I mean in “good” terms of MSE performance on data your model hasn’t seen before. (It’s supposed to predict, not postdict.) You’ve done a perfect job of getting what you can from the information you have — the rest is error you can’t do anything about with your information.Reality = Best Model + Unavoidable ErrorBut here’s the problem… we’ve jumped ahead; you don’t have this model yet.All you have is a pile of old data to learn this model from. Eventually, if you’re smart, you’ll validate this model on data it hasn’t seen before, but first you have to learn the model by finding useful patterns in data and trying to inch closer and closer to the stated objective: an MSE that’s as low as possible.Unfortunately, during the learning process, you don’t get to observe the MSE you’re after (the one that comes from reality). You only get to compute a shoddy version from your current training dataset.Photo by Jason Leung on UnsplashOh, and also, in this example “you” are not a human, you’re an optimization algorithm that was told by your human boss to twiddle the dials in the model’s settings until the MSE is as low as it will go.You say, “Sweet! I can do this!! Boss, if you give me an extremely flexible model with lots of settings to fiddle (neural networks, anyone?), I can give you a perfect training MSE. No bias and no variance.”The way to get a better training MSE than the true model’s test MSE is to fit all the noise (errors you have no predictively-useful information about) along with the signal. How do you achieve this little miracle? By making the model more complicated. Connecting the dots, essentially.This is called overfitting. Such a model has an excellent training MSE but a whopper of a variance when you try to use it for anything practical. That’s what you get for trying to cheat by creating a solution with more complexity than your information supports.The boss is too smart for your tricks. Knowing that a flexible, complicated model allows you to score too well on your training set, the boss changes the scoring function to penalize complexity. This is called regularization. (Frankly, I wish we had more regularization of engineers’ antics, to stop them from doing complicated things for complexity’s sake.)Regularization essentially says, “Each extra bit of complexity is going to cost you, so don’t do it unless it improves the fit by at least this amount…”If the boss regularizes too much — getting tyrannical about simplicity — your performance review is going to go terribly unless you oversimplify the model, so that’s what you end up doing.This is called underfitting. Such a model has an excellent training score (mostly because of all the simplicity bonuses it won) but a whopper of a bias in reality. That’s what you get for insisting that solutions should be simpler than your problem requires.And with that, we’re ready for Part 3, where we bring it all together and cram the bias-variance tradeoff into a convenient nutshell for you.Thanks for reading! How about a YouTube course?If you had fun here and you’re looking for an entire applied AI course designed to be fun for beginners and experts alike, here’s the one I made for your amusement:Looking for hands-on ML/AI tutorials?Here are some of my favorite 10 minute walkthroughs:AutoMLVertex AIAI notebooksML for tabular dataText classificationImage classificationVideo classification",
      "length": 4131,
      "excerpt": "In Part 1, we covered much of the basic terminology as well as a few key insights about the bias-variance formula (MSE = Bias² + Variance), including this misquote from Anna Karenina:",
      "siteName": null,
      "summary": "The bias-variance tradeoff is the tradeoff between overfitting and underfitting an AI model. To get a better training MSE than the true model’s test MSE, fit all the noise (errors you have no predictively-useful information about) along with the signal. How do you achieve this little miracle? By making the model more complicated.",
      "topics": [
        {
          "label": "Science & Mathematics",
          "score": 0.9940290451049805
        },
        {
          "label": "Computers & Internet",
          "score": 0.9822896122932434
        },
        {
          "label": "Education & Reference",
          "score": 0.8851271867752075
        }
      ]
    }
  },
  "3807343940": {
    "item_id": "3807343940",
    "resolved_id": "3807343940",
    "given_url": "https://huggingface.co/blog/blip-2",
    "given_title": "Zero-shot image-to-text generation with BLIP-2",
    "favorite": "0",
    "status": "0",
    "time_added": "1676633024",
    "time_updated": "1676633029",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 2,
    "resolved_title": "Zero-shot image-to-text generation with BLIP-2",
    "resolved_url": "https://huggingface.co/blog/blip-2",
    "excerpt": "This guide introduces BLIP-2 from Salesforce Research that enables a suite of state-of-the-art visual-language models that are now available in 🤗 Transformers. We'll show you how to use it for image captioning, prompted image captioning, visual question-answering, and chat-based prompting.",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "1",
    "word_count": "1534",
    "lang": "en",
    "time_to_read": 7,
    "top_image_url": "https://huggingface.co/blog/assets/blip-2/thumbnail.png",
    "tags": {
      "readinglist": {
        "item_id": "3807343940",
        "tag": "readinglist"
      }
    },
    "image": {
      "item_id": "3807343940",
      "src": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/blip-2/q-former-1.png",
      "width": "500",
      "height": "0"
    },
    "images": {
      "1": {
        "item_id": "3807343940",
        "image_id": "1",
        "src": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/blip-2/q-former-1.png",
        "width": "500",
        "height": "0",
        "credit": "",
        "caption": ""
      },
      "2": {
        "item_id": "3807343940",
        "image_id": "2",
        "src": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/blip-2/q-former-2.png",
        "width": "500",
        "height": "0",
        "credit": "",
        "caption": ""
      },
      "3": {
        "item_id": "3807343940",
        "image_id": "3",
        "src": "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/blip-2/cartoon.jpeg",
        "width": "500",
        "height": "0",
        "credit": "",
        "caption": ""
      }
    },
    "listen_duration_estimate": 594,
    "content": {
      "title": "Zero-shot image-to-text generation with BLIP-2",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><div lang=\"en\">\n\n\n<p>This guide introduces <a href=\"https://huggingface.co/docs/transformers/main/en/model_doc/blip-2\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-0\">BLIP-2</a> from Salesforce Research that enables a suite of state-of-the-art visual-language models that are now available in <a href=\"https://huggingface.co/transformers\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-1\">🤗 Transformers</a>. We'll show you how to use it for image captioning, prompted image captioning, visual question-answering, and chat-based prompting.</p>\n<h2><a href=\"https://huggingface.co/blog/blip-2#table-of-contents\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-2\"><span></span></a> <span>Table of contents</span></h2>\n<h2><a href=\"https://huggingface.co/blog/blip-2#introduction\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-3\"><span></span></a> <span>Introduction</span></h2>\n<p>Recent years have seen rapid advancements in computer vision and natural language processing. Still, many real-world problems are inherently multimodal - they involve several distinct forms of data, such as images and text. Visual-language models face the challenge of combining modalities so that they can open the door to a wide range of applications. Some of the image-to-text tasks that visual language models can tackle include image captioning, image-text retrieval, and visual question answering. Image captioning can aid the visually impaired, create useful product descriptions, identify inappropriate content beyond text, and more. Image-text retrieval can be applied in multimodal search, as well as in applications such as autonomous driving. Visual question-answering can aid in education, enable multimodal chatbots, and assist in various domain-specific information retrieval applications.</p>\n<p>Modern computer vision and natural language models have become more capable; however, they have also significantly grown in size compared to their predecessors. While pre-training a single-modality model is resource-consuming and expensive, the cost of end-to-end vision-and-language pre-training has become increasingly prohibitive. <a href=\"https://arxiv.org/pdf/2301.12597.pdf\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-4\">BLIP-2</a> tackles this challenge by introducing a new visual-language pre-training paradigm that can potentially leverage any combination of pre-trained vision encoder and LLM without having to pre-train the whole architecture end to end. This enables achieving state-of-the-art results on multiple visual-language tasks while significantly reducing the number of trainable parameters and pre-training costs. Moreover, this approach paves the way for a multimodal ChatGPT-like model.</p>\n<h2><a href=\"https://huggingface.co/blog/blip-2#whats-under-the-hood-in-blip-2\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-5\"><span></span></a> <span>What's under the hood in BLIP-2?</span></h2>\n<p>BLIP-2 bridges the modality gap between vision and language models by adding a lightweight Querying Transformer (Q-Former) between an off-the-shelf frozen pre-trained image encoder and a frozen large language model. Q-Former is the only trainable part of BLIP-2; both the image encoder and language model remain frozen.</p>\n\n<p>Q-Former is a transformer model that consists of two submodules that share the same self-attention layers:</p>\n<ul><li>an image transformer that interacts with the frozen image encoder for visual feature extraction</li>\n<li>a text transformer that can function as both a text encoder and a text decoder</li>\n</ul>\n<p>The image transformer extracts a fixed number of output features from the image encoder, independent of input image resolution, and receives learnable query embeddings as input. The queries can additionally interact with the text through the same self-attention layers.</p>\n<p>Q-Former is pre-trained in two stages. In the first stage, the image encoder is frozen, and Q-Former is trained with three losses:</p>\n<ul><li>Image-text contrastive loss: pairwise similarity between each query output and text output's CLS token is calculated, and the highest one is picked. Query embeddings and text don't “see” each other.</li>\n<li>Image-grounded text generation: queries can attend to each other but not to the text tokens, and text has a causal mask and can attend to all of the queries.</li>\n<li>Image-text matching loss: queries and text can see others, and a logit is obtained to indicate whether the text matches the image or not. To obtain negative examples, hard negative mining is used.</li>\n</ul><p>In the second pre-training stage, the query embeddings now have the relevant visual information to the text as it has passed through an information bottleneck. These embeddings are now used as a visual prefix to the input to the LLM. This pre-training phase effectively involves an image-ground text generation task using the causal LM loss.</p>\n<p>As a visual encoder, BLIP-2 uses ViT, and for an LLM, the paper authors used OPT and Flan T5 models. You can find pre-trained checkpoints for both OPT and Flan T5 on <a href=\"https://huggingface.co/models?other=blip-2\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-6\">Hugging Face Hub</a>. However, as mentioned before, the introduced pre-training approach allows combining any visual backbone with any LLM.</p>\n<h2><a href=\"https://huggingface.co/blog/blip-2#using-blip-2-with-hugging-face-transformers\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-7\"><span></span></a> <span>Using BLIP-2 with Hugging Face Transformers</span></h2>\n<p>Using Hugging Face Transformers, you can easily download and run a pre-trained BLIP-2 model on your images. Make sure to use a GPU environment with high RAM if you'd like to follow along with the examples in this blog post.</p>\n<p>Let's start by installing Transformers. As this model has been added to Transformers very recently, we need to install Transformers from the source:</p>\n<pre><code>pip install git+https://github.com/huggingface/transformers.git\n</code></pre>\n<p>Next, we'll need an input image. Every week The New Yorker runs a <a href=\"https://www.newyorker.com/cartoons/contest#thisweek\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-8\">cartoon captioning contest</a> among its readers, so let's take one of these cartoons to put BLIP-2 to the test.</p>\n<pre><code>import requests\nfrom PIL import Image\n\nurl = 'https://media.newyorker.com/cartoons/63dc6847be24a6a76d90eb99/master/w_1160,c_limit/230213_a26611_838.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')  \ndisplay(image.resize((596, 437)))\n</code></pre>\n\n<p>We have an input image. Now we need a pre-trained BLIP-2 model and corresponding preprocessor to prepare the inputs. You can find the list of all available pre-trained checkpoints on <a href=\"https://huggingface.co/models?other=blip-2\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-9\">Hugging Face Hub</a>. Here, we'll load a BLIP-2 checkpoint that leverages the pre-trained OPT model by Meta AI, which has 2.7 billion parameters.</p>\n<pre><code>from transformers import AutoProcessor, Blip2ForConditionalGeneration\nimport torch\n\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n</code></pre>\n<p>Notice that BLIP-2 is a rare case where you cannot load the model with Auto API (e.g. AutoModelForXXX), and you need to explicitly use <code>Blip2ForConditionalGeneration</code>. However, you can use <code>AutoProcessor</code> to fetch the appropriate processor class - <code>Blip2Processor</code> in this case.</p>\n<p>Let's use GPU to make text generation faster:</p>\n<pre><code>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n</code></pre>\n<h3><a href=\"https://huggingface.co/blog/blip-2#image-captioning\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-10\"><span></span></a> <span>Image Captioning</span></h3>\n<p>Let's find out if BLIP-2 can caption a New Yorker cartoon in a zero-shot manner. To caption an image, we do not have to provide any text prompt to the model, only the preprocessed input image. Without any text prompt, the model will start generating text from the BOS (beginning-of-sequence) token thus creating a caption.</p>\n<pre><code>inputs = processor(image, return_tensors=\"pt\")\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=20)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n</code></pre>\n<pre><code>\"two cartoon monsters sitting around a campfire\"\n</code></pre>\n<p>This is an impressively accurate description for a model that wasn't trained on New Yorker style cartoons!</p>\n<h3><a href=\"https://huggingface.co/blog/blip-2#prompted-image-captioning\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-11\"><span></span></a> <span>Prompted image captioning</span></h3>\n<p>We can extend image captioning by providing a text prompt, which the model will continue given the image.</p>\n<pre><code>prompt = \"this is a cartoon of\"\n\ninputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=20)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n</code></pre>\n<pre><code>\"two monsters sitting around a campfire\"\n</code></pre>\n<pre><code>prompt = \"they look like they are\"\n\ninputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=20)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n</code></pre>\n<pre><code>\"having a good time\"\n</code></pre>\n<h3><a href=\"https://huggingface.co/blog/blip-2#visual-question-answering\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-12\"><span></span></a> <span>Visual question answering</span></h3>\n<p>For visual question answering the prompt has to follow a specific format: \"Question: {} Answer:\"</p>\n<pre><code>prompt = \"Question: What is a dinosaur holding? Answer:\"\n\ninputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=10)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n</code></pre>\n<pre><code>\"A torch\"\n</code></pre>\n<h3><a href=\"https://huggingface.co/blog/blip-2#chat-based-prompting\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-13\"><span></span></a> <span>Chat-based prompting</span></h3>\n<p>Finally, we can create a ChatGPT-like interface by concatenating each generated response to the conversation. We prompt the model with some text (like \"What is a dinosaur holding?\"), the model generates an answer for it \"a torch\"), which we can concatenate to the conversation. Then we do it again, building up the context. However, make sure that the context does not exceed 512 tokens, as this is the context length of the language models used by BLIP-2 (OPT and T5).</p>\n<pre><code>context = [\n   (\"What is a dinosaur holding?\", \"a torch\"),\n   (\"Where are they?\", \"In the woods.\")\n]\nquestion = \"What for?\"\ntemplate = \"Question: {} Answer: {}.\"\n\nprompt = \" \".join([template.format(context[i][0], context[i][1]) for i in range(len(context))]) + \" Question: \" + question + \" Answer:\"\n\nprint(prompt)\n</code></pre>\n<pre><code>Question: What is a dinosaur holding? Answer: a torch. Question: Where are they? Answer: In the woods.. Question: What for? Answer:\n</code></pre>\n<pre><code>inputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=10)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n</code></pre>\n<pre><code>To light a fire.\n</code></pre>\n<h2><a href=\"https://huggingface.co/blog/blip-2#conclusion\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-14\"><span></span></a> <span>Conclusion</span></h2>\n<p>BLIP-2 is a zero-shot visual-language model that can be used for multiple image-to-text tasks with image and image and text prompts. It is an effective and efficient approach that can be applied to image understanding in numerous scenarios, especially when examples are scarce.</p>\n<p>The model bridges the gap between vision and natural language modalities by adding a transformer between pre-trained models. The new pre-training paradigm allows this model to keep up with the advances in both individual modalities.</p>\n<p>If you'd like to learn how to fine-tune BLIP-2 models for various vision-language tasks, check out <a href=\"https://github.com/salesforce/LAVIS\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-15\">LAVIS library by Salesforce</a> that offers comprehensive support for model training.</p>\n<p>To see BLIP-2 in action, try its demo on <a href=\"https://huggingface.co/spaces/Salesforce/BLIP2\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-16\">Hugging Face Spaces</a>.</p>\n<h2><a href=\"https://huggingface.co/blog/blip-2#acknowledgments\" target=\"_blank\" rel=\"noopener noreferrer\" id=\"reader.external-link.num-17\"><span></span></a> <span>Acknowledgments</span></h2>\n<p>Many thanks to the Salesforce Research team for working on BLIP-2, Niels Rogge for adding BLIP-2 to 🤗 Transformers, and to Omar Sanseviero for reviewing this blog post.</p>\n</div></article></div>",
      "textContent": "\n\n\nThis guide introduces BLIP-2 from Salesforce Research that enables a suite of state-of-the-art visual-language models that are now available in 🤗 Transformers. We'll show you how to use it for image captioning, prompted image captioning, visual question-answering, and chat-based prompting.\n Table of contents\n Introduction\nRecent years have seen rapid advancements in computer vision and natural language processing. Still, many real-world problems are inherently multimodal - they involve several distinct forms of data, such as images and text. Visual-language models face the challenge of combining modalities so that they can open the door to a wide range of applications. Some of the image-to-text tasks that visual language models can tackle include image captioning, image-text retrieval, and visual question answering. Image captioning can aid the visually impaired, create useful product descriptions, identify inappropriate content beyond text, and more. Image-text retrieval can be applied in multimodal search, as well as in applications such as autonomous driving. Visual question-answering can aid in education, enable multimodal chatbots, and assist in various domain-specific information retrieval applications.\nModern computer vision and natural language models have become more capable; however, they have also significantly grown in size compared to their predecessors. While pre-training a single-modality model is resource-consuming and expensive, the cost of end-to-end vision-and-language pre-training has become increasingly prohibitive. BLIP-2 tackles this challenge by introducing a new visual-language pre-training paradigm that can potentially leverage any combination of pre-trained vision encoder and LLM without having to pre-train the whole architecture end to end. This enables achieving state-of-the-art results on multiple visual-language tasks while significantly reducing the number of trainable parameters and pre-training costs. Moreover, this approach paves the way for a multimodal ChatGPT-like model.\n What's under the hood in BLIP-2?\nBLIP-2 bridges the modality gap between vision and language models by adding a lightweight Querying Transformer (Q-Former) between an off-the-shelf frozen pre-trained image encoder and a frozen large language model. Q-Former is the only trainable part of BLIP-2; both the image encoder and language model remain frozen.\n\nQ-Former is a transformer model that consists of two submodules that share the same self-attention layers:\nan image transformer that interacts with the frozen image encoder for visual feature extraction\na text transformer that can function as both a text encoder and a text decoder\n\nThe image transformer extracts a fixed number of output features from the image encoder, independent of input image resolution, and receives learnable query embeddings as input. The queries can additionally interact with the text through the same self-attention layers.\nQ-Former is pre-trained in two stages. In the first stage, the image encoder is frozen, and Q-Former is trained with three losses:\nImage-text contrastive loss: pairwise similarity between each query output and text output's CLS token is calculated, and the highest one is picked. Query embeddings and text don't “see” each other.\nImage-grounded text generation: queries can attend to each other but not to the text tokens, and text has a causal mask and can attend to all of the queries.\nImage-text matching loss: queries and text can see others, and a logit is obtained to indicate whether the text matches the image or not. To obtain negative examples, hard negative mining is used.\nIn the second pre-training stage, the query embeddings now have the relevant visual information to the text as it has passed through an information bottleneck. These embeddings are now used as a visual prefix to the input to the LLM. This pre-training phase effectively involves an image-ground text generation task using the causal LM loss.\nAs a visual encoder, BLIP-2 uses ViT, and for an LLM, the paper authors used OPT and Flan T5 models. You can find pre-trained checkpoints for both OPT and Flan T5 on Hugging Face Hub. However, as mentioned before, the introduced pre-training approach allows combining any visual backbone with any LLM.\n Using BLIP-2 with Hugging Face Transformers\nUsing Hugging Face Transformers, you can easily download and run a pre-trained BLIP-2 model on your images. Make sure to use a GPU environment with high RAM if you'd like to follow along with the examples in this blog post.\nLet's start by installing Transformers. As this model has been added to Transformers very recently, we need to install Transformers from the source:\npip install git+https://github.com/huggingface/transformers.git\n\nNext, we'll need an input image. Every week The New Yorker runs a cartoon captioning contest among its readers, so let's take one of these cartoons to put BLIP-2 to the test.\nimport requests\nfrom PIL import Image\n\nurl = 'https://media.newyorker.com/cartoons/63dc6847be24a6a76d90eb99/master/w_1160,c_limit/230213_a26611_838.jpg'\nimage = Image.open(requests.get(url, stream=True).raw).convert('RGB')  \ndisplay(image.resize((596, 437)))\n\n\nWe have an input image. Now we need a pre-trained BLIP-2 model and corresponding preprocessor to prepare the inputs. You can find the list of all available pre-trained checkpoints on Hugging Face Hub. Here, we'll load a BLIP-2 checkpoint that leverages the pre-trained OPT model by Meta AI, which has 2.7 billion parameters.\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\nimport torch\n\nprocessor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n\nNotice that BLIP-2 is a rare case where you cannot load the model with Auto API (e.g. AutoModelForXXX), and you need to explicitly use Blip2ForConditionalGeneration. However, you can use AutoProcessor to fetch the appropriate processor class - Blip2Processor in this case.\nLet's use GPU to make text generation faster:\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(device)\n\n Image Captioning\nLet's find out if BLIP-2 can caption a New Yorker cartoon in a zero-shot manner. To caption an image, we do not have to provide any text prompt to the model, only the preprocessed input image. Without any text prompt, the model will start generating text from the BOS (beginning-of-sequence) token thus creating a caption.\ninputs = processor(image, return_tensors=\"pt\")\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=20)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n\n\"two cartoon monsters sitting around a campfire\"\n\nThis is an impressively accurate description for a model that wasn't trained on New Yorker style cartoons!\n Prompted image captioning\nWe can extend image captioning by providing a text prompt, which the model will continue given the image.\nprompt = \"this is a cartoon of\"\n\ninputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=20)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n\n\"two monsters sitting around a campfire\"\n\nprompt = \"they look like they are\"\n\ninputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=20)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n\n\"having a good time\"\n\n Visual question answering\nFor visual question answering the prompt has to follow a specific format: \"Question: {} Answer:\"\nprompt = \"Question: What is a dinosaur holding? Answer:\"\n\ninputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=10)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n\n\"A torch\"\n\n Chat-based prompting\nFinally, we can create a ChatGPT-like interface by concatenating each generated response to the conversation. We prompt the model with some text (like \"What is a dinosaur holding?\"), the model generates an answer for it \"a torch\"), which we can concatenate to the conversation. Then we do it again, building up the context. However, make sure that the context does not exceed 512 tokens, as this is the context length of the language models used by BLIP-2 (OPT and T5).\ncontext = [\n   (\"What is a dinosaur holding?\", \"a torch\"),\n   (\"Where are they?\", \"In the woods.\")\n]\nquestion = \"What for?\"\ntemplate = \"Question: {} Answer: {}.\"\n\nprompt = \" \".join([template.format(context[i][0], context[i][1]) for i in range(len(context))]) + \" Question: \" + question + \" Answer:\"\n\nprint(prompt)\n\nQuestion: What is a dinosaur holding? Answer: a torch. Question: Where are they? Answer: In the woods.. Question: What for? Answer:\n\ninputs = processor(image, text=prompt, return_tensors=\"pt\").to(device, torch.float16)\n\ngenerated_ids = model.generate(**inputs, max_new_tokens=10)\ngenerated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\nprint(generated_text)\n\nTo light a fire.\n\n Conclusion\nBLIP-2 is a zero-shot visual-language model that can be used for multiple image-to-text tasks with image and image and text prompts. It is an effective and efficient approach that can be applied to image understanding in numerous scenarios, especially when examples are scarce.\nThe model bridges the gap between vision and natural language modalities by adding a transformer between pre-trained models. The new pre-training paradigm allows this model to keep up with the advances in both individual modalities.\nIf you'd like to learn how to fine-tune BLIP-2 models for various vision-language tasks, check out LAVIS library by Salesforce that offers comprehensive support for model training.\nTo see BLIP-2 in action, try its demo on Hugging Face Spaces.\n Acknowledgments\nMany thanks to the Salesforce Research team for working on BLIP-2, Niels Rogge for adding BLIP-2 to 🤗 Transformers, and to Omar Sanseviero for reviewing this blog post.\n",
      "length": 10327,
      "excerpt": "This guide introduces BLIP-2 from Salesforce Research that enables a suite of state-of-the-art visual-language models that are now available in 🤗 Transformers. We'll show you how to use it for image captioning, prompted image captioning, visual question-answering, and chat-based prompting.",
      "siteName": null,
      "summary": "BLIP-2 enables a suite of state-of-the-art visual-language models that are now available in Transformers. We'll show you how to use it for image captioning, prompted image Captioning, visual question-answering, and chat-based prompting. You can find pre-trained models for OPT and Flan5 on Hugging Face Hub.",
      "topics": [
        {
          "label": "Science & Mathematics",
          "score": 0.9802359342575073
        },
        {
          "label": "Computers & Internet",
          "score": 0.8679908514022827
        },
        {
          "label": "Society & Culture",
          "score": 0.5431674718856812
        }
      ]
    }
  },
  "3807997588": {
    "item_id": "3807997588",
    "resolved_id": "3807997588",
    "given_url": "https://www.philschmid.de/fine-tune-flan-t5-deepspeed",
    "given_title": "Fine-tune FLAN-T5 XL/XXL using DeepSpeed & Hugging Face Transformers",
    "favorite": "0",
    "status": "0",
    "time_added": "1676633053",
    "time_updated": "1676633056",
    "time_read": "0",
    "time_favorited": "0",
    "sort_id": 0,
    "resolved_title": "Fine-tune FLAN-T5 XL/XXL using DeepSpeed & Hugging Face Transformers",
    "resolved_url": "https://www.philschmid.de/fine-tune-flan-t5-deepspeed",
    "excerpt": "FLAN-T5, released with the Scaling Instruction-Finetuned Language Models paper, is an enhanced version of T5 that has been fine-tuned in a mixture of tasks, or simple words, a better T5 model in any aspect. FLAN-T5 outperforms T5 by double-digit improvements for the same number of parameters.",
    "is_article": "1",
    "is_index": "0",
    "has_video": "0",
    "has_image": "0",
    "word_count": "1948",
    "lang": "en",
    "time_to_read": 9,
    "top_image_url": "https://www.philschmid.de/static/blog/fine-tune-flan-t5-deepspeed/thumbnail.png",
    "tags": {
      "readinglist": {
        "item_id": "3807997588",
        "tag": "readinglist"
      }
    },
    "listen_duration_estimate": 754,
    "content": {
      "title": "",
      "byline": null,
      "dir": null,
      "lang": null,
      "content": "<div id=\"readability-page-1\" class=\"page\"><article><div lang=\"en\">\n<p>FLAN-T5, released with the <a href=\"https://arxiv.org/pdf/2210.11416.pdf\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-0\">Scaling Instruction-Finetuned Language Models</a> paper, is an enhanced version of T5 that has been fine-tuned in a mixture of tasks, or simple words, a better T5 model in any aspect. FLAN-T5 outperforms T5 by double-digit improvements for the same number of parameters. Google has open sourced <a href=\"https://huggingface.co/models?other=arxiv:2210.11416\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-1\">5 checkpoints available on Hugging Face</a> ranging from 80M parameter up to 11B parameter.</p>\n<p>In a previous blog post, we already learned how to <a href=\"https://www.philschmid.de/fine-tune-flan-t5\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-2\">“Fine-tune FLAN-T5 for chat &amp; dialogue summarization”</a> using <a href=\"https://huggingface.co/google/flan-t5-base\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-3\">the base version (250M parameter)</a> of the model. In this blog post, we look into how we can scale the training from the Base version to the <a href=\"https://huggingface.co/google/flan-t5-xl\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-4\">XL (3B)</a> or <a href=\"https://huggingface.co/google/flan-t5-xxl\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-5\">XXL (11B)</a>.</p>\n<p>This means we will learn how to fine-tune FLAN-T5 XL &amp; XXL using model parallelism, multiple GPUs, and <a href=\"https://www.deepspeed.ai/tutorials/zero/\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-6\">DeepSpeed ZeRO</a>.</p>\n<p>You will learn about the following:</p>\n<p>in addition to the tutorial, we have run a series of experiments to help you choose the right hardware setup. You can find the details in the Results &amp; Experiments section.</p>\n<p>Let's get started! 🚀</p>\n<h2>1. What is DeepSpeed ZeRO?</h2>\n<p><a href=\"https://arxiv.org/abs/2101.06840\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-8\">DeepSpeed ZeRO</a> is part of the <a href=\"https://www.deepspeed.ai/training/\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-9\">DeepSpeed Training Pillar</a>, which focus on efficient large-scale Training of Transformer models. DeepSpeed ZeRO or Zero Redundancy Optimizer is a method to reduce the memory footprint. Compared to basic data parallelism, ZeRO partitions optimizer states, gradients, and model parameters to save significant memory across multiple devices.</p>\n\n<p>If you want to learn more about DeepSpeed ZeRO, checkout: <a href=\"https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-10\">ZeRO &amp; DeepSpeed: New system optimizations enable training models with over 100 billion parameters</a></p>\n<p>DeepSpeed ZeRO is natively integrated into the <a href=\"https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/deepspeed\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-11\">Hugging Face Transformers Trainer</a>. The integration enables leveraging ZeRO by simply providing a DeepSpeed config file, and the Trainer takes care of the rest.</p>\n<p>Excerpt: <a href=\"https://www.deepspeed.ai/tutorials/zero-offload/\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-12\">DeepSpeed ZeRO-offload</a></p>\n<p>DeepSpeed ZeRO not only allows us to parallelize our models on multiple GPUs, it also implements Offloading. <a href=\"https://arxiv.org/abs/2101.06840\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-13\">ZeRO-Offload</a> implements optimizations that offload optimizer and model to the CPU to train larger models on the given GPUs, e.g. <a href=\"https://www.deepspeed.ai/tutorials/zero-offload/#training-a-10b-parameter-gpt-2-on-a-single-v100-gpu\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-14\">10B parameter GPT-2 on a single V100 GPU.</a> We used ZeRO-offload for the experiments but will not use it in the tutorial.</p>\n<h2>2. Fine-tune FLAN-T5-XXL using Deepspeed</h2>\n<p>We now know that we can use DeepSpeed ZeRO together with Hugging Face Transformers to easily scale our hardware in cases where the model no longer fits on GPU. That's exactly what we need to solve since the FLAN-T5-XXL weights in fp32 are already 44GB big. This makes it almost impossible to fit on a single GPU when adding activations and optimizer states.</p>\n<p>In this tutorial, we cover how to fine-tune <a href=\"https://huggingface.co/google/flan-t5-xxl\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-16\">FLAN-T5-XXL</a> (11B version) on the <a href=\"https://huggingface.co/datasets/cnn_dailymail\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-17\">CNN Dailymail Dataset</a> for news summarization. The provided script and pre-processing can easily be adjusted to fine-tune FLAN-T5-XL and use a different dataset.</p>\n<p><em>Note: This tutorial was created and run on a p4dn.24xlarge AWS EC2 Instance including 8x NVIDIA A100 40GB.</em></p>\n<h3>Setup Development Environment</h3>\n<p>The first step is to install the Hugging Face Libraries, including transformers and datasets, and DeepSeed. Running the following cell will install all the required packages.</p>\n<div>\n<pre><code><span><span># install torch with the correct cuda version, check nvcc --version</span>\n</span><span>pip <span>install</span> torch --extra-index-url https://download.pytorch.org/whl/cu116 --upgrade\n</span><span><span># install Hugging Face Libraries</span>\n</span><span>pip <span>install</span> <span>\"transformers==4.26.0\"</span> <span>\"datasets==2.9.0\"</span> <span>\"accelerate==0.16.0\"</span> <span>\"evaluate==0.4.0\"</span> --upgrade\n</span><span><span># install deepspeed and ninja for jit compilations of kernels</span>\n</span><span>pip <span>install</span> <span>\"deepspeed==0.8.0\"</span> ninja --upgrade\n</span><span><span># install additional dependencies needed for training</span>\n</span><span>pip <span>install</span> rouge-score nltk py7zr tensorboard\n</span></code></pre></div>\n<h3>Load and prepare dataset</h3>\n<p>Similar to the <a href=\"https://www.philschmid.de/fine-tune-flan-t5\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-20\">“Fine-tune FLAN-T5 for chat &amp; dialogue summarization”</a> we need to prepare a dataset to fine-tune our model. As mentioned in the beginning, we will fine-tune <a href=\"https://huggingface.co/google/flan-t5-xxl\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-21\">FLAN-T5-XXL</a> on the <a href=\"https://huggingface.co/datasets/cnn_dailymail\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-22\">CNN Dailymail Dataset</a>. The blog post is not going into detail about the dataset generation. If you want to learn the detailed steps check out the <a href=\"https://www.philschmid.de/fine-tune-flan-t5\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-23\">previous post</a>.</p>\n<p>We define some parameters, which we use throughout the whole example, feel free to adjust it to your needs.</p>\n<div>\n<pre><code><span><span># experiment config</span>\n</span><span>model_id <span>=</span> <span>\"google/flan-t5-xxl\"</span> <span># Hugging Face Model Id</span>\n</span><span>dataset_id <span>=</span> <span>\"cnn_dailymail\"</span> <span># Hugging Face Dataset Id</span>\n</span><span>dataset_config <span>=</span> <span>\"3.0.0\"</span> <span># config/verison of the dataset</span>\n</span><span>save_dataset_path <span>=</span> <span>\"data\"</span> <span># local path to save processed dataset</span>\n</span><span>text_column <span>=</span> <span>\"article\"</span> <span># column of input text is</span>\n</span><span>summary_column <span>=</span> <span>\"highlights\"</span> <span># column of the output text</span>\n</span><span><span># custom instruct prompt start</span>\n</span><span>prompt_template <span>=</span> <span>f\"Summarize the following news article:\\n{{input}}\\nSummary:\\n\"</span>\n</span></code></pre></div>\n<p>Compared to the <a href=\"https://www.philschmid.de/fine-tune-flan-t5\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-24\">previous example</a>, we are splitting the processing and training into two separate paths. This allows you to run the preprocessing outside of the GPU instance. We process (tokenize) the dataset and save it to disk and then load in our train script from disk again.</p>\n<div>\n<pre><code><span><span>from</span> datasets <span>import</span> load_dataset\n</span><span><span>from</span> transformers <span>import</span> AutoTokenizer\n</span><span><span>import</span> numpy <span>as</span> np\n</span><span>\n</span><span><span># Load dataset from the hub</span>\n</span><span>dataset <span>=</span> load_dataset<span>(</span>dataset_id<span>,</span>name<span>=</span>dataset_config<span>)</span>\n</span><span><span># Load tokenizer of FLAN-t5-base</span>\n</span><span>tokenizer <span>=</span> AutoTokenizer<span>.</span>from_pretrained<span>(</span>model_id<span>)</span>\n</span><span>\n</span><span><span>print</span><span>(</span><span><span>f\"Train dataset size: </span><span><span>{</span><span>len</span><span>(</span>dataset<span>[</span><span>'train'</span><span>]</span><span>)</span><span>}</span></span><span>\"</span></span><span>)</span>\n</span><span><span>print</span><span>(</span><span><span>f\"Test dataset size: </span><span><span>{</span><span>len</span><span>(</span>dataset<span>[</span><span>'test'</span><span>]</span><span>)</span><span>}</span></span><span>\"</span></span><span>)</span>\n</span><span>\n</span><span><span># Train dataset size: 287113</span>\n</span><span><span># Test dataset size: 11490</span>\n</span></code></pre></div>\n<p>We defined a <code>prompt_template</code> in our config, which we will use to construct an instruct prompt for better performance of our model. Our <code>prompt_template</code> has a “fixed” start and end, and our document is in the middle. This means we need to ensure that the “fixed” template parts + document are not exceeding the max length of the model. Therefore we calculate the max length of our document, which we will later use for padding and truncation</p>\n<div>\n<pre><code><span>prompt_lenght <span>=</span> <span>len</span><span>(</span>tokenizer<span>(</span>prompt_template<span>.</span><span>format</span><span>(</span><span>input</span><span>=</span><span>\"\"</span><span>)</span><span>)</span><span>[</span><span>\"input_ids\"</span><span>]</span><span>)</span>\n</span><span>max_sample_length <span>=</span> tokenizer<span>.</span>model_max_length <span>-</span> prompt_lenght\n</span><span><span>print</span><span>(</span><span><span>f\"Prompt length: </span><span><span>{</span>prompt_lenght<span>}</span></span><span>\"</span></span><span>)</span>\n</span><span><span>print</span><span>(</span><span><span>f\"Max input length: </span><span><span>{</span>max_sample_length<span>}</span></span><span>\"</span></span><span>)</span>\n</span><span>\n</span><span><span># Prompt length: 12</span>\n</span><span><span># Max input length: 500</span>\n</span></code></pre></div>\n<p>We know now that our documents can be “500” tokens long to fit our <code>template_prompt</code> still correctly. In addition to our input, we need to understand better our “target” sequence length meaning and how long are the summarization ins our dataset. Therefore we iterate over the dataset and calculate the max input length (at max 500) and the max target length. (takes a few minutes)</p>\n<div>\n<pre><code><span><span>from</span> datasets <span>import</span> concatenate_datasets\n</span><span><span>import</span> numpy <span>as</span> np\n</span><span>\n</span><span><span># The maximum total input sequence length after tokenization.</span>\n</span><span><span># Sequences longer than this will be truncated, sequences shorter will be padded.</span>\n</span><span>tokenized_inputs <span>=</span> concatenate_datasets<span>(</span><span>[</span>dataset<span>[</span><span>\"train\"</span><span>]</span><span>,</span> dataset<span>[</span><span>\"test\"</span><span>]</span><span>]</span><span>)</span><span>.</span><span>map</span><span>(</span><span>lambda</span> x<span>:</span> tokenizer<span>(</span>x<span>[</span>text_column<span>]</span><span>,</span> truncation<span>=</span><span>True</span><span>)</span><span>,</span> batched<span>=</span><span>True</span><span>,</span> remove_columns<span>=</span><span>[</span>text_column<span>,</span> summary_column<span>]</span><span>)</span>\n</span><span>max_source_length <span>=</span> <span>max</span><span>(</span><span>[</span><span>len</span><span>(</span>x<span>)</span> <span>for</span> x <span>in</span> tokenized_inputs<span>[</span><span>\"input_ids\"</span><span>]</span><span>]</span><span>)</span>\n</span><span>max_source_length <span>=</span> <span>min</span><span>(</span>max_source_length<span>,</span> max_sample_length<span>)</span>\n</span><span><span>print</span><span>(</span><span><span>f\"Max source length: </span><span><span>{</span>max_source_length<span>}</span></span><span>\"</span></span><span>)</span>\n</span><span>\n</span><span><span># The maximum total sequence length for target text after tokenization.</span>\n</span><span><span># Sequences longer than this will be truncated, sequences shorter will be padded.\"</span>\n</span><span>tokenized_targets <span>=</span> concatenate_datasets<span>(</span><span>[</span>dataset<span>[</span><span>\"train\"</span><span>]</span><span>,</span> dataset<span>[</span><span>\"test\"</span><span>]</span><span>]</span><span>)</span><span>.</span><span>map</span><span>(</span><span>lambda</span> x<span>:</span> tokenizer<span>(</span>x<span>[</span>summary_column<span>]</span><span>,</span> truncation<span>=</span><span>True</span><span>)</span><span>,</span> batched<span>=</span><span>True</span><span>,</span> remove_columns<span>=</span><span>[</span>text_column<span>,</span> summary_column<span>]</span><span>)</span>\n</span><span>target_lenghts <span>=</span> <span>[</span><span>len</span><span>(</span>x<span>)</span> <span>for</span> x <span>in</span> tokenized_targets<span>[</span><span>\"input_ids\"</span><span>]</span><span>]</span>\n</span><span><span># use 95th percentile as max target length</span>\n</span><span>max_target_length <span>=</span> <span>int</span><span>(</span>np<span>.</span>percentile<span>(</span>target_lenghts<span>,</span> <span>95</span><span>)</span><span>)</span>\n</span><span><span>print</span><span>(</span><span><span>f\"Max target length: </span><span><span>{</span>max_target_length<span>}</span></span><span>\"</span></span><span>)</span>\n</span></code></pre></div>\n<p>We now have everything needed to process our dataset.</p>\n<div>\n<pre><code><span><span>import</span> os\n</span><span>\n</span><span><span>def</span> <span>preprocess_function</span><span>(</span>sample<span>,</span> padding<span>=</span><span>\"max_length\"</span><span>)</span><span>:</span>\n</span><span>    <span># created prompted input</span>\n</span><span>    inputs <span>=</span> <span>[</span>prompt_template<span>.</span><span>format</span><span>(</span><span>input</span><span>=</span>item<span>)</span> <span>for</span> item <span>in</span> sample<span>[</span>text_column<span>]</span><span>]</span>\n</span><span>\n</span><span>    <span># tokenize inputs</span>\n</span><span>    model_inputs <span>=</span> tokenizer<span>(</span>inputs<span>,</span> max_length<span>=</span>max_source_length<span>,</span> padding<span>=</span>padding<span>,</span> truncation<span>=</span><span>True</span><span>)</span>\n</span><span>\n</span><span>    <span># Tokenize targets with the `text_target` keyword argument</span>\n</span><span>    labels <span>=</span> tokenizer<span>(</span>text_target<span>=</span>sample<span>[</span>summary_column<span>]</span><span>,</span> max_length<span>=</span>max_target_length<span>,</span> padding<span>=</span>padding<span>,</span> truncation<span>=</span><span>True</span><span>)</span>\n</span><span>\n</span><span>    <span># padding in the loss.</span>\n</span><span>    <span>if</span> padding <span>==</span> <span>\"max_length\"</span><span>:</span>\n</span><span>        labels<span>[</span><span>\"input_ids\"</span><span>]</span> <span>=</span> <span>[</span>\n</span><span>            <span>[</span><span>(</span>l <span>if</span> l <span>!=</span> tokenizer<span>.</span>pad_token_id <span>else</span> <span>-</span><span>100</span><span>)</span> <span>for</span> l <span>in</span> label<span>]</span> <span>for</span> label <span>in</span> labels<span>[</span><span>\"input_ids\"</span><span>]</span>\n</span><span>        <span>]</span>\n</span><span>\n</span><span>    model_inputs<span>[</span><span>\"labels\"</span><span>]</span> <span>=</span> labels<span>[</span><span>\"input_ids\"</span><span>]</span>\n</span><span>    <span>return</span> model_inputs\n</span><span>\n</span><span><span># process dataset</span>\n</span><span>tokenized_dataset <span>=</span> dataset<span>.</span><span>map</span><span>(</span>preprocess_function<span>,</span> batched<span>=</span><span>True</span><span>,</span> remove_columns<span>=</span><span>list</span><span>(</span>dataset<span>[</span><span>\"train\"</span><span>]</span><span>.</span>features<span>)</span><span>)</span>\n</span><span>\n</span><span><span># save dataset to disk</span>\n</span><span>tokenized_dataset<span>[</span><span>\"train\"</span><span>]</span><span>.</span>save_to_disk<span>(</span>os<span>.</span>path<span>.</span>join<span>(</span>save_dataset_path<span>,</span><span>\"train\"</span><span>)</span><span>)</span>\n</span><span>tokenized_dataset<span>[</span><span>\"test\"</span><span>]</span><span>.</span>save_to_disk<span>(</span>os<span>.</span>path<span>.</span>join<span>(</span>save_dataset_path<span>,</span><span>\"eval\"</span><span>)</span><span>)</span>\n</span></code></pre></div>\n<h3>Fine-tune model using <code>deepspeed</code></h3>\n<p>Done! We can now start training our model! We learned in the introduction that we would leverage the DeepSpeed integration with the Hugging Face Trainer. Therefore we need to create a <code>deespeed_config.json</code>. In the <a href=\"https://www.deepspeed.ai/docs/config-json/\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-26\">DeepSpeed Configuration,</a> we define the ZeRO strategy we want to use and if we want to use mixed precision training. The Hugging Face Trainer allows us to inherit values from the <code>TrainingArguments</code> in our <code>deepspeed_config.json</code> to avoid duplicate values, check the <a href=\"https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/deepspeed#configuration\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-27\">documentation for more information.</a></p>\n<p>We created 4 deepspeed configurations for the experiments we ran, including <code>CPU offloading</code> and <code>mixed precision</code>:</p>\n<p>Depending on your setup, you can use those, e.g. if you are running on NVIDIA V100s, you have to use the config without <code>bf16</code> since V100 are not support <code>bfloat16</code> types.</p>\n<blockquote>\n<p>When fine-tuning <code>T5</code> models we cannot use <code>fp16</code> since it leads to overflow issues, see: <a href=\"https://github.com/huggingface/transformers/issues/4586\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-28\">#4586</a>, <a href=\"https://github.com/huggingface/transformers/issues/10830\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-29\">#10830</a>, <a href=\"https://github.com/huggingface/transformers/pull/10956\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-30\">#10956</a></p>\n</blockquote>\n<p>As mentioned in the beginning, we are using a p4dn.24xlarge AWS EC2 Instance including 8x NVIDIA A100 40GB. This means we can leverage <code>bf16</code>, which reduces the memory footprint of the model by almost ~2x, which allows us to train without offloading efficiently.</p>\n<p>We are going to use the <a href=\"https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/configs/ds_flan_t5_z3_config_bf16.json\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-31\">ds_flan_t5_z3_config_bf16.json</a>. If you are irritated by the <code>auto</code> values, check the <a href=\"https://huggingface.co/docs/transformers/v4.26.1/en/main_classes/deepspeed#configuration\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-32\">documentation</a>.</p>\n<div>\n<pre><code><span><span>{</span>\n</span><span>  <span>\"bf16\"</span><span>:</span> <span>{</span>\n</span><span>    <span>\"enabled\"</span><span>:</span> <span>\"auto\"</span>\n</span><span>  <span>}</span><span>,</span>\n</span><span>  <span>\"optimizer\"</span><span>:</span> <span>{</span>\n</span><span>    <span>\"type\"</span><span>:</span> <span>\"AdamW\"</span><span>,</span>\n</span><span>    <span>\"params\"</span><span>:</span> <span>{</span>\n</span><span>      <span>\"lr\"</span><span>:</span> <span>\"auto\"</span><span>,</span>\n</span><span>      <span>\"betas\"</span><span>:</span> <span>\"auto\"</span><span>,</span>\n</span><span>      <span>\"eps\"</span><span>:</span> <span>\"auto\"</span><span>,</span>\n</span><span>      <span>\"weight_decay\"</span><span>:</span> <span>\"auto\"</span>\n</span><span>    <span>}</span>\n</span><span>  <span>}</span><span>,</span>\n</span><span>  <span>\"scheduler\"</span><span>:</span> <span>{</span>\n</span><span>    <span>\"type\"</span><span>:</span> <span>\"WarmupLR\"</span><span>,</span>\n</span><span>    <span>\"params\"</span><span>:</span> <span>{</span>\n</span><span>      <span>\"warmup_min_lr\"</span><span>:</span> <span>\"auto\"</span><span>,</span>\n</span><span>      <span>\"warmup_max_lr\"</span><span>:</span> <span>\"auto\"</span><span>,</span>\n</span><span>      <span>\"warmup_num_steps\"</span><span>:</span> <span>\"auto\"</span>\n</span><span>    <span>}</span>\n</span><span>  <span>}</span><span>,</span>\n</span><span>  <span>\"zero_optimization\"</span><span>:</span> <span>{</span>\n</span><span>    <span>\"stage\"</span><span>:</span> <span>3</span><span>,</span>\n</span><span>    <span>\"overlap_comm\"</span><span>:</span> <span>true</span><span>,</span>\n</span><span>    <span>\"contiguous_gradients\"</span><span>:</span> <span>true</span><span>,</span>\n</span><span>    <span>\"sub_group_size\"</span><span>:</span> <span>1e9</span><span>,</span>\n</span><span>    <span>\"reduce_bucket_size\"</span><span>:</span> <span>\"auto\"</span><span>,</span>\n</span><span>    <span>\"stage3_prefetch_bucket_size\"</span><span>:</span> <span>\"auto\"</span><span>,</span>\n</span><span>    <span>\"stage3_param_persistence_threshold\"</span><span>:</span> <span>\"auto\"</span><span>,</span>\n</span><span>    <span>\"stage3_max_live_parameters\"</span><span>:</span> <span>1e9</span><span>,</span>\n</span><span>    <span>\"stage3_max_reuse_distance\"</span><span>:</span> <span>1e9</span><span>,</span>\n</span><span>    <span>\"stage3_gather_16bit_weights_on_model_save\"</span><span>:</span> <span>false</span>\n</span><span>  <span>}</span><span>,</span>\n</span><span>  <span>\"gradient_accumulation_steps\"</span><span>:</span> <span>\"auto\"</span><span>,</span>\n</span><span>  <span>\"gradient_clipping\"</span><span>:</span> <span>\"auto\"</span><span>,</span>\n</span><span>  <span>\"steps_per_print\"</span><span>:</span> <span>2000</span><span>,</span>\n</span><span>  <span>\"train_batch_size\"</span><span>:</span> <span>\"auto\"</span><span>,</span>\n</span><span>  <span>\"train_micro_batch_size_per_gpu\"</span><span>:</span> <span>\"auto\"</span><span>,</span>\n</span><span>  <span>\"wall_clock_breakdown\"</span><span>:</span> <span>false</span>\n</span><span><span>}</span>\n</span></code></pre></div>\n<p>Now, we need our training script. We prepared a <a href=\"https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/scripts/run_seq2seq_deepspeed.py\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-33\">run_seq2seq_deepspeed.py</a> training script based on the <a href=\"https://www.philschmid.de/fine-tune-flan-t5\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-34\">previous blog post</a>, which supports our deepspeed config and all other hyperparameters.</p>\n<p>We can start our training with the <code>deepspeed</code> launcher providing the number of GPUs, the deepspeed config, and our hyperparameters, including our model id for <code>google/flan-t5-xxl</code>.</p>\n<div>\n<pre><code><span>deepspeed --num_gpus<span>=</span><span>8</span> scripts/run_seq2seq_deepspeed.py <span>\\</span>\n</span><span>    --model_id google/flan-t5-xxl <span>\\</span>\n</span><span>    --dataset_path data <span>\\</span>\n</span><span>    --epochs <span>3</span> <span>\\</span>\n</span><span>    --per_device_train_batch_size <span>8</span> <span>\\</span>\n</span><span>    --per_device_eval_batch_size <span>8</span> <span>\\</span>\n</span><span>    --generation_max_length <span>129</span> <span>\\</span>\n</span><span>    --lr 1e-4 <span>\\</span>\n</span><span>    --deepspeed configs/ds_flan_t5_z3_config_bf16.json\n</span></code></pre></div>\n<p>Deepspeed now loads our model on the CPU and then splits it across our 8x A100 and starts the training. The training using the <a href=\"https://huggingface.co/datasets/cnn_dailymail\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-35\">CNN Dailymail Dataset</a> takes roughly 10 hours and costs <code>~$322</code></p>\n<h2>3. Results &amp; Experiments</h2>\n<p>During the creation of the tutorial and to get a better understanding of the hardware requirements, we ran a series of experiments for FLAN-T5 XL &amp; XXL, which should help us evaluate and understand the hardware requirements and cost of training those models.</p>\n<p>Below you'll find a table of the experiments and more information about the setup.</p>\n<p>Dataset: <a href=\"https://huggingface.co/datasets/cnn_dailymail\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-37\">CNN Dailymail Dataset</a> with a train dataset size of <code>287113</code> samples with a sequence length of <code>512</code></p>\n<p>Hyperparameters: Epoch 3</p>\n<p>Setup and instance types:</p>\n<ul><li>4x V100 16GB: p3.8xlarge</li>\n<li>4x A10G 24GB: g5.24xlarge</li>\n<li>8x V100 16GB: p3.16xlarge</li>\n<li>8x A100 40GB: p4dn.24xlarge</li>\n</ul><p>We can see that <code>bf16</code> provides significant advantages over <code>fp32</code>. We could fit FLAN-T5-XXL on 4x A10G (24GB) but not on 8x V100 16GB.</p>\n<p>We also learned that if the model fits on the GPUs with a batch size &gt; 4 without offloading, we are ~2x faster and more cost-effective than offloading the model and scaling the batch size.</p>\n<hr><p>Thanks for reading! If you have any questions, feel free to contact me on <a href=\"https://twitter.com/_philschmid\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-38\">Twitter</a> or <a href=\"https://www.linkedin.com/in/philipp-schmid-a6a2bb196/\" rel=\"noopener noreferrer\" target=\"_blank\" id=\"reader.external-link.num-39\">LinkedIn</a>.</p>\n</div></article></div>",
      "textContent": "\nFLAN-T5, released with the Scaling Instruction-Finetuned Language Models paper, is an enhanced version of T5 that has been fine-tuned in a mixture of tasks, or simple words, a better T5 model in any aspect. FLAN-T5 outperforms T5 by double-digit improvements for the same number of parameters. Google has open sourced 5 checkpoints available on Hugging Face ranging from 80M parameter up to 11B parameter.\nIn a previous blog post, we already learned how to “Fine-tune FLAN-T5 for chat & dialogue summarization” using the base version (250M parameter) of the model. In this blog post, we look into how we can scale the training from the Base version to the XL (3B) or XXL (11B).\nThis means we will learn how to fine-tune FLAN-T5 XL & XXL using model parallelism, multiple GPUs, and DeepSpeed ZeRO.\nYou will learn about the following:\nin addition to the tutorial, we have run a series of experiments to help you choose the right hardware setup. You can find the details in the Results & Experiments section.\nLet's get started! 🚀\n1. What is DeepSpeed ZeRO?\nDeepSpeed ZeRO is part of the DeepSpeed Training Pillar, which focus on efficient large-scale Training of Transformer models. DeepSpeed ZeRO or Zero Redundancy Optimizer is a method to reduce the memory footprint. Compared to basic data parallelism, ZeRO partitions optimizer states, gradients, and model parameters to save significant memory across multiple devices.\n\nIf you want to learn more about DeepSpeed ZeRO, checkout: ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters\nDeepSpeed ZeRO is natively integrated into the Hugging Face Transformers Trainer. The integration enables leveraging ZeRO by simply providing a DeepSpeed config file, and the Trainer takes care of the rest.\nExcerpt: DeepSpeed ZeRO-offload\nDeepSpeed ZeRO not only allows us to parallelize our models on multiple GPUs, it also implements Offloading. ZeRO-Offload implements optimizations that offload optimizer and model to the CPU to train larger models on the given GPUs, e.g. 10B parameter GPT-2 on a single V100 GPU. We used ZeRO-offload for the experiments but will not use it in the tutorial.\n2. Fine-tune FLAN-T5-XXL using Deepspeed\nWe now know that we can use DeepSpeed ZeRO together with Hugging Face Transformers to easily scale our hardware in cases where the model no longer fits on GPU. That's exactly what we need to solve since the FLAN-T5-XXL weights in fp32 are already 44GB big. This makes it almost impossible to fit on a single GPU when adding activations and optimizer states.\nIn this tutorial, we cover how to fine-tune FLAN-T5-XXL (11B version) on the CNN Dailymail Dataset for news summarization. The provided script and pre-processing can easily be adjusted to fine-tune FLAN-T5-XL and use a different dataset.\nNote: This tutorial was created and run on a p4dn.24xlarge AWS EC2 Instance including 8x NVIDIA A100 40GB.\nSetup Development Environment\nThe first step is to install the Hugging Face Libraries, including transformers and datasets, and DeepSeed. Running the following cell will install all the required packages.\n\n# install torch with the correct cuda version, check nvcc --version\npip install torch --extra-index-url https://download.pytorch.org/whl/cu116 --upgrade\n# install Hugging Face Libraries\npip install \"transformers==4.26.0\" \"datasets==2.9.0\" \"accelerate==0.16.0\" \"evaluate==0.4.0\" --upgrade\n# install deepspeed and ninja for jit compilations of kernels\npip install \"deepspeed==0.8.0\" ninja --upgrade\n# install additional dependencies needed for training\npip install rouge-score nltk py7zr tensorboard\n\nLoad and prepare dataset\nSimilar to the “Fine-tune FLAN-T5 for chat & dialogue summarization” we need to prepare a dataset to fine-tune our model. As mentioned in the beginning, we will fine-tune FLAN-T5-XXL on the CNN Dailymail Dataset. The blog post is not going into detail about the dataset generation. If you want to learn the detailed steps check out the previous post.\nWe define some parameters, which we use throughout the whole example, feel free to adjust it to your needs.\n\n# experiment config\nmodel_id = \"google/flan-t5-xxl\" # Hugging Face Model Id\ndataset_id = \"cnn_dailymail\" # Hugging Face Dataset Id\ndataset_config = \"3.0.0\" # config/verison of the dataset\nsave_dataset_path = \"data\" # local path to save processed dataset\ntext_column = \"article\" # column of input text is\nsummary_column = \"highlights\" # column of the output text\n# custom instruct prompt start\nprompt_template = f\"Summarize the following news article:\\n{{input}}\\nSummary:\\n\"\n\nCompared to the previous example, we are splitting the processing and training into two separate paths. This allows you to run the preprocessing outside of the GPU instance. We process (tokenize) the dataset and save it to disk and then load in our train script from disk again.\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nimport numpy as np\n\n# Load dataset from the hub\ndataset = load_dataset(dataset_id,name=dataset_config)\n# Load tokenizer of FLAN-t5-base\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nprint(f\"Train dataset size: {len(dataset['train'])}\")\nprint(f\"Test dataset size: {len(dataset['test'])}\")\n\n# Train dataset size: 287113\n# Test dataset size: 11490\n\nWe defined a prompt_template in our config, which we will use to construct an instruct prompt for better performance of our model. Our prompt_template has a “fixed” start and end, and our document is in the middle. This means we need to ensure that the “fixed” template parts + document are not exceeding the max length of the model. Therefore we calculate the max length of our document, which we will later use for padding and truncation\n\nprompt_lenght = len(tokenizer(prompt_template.format(input=\"\"))[\"input_ids\"])\nmax_sample_length = tokenizer.model_max_length - prompt_lenght\nprint(f\"Prompt length: {prompt_lenght}\")\nprint(f\"Max input length: {max_sample_length}\")\n\n# Prompt length: 12\n# Max input length: 500\n\nWe know now that our documents can be “500” tokens long to fit our template_prompt still correctly. In addition to our input, we need to understand better our “target” sequence length meaning and how long are the summarization ins our dataset. Therefore we iterate over the dataset and calculate the max input length (at max 500) and the max target length. (takes a few minutes)\n\nfrom datasets import concatenate_datasets\nimport numpy as np\n\n# The maximum total input sequence length after tokenization.\n# Sequences longer than this will be truncated, sequences shorter will be padded.\ntokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[text_column], truncation=True), batched=True, remove_columns=[text_column, summary_column])\nmax_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\nmax_source_length = min(max_source_length, max_sample_length)\nprint(f\"Max source length: {max_source_length}\")\n\n# The maximum total sequence length for target text after tokenization.\n# Sequences longer than this will be truncated, sequences shorter will be padded.\"\ntokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[summary_column], truncation=True), batched=True, remove_columns=[text_column, summary_column])\ntarget_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n# use 95th percentile as max target length\nmax_target_length = int(np.percentile(target_lenghts, 95))\nprint(f\"Max target length: {max_target_length}\")\n\nWe now have everything needed to process our dataset.\n\nimport os\n\ndef preprocess_function(sample, padding=\"max_length\"):\n    # created prompted input\n    inputs = [prompt_template.format(input=item) for item in sample[text_column]]\n\n    # tokenize inputs\n    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n\n    # Tokenize targets with the `text_target` keyword argument\n    labels = tokenizer(text_target=sample[summary_column], max_length=max_target_length, padding=padding, truncation=True)\n\n    # padding in the loss.\n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# process dataset\ntokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=list(dataset[\"train\"].features))\n\n# save dataset to disk\ntokenized_dataset[\"train\"].save_to_disk(os.path.join(save_dataset_path,\"train\"))\ntokenized_dataset[\"test\"].save_to_disk(os.path.join(save_dataset_path,\"eval\"))\n\nFine-tune model using deepspeed\nDone! We can now start training our model! We learned in the introduction that we would leverage the DeepSpeed integration with the Hugging Face Trainer. Therefore we need to create a deespeed_config.json. In the DeepSpeed Configuration, we define the ZeRO strategy we want to use and if we want to use mixed precision training. The Hugging Face Trainer allows us to inherit values from the TrainingArguments in our deepspeed_config.json to avoid duplicate values, check the documentation for more information.\nWe created 4 deepspeed configurations for the experiments we ran, including CPU offloading and mixed precision:\nDepending on your setup, you can use those, e.g. if you are running on NVIDIA V100s, you have to use the config without bf16 since V100 are not support bfloat16 types.\n\nWhen fine-tuning T5 models we cannot use fp16 since it leads to overflow issues, see: #4586, #10830, #10956\n\nAs mentioned in the beginning, we are using a p4dn.24xlarge AWS EC2 Instance including 8x NVIDIA A100 40GB. This means we can leverage bf16, which reduces the memory footprint of the model by almost ~2x, which allows us to train without offloading efficiently.\nWe are going to use the ds_flan_t5_z3_config_bf16.json. If you are irritated by the auto values, check the documentation.\n\n{\n  \"bf16\": {\n    \"enabled\": \"auto\"\n  },\n  \"optimizer\": {\n    \"type\": \"AdamW\",\n    \"params\": {\n      \"lr\": \"auto\",\n      \"betas\": \"auto\",\n      \"eps\": \"auto\",\n      \"weight_decay\": \"auto\"\n    }\n  },\n  \"scheduler\": {\n    \"type\": \"WarmupLR\",\n    \"params\": {\n      \"warmup_min_lr\": \"auto\",\n      \"warmup_max_lr\": \"auto\",\n      \"warmup_num_steps\": \"auto\"\n    }\n  },\n  \"zero_optimization\": {\n    \"stage\": 3,\n    \"overlap_comm\": true,\n    \"contiguous_gradients\": true,\n    \"sub_group_size\": 1e9,\n    \"reduce_bucket_size\": \"auto\",\n    \"stage3_prefetch_bucket_size\": \"auto\",\n    \"stage3_param_persistence_threshold\": \"auto\",\n    \"stage3_max_live_parameters\": 1e9,\n    \"stage3_max_reuse_distance\": 1e9,\n    \"stage3_gather_16bit_weights_on_model_save\": false\n  },\n  \"gradient_accumulation_steps\": \"auto\",\n  \"gradient_clipping\": \"auto\",\n  \"steps_per_print\": 2000,\n  \"train_batch_size\": \"auto\",\n  \"train_micro_batch_size_per_gpu\": \"auto\",\n  \"wall_clock_breakdown\": false\n}\n\nNow, we need our training script. We prepared a run_seq2seq_deepspeed.py training script based on the previous blog post, which supports our deepspeed config and all other hyperparameters.\nWe can start our training with the deepspeed launcher providing the number of GPUs, the deepspeed config, and our hyperparameters, including our model id for google/flan-t5-xxl.\n\ndeepspeed --num_gpus=8 scripts/run_seq2seq_deepspeed.py \\\n    --model_id google/flan-t5-xxl \\\n    --dataset_path data \\\n    --epochs 3 \\\n    --per_device_train_batch_size 8 \\\n    --per_device_eval_batch_size 8 \\\n    --generation_max_length 129 \\\n    --lr 1e-4 \\\n    --deepspeed configs/ds_flan_t5_z3_config_bf16.json\n\nDeepspeed now loads our model on the CPU and then splits it across our 8x A100 and starts the training. The training using the CNN Dailymail Dataset takes roughly 10 hours and costs ~$322\n3. Results & Experiments\nDuring the creation of the tutorial and to get a better understanding of the hardware requirements, we ran a series of experiments for FLAN-T5 XL & XXL, which should help us evaluate and understand the hardware requirements and cost of training those models.\nBelow you'll find a table of the experiments and more information about the setup.\nDataset: CNN Dailymail Dataset with a train dataset size of 287113 samples with a sequence length of 512\nHyperparameters: Epoch 3\nSetup and instance types:\n4x V100 16GB: p3.8xlarge\n4x A10G 24GB: g5.24xlarge\n8x V100 16GB: p3.16xlarge\n8x A100 40GB: p4dn.24xlarge\nWe can see that bf16 provides significant advantages over fp32. We could fit FLAN-T5-XXL on 4x A10G (24GB) but not on 8x V100 16GB.\nWe also learned that if the model fits on the GPUs with a batch size > 4 without offloading, we are ~2x faster and more cost-effective than offloading the model and scaling the batch size.\nThanks for reading! If you have any questions, feel free to contact me on Twitter or LinkedIn.\n",
      "length": 12892,
      "excerpt": "FLAN-T5, released with the Scaling Instruction-Finetuned Language Models paper, is an enhanced version of T5 that has been fine-tuned in a mixture of tasks, or simple words, a better T5 model in any aspect. FLAN-T5 outperforms T5 by double-digit improvements for the same number of parameters. Google has open sourced 5 checkpoints available on Hugging Face ranging from 80M parameter up to 11B parameter.",
      "siteName": null,
      "summary": "In this tutorial, we cover how to fine-tune FLAN-T5-XXL (11B version) on the CNN Dailymail Dataset for news summarization. The tutorial was created and run on a p4dn.24xlarge AWS EC2 Instance including 8x NVIDIA A100 40GB.",
      "topics": [
        {
          "label": "Computers & Internet",
          "score": 0.9991716146469116
        },
        {
          "label": "Education & Reference",
          "score": 0.8006901144981384
        },
        {
          "label": "Science & Mathematics",
          "score": 0.7219294309616089
        }
      ]
    }
  }
}